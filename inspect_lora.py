"""
LoRA íŒŒì¼ ê²€ì‚¬ ìŠ¤í¬ë¦½íŠ¸
safetensors íŒŒì¼ ë‚´ë¶€ì˜ í‚¤(key) ì´ë¦„ê³¼ í˜•ì‹ì„ í™•ì¸í•©ë‹ˆë‹¤.
"""

import sys
from safetensors.torch import load_file
from pathlib import Path


def inspect_lora_file(file_path):
    """
    LoRA safetensors íŒŒì¼ì˜ ë‚´ìš©ì„ ê²€ì‚¬í•©ë‹ˆë‹¤.

    Args:
        file_path: .safetensors íŒŒì¼ ê²½ë¡œ
    """
    print(f"\n{'='*80}")
    print(f"LoRA íŒŒì¼ ê²€ì‚¬: {file_path}")
    print(f"{'='*80}\n")

    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not Path(file_path).exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        return

    # safetensors ë¡œë“œ
    try:
        state_dict = load_file(file_path)
        print(f"âœ… íŒŒì¼ ë¡œë“œ ì„±ê³µ!")
        print(f"   ì´ í‚¤ ê°œìˆ˜: {len(state_dict)}")

        # íŒŒì¼ í¬ê¸°
        file_size = Path(file_path).stat().st_size
        print(f"   íŒŒì¼ í¬ê¸°: {file_size / (1024*1024):.2f} MB")

        # í‚¤ í˜•ì‹ ë¶„ì„
        print(f"\nğŸ“‹ í‚¤(Key) í˜•ì‹ ë¶„ì„:\n")

        # ìƒ˜í”Œ í‚¤ ì¶œë ¥ (ì²˜ìŒ 10ê°œ)
        print("ì²˜ìŒ 10ê°œ í‚¤:")
        for i, (key, tensor) in enumerate(state_dict.items()):
            if i >= 10:
                break
            print(f"  [{i+1}] {key}")
            print(f"      Shape: {tensor.shape}, Dtype: {tensor.dtype}")

        # í‚¤ í˜•ì‹ íŒë³„
        print(f"\nğŸ” í˜•ì‹ íŒë³„:")

        sample_keys = list(state_dict.keys())[:5]

        if any('lora_unet_' in k for k in sample_keys):
            print("  âœ… WebUI/Civitai í˜•ì‹")
            print("     (ì˜ˆ: lora_unet_down_blocks_0_...)")
        elif any('base_model.model.' in k for k in sample_keys):
            print("  âœ… PEFT í˜•ì‹")
            print("     (ì˜ˆ: base_model.model.down_blocks.0...)")
        elif any('unet.' in k or 'text_encoder.' in k for k in sample_keys):
            print("  âœ… Diffusers í˜•ì‹")
            print("     (ì˜ˆ: unet.down_blocks.0...)")
        else:
            print("  âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” í˜•ì‹")

        # lora_down/lora_up í™•ì¸
        has_lora_down = any('lora_down' in k for k in sample_keys)
        has_lora_up = any('lora_up' in k for k in sample_keys)
        has_lora_A = any('lora_A' in k for k in sample_keys)
        has_lora_B = any('lora_B' in k for k in sample_keys)

        print(f"\n  LoRA ë ˆì´ì–´ íƒ€ì…:")
        if has_lora_down and has_lora_up:
            print(f"    âœ… lora_down, lora_up (WebUI ìŠ¤íƒ€ì¼)")
        if has_lora_A and has_lora_B:
            print(f"    âœ… lora_A, lora_B (PEFT ìŠ¤íƒ€ì¼)")

        print(f"\n{'='*80}\n")

    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python inspect_lora.py <safetensors_íŒŒì¼_ê²½ë¡œ>")
        print("\nì˜ˆì‹œ:")
        print("  python inspect_lora.py my_lora_model/checkpoint-250/lora_weights.safetensors")
        print("  python inspect_lora.py downloaded_civitai_model.safetensors")
        sys.exit(1)

    file_path = sys.argv[1]
    inspect_lora_file(file_path)
