"""
Modal ë°°í¬ íŒŒì¼
LoRA í•™ìŠµ ë° ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ì„œë²„ë¦¬ìŠ¤ GPU ì• í”Œë¦¬ì¼€ì´ì…˜
"""

import modal
import os
from pathlib import Path

# Modal ì•± ìƒì„±
app = modal.App("lora-training-inference")

# ë² ì´ìŠ¤ ì´ë¯¸ì§€ ë¹Œë“œ í•¨ìˆ˜ (ë² ì´ìŠ¤ ëª¨ë¸ + rembg ëª¨ë¸ í¬í•¨)
def download_base_model_to_image():
    """
    ì´ë¯¸ì§€ ë¹Œë“œ ì‹œ ë² ì´ìŠ¤ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì´ë¯¸ì§€ì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.
    ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹œê°„ì„ ëŒ€í­ ë‹¨ì¶•ì‹œí‚µë‹ˆë‹¤.
    """
    from diffusers import StableDiffusionPipeline
    import torch

    model_id = "stablediffusionapi/anything-v5"
    cache_dir = "/base_models/anything-v5"

    print(f"ğŸ“¥ Downloading base model: {model_id}")

    pipe = StableDiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        safety_checker=None,
        cache_dir=cache_dir
    )

    # ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥
    pipe.save_pretrained(cache_dir)

    print(f"âœ… Base model cached to: {cache_dir}")


def download_rembg_model_to_image():
    """
    rembgì˜ u2net ëª¨ë¸(176MB)ì„ ì´ë¯¸ì§€ì— ë¯¸ë¦¬ ë‹¤ìš´ë¡œë“œ
    ë§¤ í•™ìŠµë§ˆë‹¤ ë‹¤ìš´ë°›ì§€ ì•Šë„ë¡ ìµœì í™”
    """
    from rembg import new_session
    import os

    # u2net ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (rembgê°€ ìë™ìœ¼ë¡œ ~/.u2netì— ì €ì¥)
    print("ğŸ“¥ Downloading rembg u2net model (176MB)...")

    # HOME í™˜ê²½ë³€ìˆ˜ ì„¤ì • (Modal í™˜ê²½)
    os.environ["HOME"] = "/root"

    # u2net ì„¸ì…˜ ìƒì„± (ì²« ìƒì„± ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ)
    session = new_session("u2net")

    print("âœ… Rembg u2net model cached to /root/.u2net/")


# ë² ì´ìŠ¤ ì´ë¯¸ì§€ (ë¼ì´ë¸ŒëŸ¬ë¦¬ + ë² ì´ìŠ¤ ëª¨ë¸ í¬í•¨ - ìºì‹± ìµœì í™”)
# íŒ¨í‚¤ì§€ì™€ ë² ì´ìŠ¤ ëª¨ë¸ì€ ê±°ì˜ ë³€ê²½ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ í•œ ë²ˆ ë¹Œë“œë˜ë©´ ê³„ì† ì¬ì‚¬ìš©ë¨
base_image = (
    modal.Image.debian_slim(python_version="3.11")
    .env({"CACHE_BUSTER": "1"})
    .apt_install("git", "libgl1-mesa-glx", "libglib2.0-0")
    .pip_install(
        "torch>=2.0.0",
        "torchvision",
        "diffusers>=0.28.0",
        "transformers>=4.35.0",
        "accelerate>=0.24.0",
        "peft>=0.7.0",
        "safetensors>=0.4.0",
        "bitsandbytes>=0.41.0",
        "xformers>=0.0.22",  # xFormers ìµœì í™” ì¶”ê°€
        "Pillow>=10.0.0",
        "opencv-python>=4.8.0",
        "rembg>=2.0.0",
        "onnxruntime>=1.16.0",
        "easyocr>=1.7.0",
        "numpy>=1.24.0",
        "tqdm>=4.66.0",
        "datasets>=2.14.0",
        "fastapi>=0.104.0",
        "uvicorn[standard]>=0.24.0",
        "boto3>=1.34.0",
        "requests>=2.31.0",
    )
    # ë² ì´ìŠ¤ ëª¨ë¸ì„ ì´ë¯¸ì§€ì— í¬í•¨ (ì²« ë¹Œë“œ ì‹œ ì‹œê°„ ê±¸ë¦¬ì§€ë§Œ ì´í›„ ë§¤ìš° ë¹ ë¦„)
    .run_function(download_base_model_to_image)
    # rembg u2net ëª¨ë¸ í¬í•¨ (176MB, ë§¤ í•™ìŠµë§ˆë‹¤ ë‹¤ìš´ë°›ì§€ ì•Šë„ë¡)
    .run_function(download_rembg_model_to_image)
)

# ìµœì¢… ì´ë¯¸ì§€ (core ë””ë ‰í† ë¦¬ ì¶”ê°€ - ìì£¼ ë³€ê²½ë˜ëŠ” íŒŒì¼)
# coreê°€ ë³€ê²½ë˜ì–´ë„ base_imageëŠ” ì¬ì‚¬ìš©ë¨
image = base_image.add_local_dir(
    local_path=str(Path(__file__).parent / "core"),
    remote_path="/root/core"
)

# Modal Volume ì„¤ì • (í•™ìŠµ ë°ì´í„° ìºì‹±ìš©)
CACHE_DIR = "/cache"

# AWS Secrets ì„¤ì •
secrets = modal.Secret.from_name("lora-secrets")

# LoRA í•™ìŠµ í´ë˜ìŠ¤ (GPU T4 ì‚¬ìš©)
@app.cls(
    image=image,
    gpu="T4",  # í•™ìŠµìš© GPU
    timeout=7200,  # 2ì‹œê°„
    volumes={
        CACHE_DIR: modal.Volume.from_name("lora-cache", create_if_missing=True)
    },
    secrets=[secrets],
    memory=32768,  # 32GB RAM
    enable_memory_snapshot=True,  # ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· í™œì„±í™” - ë¶€íŒ… ì‹œê°„ íšê¸°ì  ë‹¨ì¶•!
    concurrency_limit=10,  # ìµœëŒ€ 10ê°œ ì»¨í…Œì´ë„ˆë§Œ ë™ì‹œ ì‹¤í–‰ (GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬)
)
class LoraTrainer:
    """
    LoRA í•™ìŠµì„ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤

    enable_memory_snapshot=Trueë¡œ ì¸í•´:
    - ì²« ë²ˆì§¸ ì»¨í…Œì´ë„ˆ ë¶€íŒ… ì‹œ: ëª¨ë¸ ë¡œë“œ + ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìƒì„± (ëŠë¦¼)
    - ì´í›„ ì»¨í…Œì´ë„ˆë“¤: ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì—ì„œ ë³µì› (ë§¤ìš° ë¹ ë¦„, ìˆ˜ ì´ˆ ì´ë‚´)
    """

    @modal.enter()
    def load_models(self):
        """
        ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹œ ë² ì´ìŠ¤ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.
        ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì´ í™œì„±í™”ë˜ì–´ ìˆì–´ ì´ ì´ˆê¸°í™”ëŠ” í•œ ë²ˆë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
        """
        from diffusers import StableDiffusionPipeline
        import torch

        self.base_model_path = "stablediffusionapi/anything-v5"

        # í˜„ì¬ í•™ìŠµ ì¤‘ì¸ ì‘ì—… ì •ë³´ ì´ˆê¸°í™”
        self.current_training = None

        print(f"ğŸ“¦ Loading base model from: {self.base_model_path}")

        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ (ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì— í¬í•¨ë¨)
        self.pipe = StableDiffusionPipeline.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            safety_checker=None
        )
        self.pipe.to("cuda")

        print("âœ… Base model loaded and ready!")
        print("ğŸ’¾ Memory snapshot will be created after this initialization")

    @modal.exit()
    def cleanup_on_exit(self):
        """
        ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì‹œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        í•™ìŠµì´ ì™„ë£Œë˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ ì¢…ë£Œë  ê²½ìš° ì‹¤íŒ¨ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.
        """
        import requests

        # í•™ìŠµ ì¤‘ì¸ ì‘ì—…ì´ ìˆê³ , ì™„ë£Œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì‹¤íŒ¨ ì½œë°± ì „ì†¡
        if self.current_training is not None:
            callback_url = self.current_training.get("callback_url")
            user_id = self.current_training.get("user_id")
            model_id = self.current_training.get("model_id")
            job_id = self.current_training.get("job_id")
            model_name = self.current_training.get("model_name")

            if callback_url:
                try:
                    print(f"âš ï¸ Container shutting down during training for job {job_id}")
                    print(f"ğŸ“¤ Sending failure callback...")

                    callback_data = {
                        "userId": user_id,
                        "modelId": model_id,
                        "jobId": job_id,
                        "modelName": model_name,
                        "status": "FAIL",
                        "error": "Training interrupted: Container was shut down or cancelled",
                        "traceback": "Container exit during training (possible timeout, manual cancellation, or server shutdown)"
                    }
                    response = requests.post(callback_url, json=callback_data, timeout=10)
                    response.raise_for_status()
                    print(f"âœ… Failure callback sent for interrupted job {job_id}")
                except Exception as e:
                    print(f"âš ï¸ Failed to send failure callback on exit: {e}")

    @modal.method()
    def train_lora(
        self,
        user_id: str,
        model_id: int,
        job_id: int,
        model_name: str,
        training_image_urls: list[str],
        callback_url: str = None,
        trigger_word: str = None,
        epochs: int = 250,
        learning_rate: float = 2e-5,
        lora_rank: int = 32,
        base_model: str = "stablediffusionapi/anything-v5",
        skip_preprocessing: bool = False
    ):
        """
        Execute LoRA training.

        Args:
            user_id: User ID
            model_id: Model ID
            job_id: Training job ID (unique identifier)
            model_name: Model name
            training_image_urls: S3 presigned URL list
            callback_url: Callback URL when training completes
            trigger_word: Trigger word (None = no trigger word in captions)
            epochs: Number of training epochs (default: 250)
            learning_rate: Learning rate (default: 2e-5)
            lora_rank: LoRA Rank (default: 32)
            base_model: Base model (default: stablediffusionapi/anything-v5)
            skip_preprocessing: Skip preprocessing (captioning is always performed)

        Returns:
            dict: Training result
        """
        from core.config import TrainingConfig
        from core.train import train_with_preprocessing
        import requests
        import shutil
        import boto3
        import time

        # í˜„ì¬ í•™ìŠµ ì¤‘ì¸ ì‘ì—… ì •ë³´ ì €ì¥ (ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì‹œ ì‹¤íŒ¨ ì½œë°± ì „ì†¡ìš©)
        self.current_training = {
            "user_id": user_id,
            "model_id": model_id,
            "job_id": job_id,
            "model_name": model_name,
            "callback_url": callback_url
        }

        print(f"Starting training for job: {job_id}, model: {model_name}")
        print(f"Number of training images: {len(training_image_urls)}")

        # Progress callback function
        def send_progress_callback(status, message):
            """Send progress to backend"""
            if callback_url:
                try:
                    progress_data = {
                        "userId": user_id,
                        "modelId": model_id,
                        "jobId": job_id,
                        "status": status,
                        "message": message
                    }
                    requests.post(callback_url, json=progress_data, timeout=2)
                    print(f"ğŸ“Š Progress sent: {status} - {message}")
                except Exception as e:
                    print(f"âš ï¸ Failed to send progress: {e}")

        # 1. Loading server
        send_progress_callback("LOADING", "Loading server")
        time.sleep(0.5)

        # 2. Downloading images from S3
        send_progress_callback("DOWNLOADING", "Downloading training images from S3")

        # Download training images from S3 (jobId ê¸°ë°˜ í´ë”)
        temp_dataset_path = f"{CACHE_DIR}/training-{job_id}/dataset"
        os.makedirs(temp_dataset_path, exist_ok=True)

        print("Downloading training images from S3...")
        for idx, url in enumerate(training_image_urls):
            ext = ".jpg"
            if ".png" in url.lower():
                ext = ".png"

            local_path = os.path.join(temp_dataset_path, f"image_{idx:04d}{ext}")
            response = requests.get(url, stream=True)
            response.raise_for_status()

            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

        print(f"Downloaded {len(training_image_urls)} images")
        send_progress_callback("DOWNLOADING_COMPLETE", f"Downloaded {len(training_image_urls)} images")

        # Training configuration (jobId ê¸°ë°˜ ê²½ë¡œ)
        output_dir = f"{CACHE_DIR}/training-{job_id}/model"
        clean_dataset_path = f"{CACHE_DIR}/training-{job_id}/dataset_clean"

        config = TrainingConfig(
            raw_dataset_path=temp_dataset_path,
            clean_dataset_path=clean_dataset_path,
            output_dir=output_dir,
            model_id=base_model,
            num_epochs=epochs,
            learning_rate=learning_rate,
            lora_r=lora_rank,
            trigger_word=trigger_word
        )

        # Training callback function (detailed progress tracking)
        def training_callback(status, phase, current_epoch, total_epochs, message):
            """Training progress callback - handles all phases"""
            print(f"Training callback: phase={phase}, epoch={current_epoch}/{total_epochs}, message={message}")

            if phase == "preprocessing":
                # ì „ì²˜ë¦¬ ë° ìº¡ì…”ë‹ ë‹¨ê³„
                if "ì™„ë£Œ" in message or "complete" in message.lower():
                    send_progress_callback("CAPTIONING_COMPLETE", message)
                else:
                    send_progress_callback("PREPROCESSING", message)
            elif phase == "training":
                # í•™ìŠµ ë‹¨ê³„
                if current_epoch > 0:
                    progress_message = f"Training {current_epoch}/{total_epochs}"
                    send_progress_callback("TRAINING", progress_message)
            else:
                # ê¸°íƒ€ ë‹¨ê³„
                send_progress_callback(status, message)

        # Execute training
        try:
            # 3. Start training
            send_progress_callback("TRAINING", "Starting training pipeline...")

            train_result = train_with_preprocessing(
                raw_dataset_path=temp_dataset_path,
                output_dir=output_dir,
                config=config,
                skip_preprocessing=skip_preprocessing,  # ë™ì  íŒŒë¼ë¯¸í„°
                callback=training_callback
            )

            # í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì°¾ê¸° (ìµœì¢… checkpointì˜ safetensors íŒŒì¼)
            # output_dir/checkpoint-{ìµœì¢…epoch}/lora_weights.safetensors
            import glob
            checkpoint_dirs = glob.glob(os.path.join(output_dir, "checkpoint-*"))
            if not checkpoint_dirs:
                raise FileNotFoundError(f"No checkpoint found in {output_dir}")

            # ê°€ì¥ ë†’ì€ ì—í¬í¬ ë²ˆí˜¸ì˜ checkpoint ì°¾ê¸°
            latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split('-')[-1]))
            model_file_path = os.path.join(latest_checkpoint, "lora_weights.safetensors")

            if not os.path.exists(model_file_path):
                raise FileNotFoundError(f"LoRA weights not found at {model_file_path}")

            print(f"Found trained model: {model_file_path}")

            # 4. ëª¨ë¸ ì—…ë¡œë“œ ì¤‘
            send_progress_callback("UPLOADING", "Uploading model")

            # S3 ì—…ë¡œë“œ (ë‹¨ì¼ .safetensors íŒŒì¼ë§Œ - Civitai ë°©ì‹)
            print("Uploading trained model to S3...")
            s3_client = boto3.client('s3')
            bucket_name = os.environ.get("AWS_S3_MODELS_BUCKET", "lora-models-bucket")

            # S3 í‚¤: training-{jobId}/{modelName}.safetensors (ê³ ìœ ì„± ë³´ì¥)
            s3_model_key = f"training-{job_id}/{model_name}.safetensors"
            s3_client.upload_file(
                model_file_path,
                bucket_name,
                s3_model_key,
                ExtraArgs={'ContentType': 'application/octet-stream'}
            )
            print(f"âœ… Uploaded model (WebUI format): s3://{bucket_name}/{s3_model_key}")

            file_size = os.path.getsize(model_file_path)

            # ì½œë°± í˜¸ì¶œ
            if callback_url:
                try:
                    callback_data = {
                        "userId": user_id,
                        "modelId": model_id,
                        "jobId": job_id,
                        "modelName": model_name,
                        "s3ModelKey": s3_model_key,
                        "fileSize": file_size,
                        "status": "SUCCESS"
                    }
                    response = requests.post(callback_url, json=callback_data, timeout=10)
                    response.raise_for_status()
                    print(f"âœ… Callback successful: {callback_url}")
                except Exception as e:
                    print(f"âŒ Callback failed: {e}")

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (ì „ì²´ training-{jobId} í´ë” ì‚­ì œ)
            training_folder = f"{CACHE_DIR}/training-{job_id}"
            shutil.rmtree(training_folder, ignore_errors=True)

            # í•™ìŠµ ì™„ë£Œ - current_training ì´ˆê¸°í™” (ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì‹œ ì¤‘ë³µ ì‹¤íŒ¨ ì½œë°± ë°©ì§€)
            self.current_training = None

            return {
                "status": "SUCCESS",
                "s3_model_key": s3_model_key,
                "file_size": file_size,
                **train_result
            }

        except Exception as e:
            import traceback
            error_traceback = traceback.format_exc()
            print(f"âŒ Training failed: {e}")
            print(f"Traceback:\n{error_traceback}")

            # ì‹¤íŒ¨ ì½œë°± (ì¦‰ì‹œ ì „ì†¡)
            if callback_url:
                try:
                    callback_data = {
                        "userId": user_id,
                        "modelId": model_id,
                        "jobId": job_id,
                        "modelName": model_name,
                        "status": "FAIL",
                        "error": str(e),
                        "traceback": error_traceback[:1000]  # ìµœëŒ€ 1000ìê¹Œì§€ë§Œ ì „ì†¡
                    }
                    response = requests.post(callback_url, json=callback_data, timeout=10)
                    response.raise_for_status()
                    print(f"âœ… Failure callback sent to backend for job {job_id}.")
                except Exception as cb_e:
                    print(f"âš ï¸ Failed to send failure callback: {cb_e}")

            # ì •ë¦¬ (ì‹¤íŒ¨ ì‹œì—ë„ ì„ì‹œ í´ë” ì‚­ì œ)
            training_folder = f"{CACHE_DIR}/training-{job_id}"
            shutil.rmtree(training_folder, ignore_errors=True)

            # í•™ìŠµ ì‹¤íŒ¨ - current_training ì´ˆê¸°í™” (ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì‹œ ì¤‘ë³µ ì‹¤íŒ¨ ì½œë°± ë°©ì§€)
            self.current_training = None

            raise


# ì´ë¯¸ì§€ ìƒì„± í´ë˜ìŠ¤ (GPU T4 ì‚¬ìš©)
@app.cls(
    image=image,
    gpu="T4",  # ìƒì„±ìš© GPU (T4 ì‚¬ìš©)
    timeout=600,  # 10ë¶„
    volumes={
        CACHE_DIR: modal.Volume.from_name("lora-cache", create_if_missing=True)
    },
    secrets=[secrets],
    memory=16384,  # 16GB RAM
    enable_memory_snapshot=True,  # ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· í™œì„±í™” - ë¶€íŒ… ì‹œê°„ íšê¸°ì  ë‹¨ì¶•!
    scaledown_window=2,  # 2ì´ˆ í›„ ì¢…ë£Œ (ìµœì†Œ ì„¤ì •ê°’)
    concurrency_limit=10,  # ìµœëŒ€ 10ê°œ ì»¨í…Œì´ë„ˆë§Œ ë™ì‹œ ì‹¤í–‰ (GPU ë¦¬ì†ŒìŠ¤ ê´€ë¦¬)
)
class ImageGenerator:
    """
    ì´ë¯¸ì§€ ìƒì„±ì„ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤

    enable_memory_snapshot=Trueë¡œ ì¸í•´:
    - ì²« ë²ˆì§¸ ì»¨í…Œì´ë„ˆ ë¶€íŒ… ì‹œ: ëª¨ë¸ ë¡œë“œ + ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìƒì„± (ëŠë¦¼)
    - ì´í›„ ì»¨í…Œì´ë„ˆë“¤: ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì—ì„œ ë³µì› (ë§¤ìš° ë¹ ë¦„, ìˆ˜ ì´ˆ ì´ë‚´)
    """

    @modal.enter()
    def load_models(self):
        """
        ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        ë² ì´ìŠ¤ ëª¨ë¸ì€ ìš”ì²­ ì‹œ ë™ì ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ ìºì‹±í•©ë‹ˆë‹¤.
        """
        import torch

        # í˜„ì¬ ìƒì„± ì¤‘ì¸ ì‘ì—… ì •ë³´ ì´ˆê¸°í™”
        self.current_generation = None

        # íŒŒì´í”„ë¼ì¸ ìºì‹œ (base_model_id -> pipeline)
        self.pipelines = {}

        # ì™¸ë¶€ VAE ìºì‹œ
        self.external_vae = None

        print("ğŸš€ Image Generator initialized (pipelines will be loaded on-demand)")

    def get_or_load_pipeline(self, base_model: str):
        """
        ë² ì´ìŠ¤ ëª¨ë¸ì— ë”°ë¼ íŒŒì´í”„ë¼ì¸ì„ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë¡œë“œí•©ë‹ˆë‹¤.

        Args:
            base_model: ë² ì´ìŠ¤ ëª¨ë¸ ID

        Returns:
            StableDiffusionPipeline: ë¡œë“œëœ íŒŒì´í”„ë¼ì¸
        """
        from diffusers import StableDiffusionPipeline
        import torch

        # ì´ë¯¸ ë¡œë“œëœ íŒŒì´í”„ë¼ì¸ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
        if base_model in self.pipelines:
            print(f"ğŸš€ Using cached pipeline for: {base_model}")
            return self.pipelines[base_model]

        # ìƒˆë¡œ ë¡œë“œ
        print(f"ğŸ“¦ Loading new base model: {base_model}")

        # Modal Volume ìºì‹± ë¡œì§
        cache_base = "/cache/base_models"
        safe_model_id = base_model.replace("/", "--")
        cached_model_path = os.path.join(cache_base, safe_model_id)

        # ìºì‹œëœ ëª¨ë¸ í™•ì¸
        if os.path.exists(cached_model_path) and os.path.exists(os.path.join(cached_model_path, "model_index.json")):
            base_model_path = cached_model_path
            print(f"âœ… Using cached base model from volume: {base_model_path}")
        else:
            # ìºì‹œê°€ ì—†ìœ¼ë©´ HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ ìºì‹±
            print(f"ğŸ“¥ Downloading base model from HuggingFace: {base_model}")
            print(f"ğŸ’¾ Will cache to: {cached_model_path}")

            temp_pipe = StableDiffusionPipeline.from_pretrained(
                base_model,
                torch_dtype=torch.float16,
                safety_checker=None
            )

            os.makedirs(cache_base, exist_ok=True)
            temp_pipe.save_pretrained(cached_model_path)
            print(f"âœ… Base model cached successfully!")

            base_model_path = cached_model_path

        print(f"Loading base model: {base_model_path}")
        pipe = StableDiffusionPipeline.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            safety_checker=None
        )
        pipe.to("cuda")

        # xFormers ìµœì í™” í™œì„±í™”
        try:
            pipe.enable_xformers_memory_efficient_attention()
            print("âœ… xFormers memory efficient attention enabled")
        except Exception as e:
            print(f"âš ï¸ xFormers not available: {e}")

        # Attention slicing í™œì„±í™”
        try:
            pipe.enable_attention_slicing(slice_size="auto")
            print("âœ… Attention slicing enabled")
        except Exception as e:
            print(f"âš ï¸ Could not enable attention slicing: {e}")

        # ìºì‹œì— ì €ì¥
        self.pipelines[base_model] = pipe
        print(f"âœ… Pipeline cached for: {base_model}")

        return pipe

    def get_or_load_external_vae(self):
        """
        ì™¸ë¶€ VAEë¥¼ ìºì‹±í•´ì„œ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.

        Returns:
            AutoencoderKL: ì™¸ë¶€ VAE ëª¨ë¸
        """
        from diffusers import AutoencoderKL
        import torch

        # ì´ë¯¸ ë©”ëª¨ë¦¬ì— ë¡œë“œë˜ì–´ ìˆìœ¼ë©´ ì¬ì‚¬ìš©
        if self.external_vae is not None:
            print("ğŸš€ Using cached external VAE (in-memory)")
            return self.external_vae

        # Modal Volume ìºì‹œ ê²½ë¡œ
        cache_dir = "/cache/vae_model"

        # Volumeì— ìºì‹œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        if os.path.exists(cache_dir) and os.path.exists(os.path.join(cache_dir, "config.json")):
            print(f"âœ… Loading external VAE from volume cache: {cache_dir}")
            vae = AutoencoderKL.from_pretrained(
                cache_dir,
                torch_dtype=torch.float16
            ).to("cuda")
        else:
            # ìºì‹œê°€ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ í›„ ì €ì¥
            print(f"ğŸ“¥ Downloading external VAE (first time only)...")
            vae = AutoencoderKL.from_pretrained(
                "stabilityai/sd-vae-ft-mse",
                torch_dtype=torch.float16
            ).to("cuda")

            # Modal Volumeì— ì €ì¥
            os.makedirs(cache_dir, exist_ok=True)
            vae.save_pretrained(cache_dir)
            print(f"âœ… External VAE cached to: {cache_dir}")

        # ë©”ëª¨ë¦¬ì— ìºì‹œ
        self.external_vae = vae
        print("âœ… External VAE loaded and cached in memory")

        return vae

    @modal.exit()
    def cleanup_on_exit(self):
        """
        ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì‹œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        ì´ë¯¸ì§€ ìƒì„±ì´ ì™„ë£Œë˜ì§€ ì•Šì€ ìƒíƒœì—ì„œ ì¢…ë£Œë  ê²½ìš° ì‹¤íŒ¨ ë©”ì‹œì§€ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.
        """
        import requests

        # ìƒì„± ì¤‘ì¸ ì‘ì—…ì´ ìˆê³ , ì™„ë£Œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì‹¤íŒ¨ ì½œë°± ì „ì†¡
        if self.current_generation is not None:
            callback_url = self.current_generation.get("callback_url")
            user_id = self.current_generation.get("user_id")
            model_id = self.current_generation.get("model_id")
            history_id = self.current_generation.get("history_id")

            if callback_url:
                try:
                    print(f"âš ï¸ Container shutting down during generation for history {history_id}")
                    print(f"ğŸ“¤ Sending failure callback...")

                    callback_data = {
                        "historyId": history_id,
                        "userId": user_id,
                        "modelId": model_id,
                        "status": "FAIL",
                        "error": "Generation interrupted: Container was shut down or cancelled"
                    }
                    response = requests.post(callback_url, json=callback_data, timeout=10)
                    response.raise_for_status()
                    print(f"âœ… Failure callback sent for interrupted generation {history_id}")
                except Exception as e:
                    print(f"âš ï¸ Failed to send failure callback on exit: {e}")

    @modal.method()
    def generate_images(
        self,
        user_id: str,
        prompt: str,
        lora_model_url: str,
        model_id: int,
        history_id: int = None,
        negative_prompt: str = "low quality, blurry, ugly, distorted, deformed",
        num_images: int = 1,
        steps: int = 40,
        guidance_scale: float = 7.5,
        lora_scale: float = 1.0,
        seed: int = None,
        callback_url: str = None,
        base_model: str = "stablediffusionapi/anything-v5"
    ):
        """
        ì´ë¯¸ì§€ ìƒì„±ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            user_id: ì‚¬ìš©ì ID
            prompt: ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸
            lora_model_url: S3 LoRA ëª¨ë¸ URL
            model_id: ëª¨ë¸ ID
            history_id: GenerationHistory ID
            negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
            num_images: ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜
            steps: ì¶”ë¡  ìŠ¤í…
            guidance_scale: CFG scale
            lora_scale: LoRA ê°•ë„
            seed: ëœë¤ ì‹œë“œ
            callback_url: ì™„ë£Œ ì‹œ ì½œë°± URL
            base_model: ë² ì´ìŠ¤ ëª¨ë¸ ID (ê¸°ë³¸: stablediffusionapi/anything-v5)

        Returns:
            list: S3 í‚¤ ë¦¬ìŠ¤íŠ¸
        """
        from core.config import InferenceConfig
        from core.generate import generate_images
        from urllib.parse import urlparse, unquote
        import requests
        import boto3
        import shutil

        # í˜„ì¬ ìƒì„± ì¤‘ì¸ ì‘ì—… ì •ë³´ ì €ì¥ (ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì‹œ ì‹¤íŒ¨ ì½œë°± ì „ì†¡ìš©)
        self.current_generation = {
            "user_id": user_id,
            "model_id": model_id,
            "history_id": history_id,
            "callback_url": callback_url
        }

        print(f"Generating images for user: {user_id}")
        print(f"Prompt: {prompt}")
        print(f"Number of images: {num_images}")

        # LoRA ëª¨ë¸ ìºì‹œ ê²½ë¡œ
        temp_lora_path = f"{CACHE_DIR}/lora_models/model_{model_id}"

        # ì´ë¯¸ ìºì‹œëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
        if os.path.exists(temp_lora_path):
            safetensors_files = [f for f in os.listdir(temp_lora_path) if f.endswith('.safetensors')]
            if safetensors_files:
                print(f"âœ… Using cached LoRA model (shared): {temp_lora_path}")
                print(f"Cached files: {os.listdir(temp_lora_path)}")
                downloaded_files = [os.path.join(temp_lora_path, f) for f in os.listdir(temp_lora_path)]
            else:
                print(f"âš ï¸ Cache corrupted, re-downloading...")
                downloaded_files = None
        else:
            downloaded_files = None

        if downloaded_files is None:
            # ìºì‹œê°€ ì—†ìœ¼ë©´ S3ì—ì„œ ë‹¤ìš´ë¡œë“œ
            os.makedirs(temp_lora_path, exist_ok=True)
            print(f"Downloading LoRA model from S3...")

            s3_client = boto3.client('s3')
            bucket_name = os.environ.get("AWS_S3_MODELS_BUCKET", "lora-models-bucket")

            # lora_model_url íŒŒì‹± (ë‹¨ì¼ .safetensors íŒŒì¼ ê²½ë¡œ)
            if lora_model_url.startswith('http'):
                # HTTP URLì—ì„œ S3 í‚¤ ì¶”ì¶œ
                parsed = urlparse(lora_model_url)
                path_parts = parsed.path.strip('/').split('/')
                # bucket ì´ë¦„ ë‹¤ìŒë¶€í„°ê°€ S3 í‚¤
                if len(path_parts) >= 2:
                    s3_key = '/'.join(path_parts[1:])
                else:
                    s3_key = path_parts[-1]
                s3_key = unquote(s3_key)
            else:
                # s3://bucket/key í˜•ì‹
                s3_key = lora_model_url.replace(f's3://{bucket_name}/', '')
                s3_key = unquote(s3_key)

            print(f"S3 key: {s3_key}")

            # ë‹¨ì¼ safetensors íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            local_file = os.path.join(temp_lora_path, os.path.basename(s3_key))

            print(f"Downloading: s3://{bucket_name}/{s3_key} -> {local_file}")
            s3_client.download_file(bucket_name, s3_key, local_file)
            downloaded_files = [local_file]

            print(f"âœ… LoRA model downloaded to {temp_lora_path}")

        # .safetensors íŒŒì¼ í™•ì¸
        safetensors_files = [f for f in os.listdir(temp_lora_path) if f.endswith('.safetensors')]
        if not safetensors_files:
            print(f"âŒ ERROR: No .safetensors file found in {temp_lora_path}")
            raise FileNotFoundError(f"No .safetensors file found in {temp_lora_path}")

        # ì¶œë ¥ ë””ë ‰í† ë¦¬
        output_dir = f"{CACHE_DIR}/outputs/{user_id}"
        os.makedirs(output_dir, exist_ok=True)

        config = InferenceConfig(
            model_id=base_model,
            lora_path=temp_lora_path,
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_images=num_images,
            steps=steps,
            guidance_scale=guidance_scale,
            lora_scale=lora_scale,
            seed=seed,
            output_dir=output_dir
        )

        # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (ë°±ì—”ë“œë¡œ POST ìš”ì²­)
        import time
        last_update_time = [0]  # mutable object to track last update time

        def progress_callback(status, current_image, total_images, current_step, total_steps, message):
            """1ì´ˆë§ˆë‹¤ ë°±ì—”ë“œë¡œ ì§„í–‰ë¥  ì „ì†¡"""
            current_time = time.time()

            # 1ì´ˆë§ˆë‹¤ë§Œ ì „ì†¡ (ì²« í˜¸ì¶œì€ ì¦‰ì‹œ ì „ì†¡)
            if current_time - last_update_time[0] < 1.0 and last_update_time[0] != 0:
                return

            last_update_time[0] = current_time

            if history_id and callback_url:
                try:
                    progress_data = {
                        "historyId": history_id,
                        "status": "GENERATING",
                        "currentStep": current_step,
                        "totalSteps": total_steps,
                        "message": message
                    }
                    requests.post(callback_url, json=progress_data, timeout=2)
                    print(f"ğŸ“Š Progress sent to backend: {message} ({current_step}/{total_steps})")
                except Exception as e:
                    print(f"âš ï¸ Failed to send progress: {e}")

        # ë² ì´ìŠ¤ ëª¨ë¸ì— ë”°ë¼ íŒŒì´í”„ë¼ì¸ ê°€ì ¸ì˜¤ê¸° (ë™ì  ìºì‹±)
        pipe = self.get_or_load_pipeline(base_model)

        # Reze ëª¨ë¸ íŠ¹ë³„ ì²˜ë¦¬ (0/Reze.safetensors)
        is_reze_model = "0/Reze.safetensors" in lora_model_url
        print(f"ğŸ” Model: {'Reze (ì›ë³¸ VAE)' if is_reze_model else 'ì¼ë°˜ ëª¨ë¸ (ì™¸ë¶€ VAE)'}")

        # VAE ë™ì  êµì²´
        if is_reze_model:
            # Reze ëª¨ë¸: ì›ë³¸ VAE ìœ ì§€
            print("âœ… Using original VAE for Reze model")
        else:
            # ë‚˜ë¨¸ì§€ ëª¨ë¸: ì™¸ë¶€ VAE ì‚¬ìš© (ìºì‹±ë¨)
            vae = self.get_or_load_external_vae()
            pipe.vae = vae

        # íŒŒì´í”„ë¼ì¸ì— LoRA ë¡œë“œ
        print(f"Loading LoRA weights: {temp_lora_path}")

        # ê¸°ì¡´ LoRA ì–¸ë¡œë“œ (ìˆë‹¤ë©´)
        try:
            pipe.unload_lora_weights()
        except:
            pass

        # LoRA ë¡œë“œ
        safetensors_files = [f for f in os.listdir(temp_lora_path) if f.endswith('.safetensors')]
        lora_file = safetensors_files[0]
        print(f"Found LoRA file: {lora_file}")
        pipe.load_lora_weights(
            temp_lora_path,
            weight_name=lora_file
        )
        print("âœ… LoRA loaded successfully!")

        # ì´ë¯¸ì§€ ìƒì„± (ì‚¬ì „ ë¡œë“œëœ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©)
        try:
            from datetime import datetime
            from pathlib import Path
            import torch

            full_prompt = prompt
            print("="*60)
            print(f"Generating {num_images} image(s)")
            print(f"Prompt: {full_prompt}")
            print(f"Negative: {negative_prompt}")
            print(f"Steps: {steps} | CFG Scale: {guidance_scale}")
            if seed is not None:
                print(f"Seed: {seed}")
            print("="*60)

            # ì‹œë“œ ì„¤ì •
            generator = None
            if seed is not None:
                generator = torch.Generator(device="cuda").manual_seed(seed)

            # ì´ë¯¸ì§€ ìƒì„±
            generated_files = []

            # ìƒì„± ì‹œì‘ ì½œë°±
            if progress_callback:
                progress_callback(
                    status="GENERATING",
                    current_image=0,
                    total_images=num_images,
                    current_step=0,
                    total_steps=steps,
                    message=f"ì´ë¯¸ì§€ ìƒì„± ì‹œì‘... (0/{num_images})"
                )

            for i in range(num_images):
                print(f"\n[{i+1}/{num_images}] Generating...")

                # Stepë³„ ì½œë°± í•¨ìˆ˜ ì •ì˜
                def step_callback(pipe_instance, step_index, timestep, callback_kwargs):
                    if progress_callback:
                        progress_callback(
                            status="GENERATING",
                            current_image=i + 1,
                            total_images=num_images,
                            current_step=step_index + 1,
                            total_steps=steps,
                            message=f"ì´ë¯¸ì§€ {i+1}/{num_images} ìƒì„± ì¤‘... (step {step_index+1}/{steps})"
                        )
                    return callback_kwargs

                with torch.no_grad():
                    image = pipe(
                        prompt=full_prompt,
                        negative_prompt=negative_prompt,
                        num_inference_steps=steps,
                        guidance_scale=guidance_scale,
                        generator=generator,
                        callback_on_step_end=step_callback,
                        callback_on_step_end_tensor_inputs=["latents"],
                        cross_attention_kwargs={"scale": lora_scale}
                    ).images[0]

                # íŒŒì¼ëª… ìƒì„±
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"{timestamp}_{i+1}.png"
                output_path = Path(output_dir) / filename

                # ì €ì¥
                image.save(output_path)
                generated_files.append(str(output_path))
                print(f"âœ… Saved: {output_path}")

                # ì´ë¯¸ì§€ ì™„ë£Œ ì½œë°±
                if progress_callback:
                    progress_callback(
                        status="GENERATING",
                        current_image=i + 1,
                        total_images=num_images,
                        current_step=steps,
                        total_steps=steps,
                        message=f"ì´ë¯¸ì§€ {i+1}/{num_images} ì™„ë£Œ"
                    )

            print("\n" + "="*60)
            print(f"âœ… Successfully generated {len(generated_files)} image(s)")
            print(f"ğŸ“ Output folder: {output_dir}")
            print("="*60)

            # S3 ì—…ë¡œë“œ
            print("Uploading generated images to S3...")
            s3_client = boto3.client('s3')
            bucket_name = os.environ.get("AWS_S3_IMAGES_BUCKET", "lora-generated-image-bucket")

            s3_keys = []
            for generated_file in generated_files:
                filename = os.path.basename(generated_file)
                s3_key = f"user-{user_id}/{filename}"

                s3_client.upload_file(
                    generated_file,
                    bucket_name,
                    s3_key,
                    ExtraArgs={'ContentType': 'image/png'}
                )

                s3_keys.append(s3_key)
                print(f"âœ… Uploaded: s3://{bucket_name}/{s3_key}")

            # ì½œë°± í˜¸ì¶œ
            if callback_url:
                try:
                    callback_data = {
                        "historyId": history_id,
                        "userId": user_id,
                        "modelId": model_id,
                        "prompt": prompt,
                        "negativePrompt": negative_prompt,
                        "steps": steps,
                        "guidanceScale": guidance_scale,
                        "seed": seed,
                        "numImages": num_images,
                        "imageS3Keys": s3_keys,
                        "status": "SUCCESS"
                    }
                    response = requests.post(callback_url, json=callback_data, timeout=10)
                    response.raise_for_status()
                    print(f"âœ… Callback successful: {callback_url}")
                except Exception as e:
                    print(f"âŒ Callback failed: {e}")

            # ì§„í–‰ë¥  ë”•ì…”ë„ˆë¦¬ì—ì„œ ì œê±° (ì™„ë£Œë¨)
            if history_id and history_id in generation_progress:
                del generation_progress[history_id]

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (LoRA ëª¨ë¸ì€ ìºì‹œë¡œ ìœ ì§€)
            shutil.rmtree(output_dir, ignore_errors=True)

            # ìƒì„± ì™„ë£Œ - current_generation ì´ˆê¸°í™” (ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì‹œ ì¤‘ë³µ ì‹¤íŒ¨ ì½œë°± ë°©ì§€)
            self.current_generation = None

            return s3_keys

        except Exception as e:
            print(f"âŒ Image generation failed: {e}")

            # ì‹¤íŒ¨ ì½œë°±
            if callback_url:
                try:
                    callback_data = {
                        "historyId": history_id,
                        "userId": user_id,
                        "modelId": model_id,
                        "status": "FAIL",
                        "error": str(e)
                    }
                    requests.post(callback_url, json=callback_data, timeout=10)
                except Exception as cb_e:
                    print(f"âš ï¸ Failed to send failure callback: {cb_e}")
            print(f"âŒ Failure callback sent to backend for history {history_id}.")

            # ì§„í–‰ë¥  ë”•ì…”ë„ˆë¦¬ì—ì„œ ì œê±° (ì‹¤íŒ¨í•¨)
            if history_id and history_id in generation_progress:
                del generation_progress[history_id]

            shutil.rmtree(output_dir, ignore_errors=True)

            # ìƒì„± ì‹¤íŒ¨ - current_generation ì´ˆê¸°í™” (ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì‹œ ì¤‘ë³µ ì‹¤íŒ¨ ì½œë°± ë°©ì§€)
            self.current_generation = None

            raise


# ì „ì—­ ìƒíƒœ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬ (SSE ì§„í–‰ë¥  ìŠ¤íŠ¸ë¦¬ë°)
# {history_id: {"status": "GENERATING", "current_step": 1, "total_steps": 30, ...}}
generation_progress = {}

# FastAPI ì›¹ ì„œë²„
@app.function(
    image=image,
    secrets=[secrets],
    min_containers=1,  # í•­ìƒ 1ê°œ ì¸ìŠ¤í„´ìŠ¤ ìœ ì§€
)
@modal.asgi_app()
def fastapi_app():
    """
    FastAPI ì›¹ ì„œë²„ (Modal ë°°í¬)
    """
    from fastapi import FastAPI, BackgroundTasks, Request
    from fastapi.responses import JSONResponse, StreamingResponse
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel, Field
    from typing import Optional, List
    import asyncio
    import json

    web_app = FastAPI(
        title="LoRA Training and Inference API (Modal)",
        description="Modalì—ì„œ ì‹¤í–‰ë˜ëŠ” ì„œë²„ë¦¬ìŠ¤ GPU ê¸°ë°˜ LoRA í•™ìŠµ ë° ì´ë¯¸ì§€ ìƒì„± API",
        version="2.0.0",
    )

    # CORS ì„¤ì •
    web_app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš©
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Pydantic ëª¨ë¸
    class TrainRequest(BaseModel):
        user_id: str = Field(..., description="ì‚¬ìš©ì ID")
        model_id: Optional[int] = Field(None, description="ëª¨ë¸ ID (í•™ìŠµ ì™„ë£Œ í›„ ìƒì„±)")
        job_id: int = Field(..., description="í•™ìŠµ ì‘ì—… ID (ê³ ìœ  ì‹ë³„ì)")
        model_name: str = Field(..., description="ëª¨ë¸ ì´ë¦„")
        training_image_urls: List[str] = Field(..., description="S3 í•™ìŠµ ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸")
        callback_url: Optional[str] = Field(None, description="í•™ìŠµ ì™„ë£Œ ì‹œ ì½œë°± URL")
        # í•™ìŠµ íŒŒë¼ë¯¸í„° (ì„ íƒì )
        trigger_word: Optional[str] = Field(None, description="íŠ¸ë¦¬ê±° ì›Œë“œ")
        epochs: Optional[int] = Field(250, description="í•™ìŠµ ì—í¬í¬ ìˆ˜")
        learning_rate: Optional[float] = Field(2e-5, description="í•™ìŠµë¥ ")
        lora_rank: Optional[int] = Field(32, description="LoRA Rank")
        base_model: Optional[str] = Field("stablediffusionapi/anything-v5", description="ë² ì´ìŠ¤ ëª¨ë¸")
        skip_preprocessing: Optional[bool] = Field(False, description="ì „ì²˜ë¦¬ ìŠ¤í‚µ ì—¬ë¶€ (ìº¡ì…”ë‹ì€ í•­ìƒ ìˆ˜í–‰)")

    class GenerateRequest(BaseModel):
        user_id: str = Field(..., description="ì‚¬ìš©ì ID")
        model_id: int = Field(..., description="ëª¨ë¸ ID")
        history_id: Optional[int] = Field(None, description="GenerationHistory ID")
        prompt: str = Field(..., description="ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸")
        lora_model_url: str = Field(..., description="S3 LoRA ëª¨ë¸ URL")
        negative_prompt: Optional[str] = Field("low quality, blurry, ugly, distorted, deformed")
        num_images: int = Field(1, description="ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜")
        steps: int = Field(40, description="ì¶”ë¡  ìŠ¤í…")
        guidance_scale: float = Field(7.5, description="CFG scale")
        lora_scale: float = Field(1.0, description="LoRA ê°•ë„")
        seed: Optional[int] = Field(None, description="ëœë¤ ì‹œë“œ")
        callback_url: Optional[str] = Field(None, description="ì™„ë£Œ ì‹œ ì½œë°± URL")
        base_model: Optional[str] = Field("stablediffusionapi/anything-v5", description="ë² ì´ìŠ¤ ëª¨ë¸ ID")

    class MessageResponse(BaseModel):
        message: str

    # ì—”ë“œí¬ì¸íŠ¸
    @web_app.get("/")
    def root():
        return {"message": "LoRA Modal API is running"}

    @web_app.post("/train")
    async def start_training(req: TrainRequest):
        """í•™ìŠµ ì‹œì‘ (ë¹„ë™ê¸°)"""
        try:
            # LoraTrainer í´ë˜ìŠ¤ ë©”ì„œë“œ í˜¸ì¶œ (spawnìœ¼ë¡œ ë¹„ë™ê¸° ì‹¤í–‰)
            trainer = LoraTrainer()
            trainer.train_lora.spawn(
                user_id=req.user_id,
                model_id=req.model_id,
                job_id=req.job_id,
                model_name=req.model_name,
                training_image_urls=req.training_image_urls,
                callback_url=req.callback_url,
                trigger_word=req.trigger_word,
                epochs=req.epochs,
                learning_rate=req.learning_rate,
                lora_rank=req.lora_rank,
                base_model=req.base_model,
                skip_preprocessing=req.skip_preprocessing
            )

            return {"message": f"Training started on Modal GPU (T4) for job {req.job_id}"}

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"message": f"Failed to start training: {str(e)}"}
            )

    @web_app.post("/generate")
    async def generate_images_api(req: GenerateRequest):
        """ì´ë¯¸ì§€ ìƒì„± (ë¹„ë™ê¸°)"""
        try:
            # ImageGenerator í´ë˜ìŠ¤ ë©”ì„œë“œ í˜¸ì¶œ (spawnìœ¼ë¡œ ë¹„ë™ê¸° ì‹¤í–‰)
            generator = ImageGenerator()
            call = generator.generate_images.spawn(
                user_id=req.user_id,
                model_id=req.model_id,
                history_id=req.history_id,
                prompt=req.prompt,
                lora_model_url=req.lora_model_url,
                negative_prompt=req.negative_prompt,
                num_images=req.num_images,
                steps=req.steps,
                guidance_scale=req.guidance_scale,
                lora_scale=req.lora_scale,
                seed=req.seed,
                callback_url=req.callback_url,
                base_model=req.base_model
            )

            return {"message": "Image generation started on Modal GPU (T4)", "call_id": call.object_id}

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"message": f"Failed to start generation: {str(e)}"}
            )

    @web_app.get("/generate/stream")
    async def stream_generation_progress():
        """SSE ìŠ¤íŠ¸ë¦¬ë° - ì´ë¯¸ì§€ ìƒì„± ì§„í–‰ë¥ """
        async def event_generator():
            last_sent = {}

            while True:
                # ì§„í–‰ ì¤‘ì¸ ì‘ì—…ì´ ìˆëŠ”ì§€ í™•ì¸
                if generation_progress:
                    for history_id, progress in generation_progress.items():
                        # ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ ì „ì†¡
                        if last_sent.get(history_id) != progress:
                            data = {
                                "status": "IN_PROGRESS",
                                "historyId": history_id,
                                "current_step": progress.get("current_step", 0),
                                "total_steps": progress.get("total_steps", 0),
                                "message": progress.get("message", "Generating...")
                            }
                            yield f"data: {json.dumps(data)}\n\n"
                            last_sent[history_id] = progress.copy()

                await asyncio.sleep(0.5)  # 0.5ì´ˆë§ˆë‹¤ ì²´í¬

        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"  # Nginx ë²„í¼ë§ ë¹„í™œì„±í™”
            }
        )

    @web_app.get("/health")
    def health_check():
        return {"status": "healthy"}

    return web_app
