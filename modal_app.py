"""
Modal ë°°í¬ íŒŒì¼
LoRA í•™ìŠµ ë° ì´ë¯¸ì§€ ìƒì„±ì„ ìœ„í•œ ì„œë²„ë¦¬ìŠ¤ GPU ì• í”Œë¦¬ì¼€ì´ì…˜
"""

import modal
import os
from pathlib import Path

# Modal ì•± ìƒì„±
app = modal.App("lora-training-inference")

# ë² ì´ìŠ¤ ì´ë¯¸ì§€ ë¹Œë“œ í•¨ìˆ˜ (ë² ì´ìŠ¤ ëª¨ë¸ í¬í•¨)
def download_base_model_to_image():
    """
    ì´ë¯¸ì§€ ë¹Œë“œ ì‹œ ë² ì´ìŠ¤ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì´ë¯¸ì§€ì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.
    ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹œê°„ì„ ëŒ€í­ ë‹¨ì¶•ì‹œí‚µë‹ˆë‹¤.
    """
    from diffusers import StableDiffusionPipeline
    import torch

    model_id = "stablediffusionapi/anything-v5"
    cache_dir = "/base_models/anything-v5"

    print(f"ğŸ“¥ Downloading base model: {model_id}")

    pipe = StableDiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        safety_checker=None,
        cache_dir=cache_dir
    )

    # ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥
    pipe.save_pretrained(cache_dir)

    print(f"âœ… Base model cached to: {cache_dir}")


# ë² ì´ìŠ¤ ì´ë¯¸ì§€ (ë¼ì´ë¸ŒëŸ¬ë¦¬ + ë² ì´ìŠ¤ ëª¨ë¸ í¬í•¨ - ìºì‹± ìµœì í™”)
# íŒ¨í‚¤ì§€ì™€ ë² ì´ìŠ¤ ëª¨ë¸ì€ ê±°ì˜ ë³€ê²½ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ í•œ ë²ˆ ë¹Œë“œë˜ë©´ ê³„ì† ì¬ì‚¬ìš©ë¨
base_image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("git", "libgl1-mesa-glx", "libglib2.0-0")
    .pip_install(
        "torch>=2.0.0",
        "torchvision",
        "diffusers>=0.28.0",
        "transformers>=4.35.0",
        "accelerate>=0.24.0",
        "peft>=0.7.0",
        "safetensors>=0.4.0",
        "bitsandbytes>=0.41.0",
        "Pillow>=10.0.0",
        "opencv-python>=4.8.0",
        "rembg>=2.0.0",
        "onnxruntime>=1.16.0",
        "easyocr>=1.7.0",
        "numpy>=1.24.0",
        "tqdm>=4.66.0",
        "datasets>=2.14.0",
        "fastapi>=0.104.0",
        "uvicorn[standard]>=0.24.0",
        "boto3>=1.34.0",
        "requests>=2.31.0",
    )
    # ë² ì´ìŠ¤ ëª¨ë¸ì„ ì´ë¯¸ì§€ì— í¬í•¨ (ì²« ë¹Œë“œ ì‹œ ì‹œê°„ ê±¸ë¦¬ì§€ë§Œ ì´í›„ ë§¤ìš° ë¹ ë¦„)
    .run_function(download_base_model_to_image)
)

# ìµœì¢… ì´ë¯¸ì§€ (core ë””ë ‰í† ë¦¬ ì¶”ê°€ - ìì£¼ ë³€ê²½ë˜ëŠ” íŒŒì¼)
# coreê°€ ë³€ê²½ë˜ì–´ë„ base_imageëŠ” ì¬ì‚¬ìš©ë¨
image = base_image.add_local_dir(
    local_path=str(Path(__file__).parent / "core"),
    remote_path="/root/core"
)

# Modal Volume ì„¤ì • (ëª¨ë¸ ìºì‹±ìš©)
volume = modal.Volume.from_name("lora-models", create_if_missing=True)
BASE_MODEL_PATH = "/base_models"
CACHE_DIR = "/cache"

# AWS Secrets ì„¤ì •
secrets = modal.Secret.from_name("lora-secrets")

# ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ (ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰)
@app.function(
    image=image,
    volumes={BASE_MODEL_PATH: volume},
    timeout=3600,  # 1ì‹œê°„
    secrets=[secrets]
)
def download_base_model():
    """
    ë² ì´ìŠ¤ Stable Diffusion ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ìºì‹±í•©ë‹ˆë‹¤.
    """
    from diffusers import StableDiffusionPipeline
    import torch

    model_id = "stablediffusionapi/anything-v5"
    local_path = f"{BASE_MODEL_PATH}/anything-v5"

    print(f"Downloading base model: {model_id}")

    # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° ìŠ¤í‚µ
    if os.path.exists(local_path):
        print(f"âœ… Base model already cached at {local_path}")
        return local_path

    # ë‹¤ìš´ë¡œë“œ
    pipe = StableDiffusionPipeline.from_pretrained(
        model_id,
        torch_dtype=torch.float16,
        safety_checker=None
    )

    # ì €ì¥
    pipe.save_pretrained(local_path)
    volume.commit()

    print(f"âœ… Base model downloaded to {local_path}")
    return local_path


# LoRA í•™ìŠµ í´ë˜ìŠ¤ (GPU A10G ì‚¬ìš©)
@app.cls(
    image=image,
    gpu="A10G",  # í•™ìŠµìš© GPU
    timeout=7200,  # 2ì‹œê°„
    volumes={
        CACHE_DIR: modal.Volume.from_name("lora-cache", create_if_missing=True)
    },
    secrets=[secrets],
    memory=32768,  # 32GB RAM
    enable_memory_snapshot=True,  # ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· í™œì„±í™” - ë¶€íŒ… ì‹œê°„ íšê¸°ì  ë‹¨ì¶•!
)
class LoraTrainer:
    """
    LoRA í•™ìŠµì„ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤

    enable_memory_snapshot=Trueë¡œ ì¸í•´:
    - ì²« ë²ˆì§¸ ì»¨í…Œì´ë„ˆ ë¶€íŒ… ì‹œ: ëª¨ë¸ ë¡œë“œ + ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìƒì„± (ëŠë¦¼)
    - ì´í›„ ì»¨í…Œì´ë„ˆë“¤: ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì—ì„œ ë³µì› (ë§¤ìš° ë¹ ë¦„, ìˆ˜ ì´ˆ ì´ë‚´)
    """

    @modal.enter()
    def load_models(self):
        """
        ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹œ ë² ì´ìŠ¤ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.
        ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì´ í™œì„±í™”ë˜ì–´ ìˆì–´ ì´ ì´ˆê¸°í™”ëŠ” í•œ ë²ˆë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
        """
        from diffusers import StableDiffusionPipeline
        import torch

        print("ğŸš€ Initializing LoRA Trainer...")

        # ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ
        self.base_model_path = "/base_models/anything-v5"

        if not os.path.exists(self.base_model_path):
            print("âš ï¸ Base model not found in image, using HuggingFace")
            self.base_model_path = "stablediffusionapi/anything-v5"

        print(f"ğŸ“¦ Loading base model from: {self.base_model_path}")

        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ (ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì— í¬í•¨ë¨)
        self.pipe = StableDiffusionPipeline.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            safety_checker=None
        )
        self.pipe.to("cuda")

        print("âœ… Base model loaded and ready!")
        print("ğŸ’¾ Memory snapshot will be created after this initialization")

    @modal.method()
    def train_lora(
        self,
        user_id: str,
        model_id: int,
        model_name: str,
        training_image_urls: list[str],
        callback_url: str = None
    ):
        """
        LoRA í•™ìŠµì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            user_id: ì‚¬ìš©ì ID
            model_id: ëª¨ë¸ ID
            model_name: ëª¨ë¸ ì´ë¦„
            training_image_urls: S3 presigned URL ë¦¬ìŠ¤íŠ¸
            callback_url: ì™„ë£Œ ì‹œ í˜¸ì¶œí•  ì½œë°± URL

        Returns:
            dict: í•™ìŠµ ê²°ê³¼
        """
        from core.config import TrainingConfig
        from core.train import train_with_preprocessing
        import requests
        import shutil
        import boto3
        import time

        print(f"Starting training for user: {user_id}, model: {model_name}")
        print(f"Number of training images: {len(training_image_urls)}")

        # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜
        def send_progress_callback(status, message):
            """ë°±ì—”ë“œë¡œ ì§„í–‰ë¥  ì „ì†¡"""
            if callback_url:
                try:
                    progress_data = {
                        "userId": user_id,
                        "modelId": model_id,
                        "status": status,
                        "message": message
                    }
                    requests.post(callback_url, json=progress_data, timeout=2)
                    print(f"ğŸ“Š Progress sent: {status} - {message}")
                except Exception as e:
                    print(f"âš ï¸ Failed to send progress: {e}")

        # 1. ì„œë²„ ë¡œë“œ ì¤‘
        send_progress_callback("LOADING", "Loading server")
        time.sleep(0.5)

        # 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ì¤‘ (ë‹¤ìš´ë¡œë“œ)
        send_progress_callback("PREPROCESSING", "Preprocessing images")

        # S3 ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        temp_dataset_path = f"{CACHE_DIR}/dataset_{user_id}_{model_name}"
        os.makedirs(temp_dataset_path, exist_ok=True)

        print("Downloading training images from S3...")
        for idx, url in enumerate(training_image_urls):
            ext = ".jpg"
            if ".png" in url.lower():
                ext = ".png"

            local_path = os.path.join(temp_dataset_path, f"image_{idx:04d}{ext}")
            response = requests.get(url, stream=True)
            response.raise_for_status()

            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

        print(f"Downloaded {len(training_image_urls)} images")

        # í•™ìŠµ ì„¤ì •
        output_dir = f"{CACHE_DIR}/models/{user_id}/{model_name}"

        config = TrainingConfig(
            raw_dataset_path=temp_dataset_path,
            output_dir=output_dir,
            model_id=self.base_model_path
        )

        # í•™ìŠµ ì½œë°± í•¨ìˆ˜ (ì—í¬í¬ë³„ ì§„í–‰ë¥ )
        def training_callback(status, phase, current_epoch, total_epochs, message):
            """í•™ìŠµ ì§„í–‰ë¥  ì½œë°±"""
            if phase == "training" and current_epoch > 0:
                progress_message = f"Training {current_epoch}/{total_epochs}"
                send_progress_callback("TRAINING", progress_message)

        # í•™ìŠµ ì‹¤í–‰
        try:
            # 3. í•™ìŠµ ì‹œì‘
            send_progress_callback("TRAINING", "Training 0/{}".format(config.num_epochs))

            train_result = train_with_preprocessing(
                raw_dataset_path=temp_dataset_path,
                output_dir=output_dir,
                config=config,
                skip_preprocessing=False,
                callback=training_callback
            )

            # í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ ì°¾ê¸° (ìµœì¢… checkpointì˜ safetensors íŒŒì¼)
            # output_dir/checkpoint-{ìµœì¢…epoch}/lora_weights.safetensors
            import glob
            checkpoint_dirs = glob.glob(os.path.join(output_dir, "checkpoint-*"))
            if not checkpoint_dirs:
                raise FileNotFoundError(f"No checkpoint found in {output_dir}")

            # ê°€ì¥ ë†’ì€ ì—í¬í¬ ë²ˆí˜¸ì˜ checkpoint ì°¾ê¸°
            latest_checkpoint = max(checkpoint_dirs, key=lambda x: int(x.split('-')[-1]))
            model_file_path = os.path.join(latest_checkpoint, "lora_weights.safetensors")

            if not os.path.exists(model_file_path):
                raise FileNotFoundError(f"LoRA weights not found at {model_file_path}")

            print(f"Found trained model: {model_file_path}")

            # 4. ëª¨ë¸ ì—…ë¡œë“œ ì¤‘
            send_progress_callback("UPLOADING", "Uploading model")

            # S3 ì—…ë¡œë“œ (ë‹¨ì¼ .safetensors íŒŒì¼ë§Œ - Civitai ë°©ì‹)
            print("Uploading trained model to S3...")
            s3_client = boto3.client('s3')
            bucket_name = os.environ.get("AWS_S3_MODELS_BUCKET", "lora-models-bucket")

            # S3 í‚¤: model-{modelId}/{modelName}.safetensors
            s3_model_key = f"model-{model_id}/{model_name}.safetensors"
            s3_client.upload_file(
                model_file_path,
                bucket_name,
                s3_model_key,
                ExtraArgs={'ContentType': 'application/octet-stream'}
            )
            print(f"âœ… Uploaded model (WebUI format): s3://{bucket_name}/{s3_model_key}")

            file_size = os.path.getsize(model_file_path)

            # ì½œë°± í˜¸ì¶œ
            if callback_url:
                try:
                    callback_data = {
                        "userId": user_id,
                        "modelId": model_id,
                        "modelName": model_name,
                        "s3ModelKey": s3_model_key,
                        "fileSize": file_size,
                        "status": "SUCCESS"
                    }
                    response = requests.post(callback_url, json=callback_data, timeout=10)
                    response.raise_for_status()
                    print(f"âœ… Callback successful: {callback_url}")
                except Exception as e:
                    print(f"âŒ Callback failed: {e}")

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            shutil.rmtree(temp_dataset_path, ignore_errors=True)
            shutil.rmtree(output_dir, ignore_errors=True)

            return {
                "status": "SUCCESS",
                "s3_model_key": s3_model_key,
                "file_size": file_size,
                **train_result
            }

        except Exception as e:
            print(f"âŒ Training failed: {e}")

            # ì‹¤íŒ¨ ì½œë°±
            if callback_url:
                try:
                    callback_data = {
                        "userId": user_id,
                        "modelId": model_id,
                        "modelName": model_name,
                        "status": "FAIL",
                        "error": str(e)
                    }
                    requests.post(callback_url, json=callback_data, timeout=10)
                    print(f"âŒ Failure callback sent to backend for model {model_id}.")
                except Exception as cb_e:
                    print(f"âš ï¸ Failed to send failure callback: {cb_e}")

            # ì •ë¦¬
            shutil.rmtree(temp_dataset_path, ignore_errors=True)
            shutil.rmtree(output_dir, ignore_errors=True)

            raise


# ì´ë¯¸ì§€ ìƒì„± í´ë˜ìŠ¤ (GPU T4 ì‚¬ìš©)
@app.cls(
    image=image,
    gpu="T4",  # ìƒì„±ìš© GPU (T4 ì‚¬ìš©)
    timeout=600,  # 10ë¶„
    volumes={
        CACHE_DIR: modal.Volume.from_name("lora-cache", create_if_missing=True)
    },
    secrets=[secrets],
    memory=16384,  # 16GB RAM
    enable_memory_snapshot=True,  # ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· í™œì„±í™” - ë¶€íŒ… ì‹œê°„ íšê¸°ì  ë‹¨ì¶•!
)
class ImageGenerator:
    """
    ì´ë¯¸ì§€ ìƒì„±ì„ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤

    enable_memory_snapshot=Trueë¡œ ì¸í•´:
    - ì²« ë²ˆì§¸ ì»¨í…Œì´ë„ˆ ë¶€íŒ… ì‹œ: ëª¨ë¸ ë¡œë“œ + ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìƒì„± (ëŠë¦¼)
    - ì´í›„ ì»¨í…Œì´ë„ˆë“¤: ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì—ì„œ ë³µì› (ë§¤ìš° ë¹ ë¦„, ìˆ˜ ì´ˆ ì´ë‚´)
    """

    @modal.enter()
    def load_models(self):
        """
        ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹œ ë² ì´ìŠ¤ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.
        ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì´ í™œì„±í™”ë˜ì–´ ìˆì–´ ì´ ì´ˆê¸°í™”ëŠ” í•œ ë²ˆë§Œ ìˆ˜í–‰ë©ë‹ˆë‹¤.
        """
        from diffusers import StableDiffusionPipeline
        import torch

        print("ğŸš€ Initializing Image Generator...")

        # ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ
        self.base_model_path = "/base_models/anything-v5"

        if not os.path.exists(self.base_model_path):
            print("âš ï¸ Base model not found in image, using HuggingFace")
            self.base_model_path = "stablediffusionapi/anything-v5"

        print(f"ğŸ“¦ Loading base model from: {self.base_model_path}")

        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ (ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì— í¬í•¨ë¨)
        self.pipe = StableDiffusionPipeline.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            safety_checker=None
        )
        self.pipe.to("cuda")

        print("âœ… Base model loaded and ready!")
        print("ğŸ’¾ Memory snapshot will be created after this initialization")

    @modal.method()
    def generate_images(
        self,
        user_id: str,
        prompt: str,
        lora_model_url: str,
        model_id: int,
        history_id: int = None,
        negative_prompt: str = "low quality, blurry, ugly, distorted, deformed",
        num_images: int = 1,
        steps: int = 40,
        guidance_scale: float = 7.5,
        seed: int = None,
        callback_url: str = None
    ):
        """
        ì´ë¯¸ì§€ ìƒì„±ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.

        Args:
            user_id: ì‚¬ìš©ì ID
            prompt: ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸
            lora_model_url: S3 LoRA ëª¨ë¸ URL
            model_id: ëª¨ë¸ ID
            history_id: GenerationHistory ID
            negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
            num_images: ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜
            steps: ì¶”ë¡  ìŠ¤í…
            guidance_scale: CFG scale
            seed: ëœë¤ ì‹œë“œ
            callback_url: ì™„ë£Œ ì‹œ ì½œë°± URL

        Returns:
            list: S3 í‚¤ ë¦¬ìŠ¤íŠ¸
        """
        from core.config import InferenceConfig
        from core.generate import generate_images
        from urllib.parse import urlparse, unquote
        import requests
        import boto3
        import shutil

        print(f"Generating images for user: {user_id}")
        print(f"Prompt: {prompt}")
        print(f"Number of images: {num_images}")

        # LoRA ëª¨ë¸ ìºì‹œ ê²½ë¡œ
        temp_lora_path = f"{CACHE_DIR}/lora_models/model_{model_id}"

        # ì´ë¯¸ ìºì‹œëœ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
        if os.path.exists(temp_lora_path):
            safetensors_files = [f for f in os.listdir(temp_lora_path) if f.endswith('.safetensors')]
            if safetensors_files:
                print(f"âœ… Using cached LoRA model (shared): {temp_lora_path}")
                print(f"Cached files: {os.listdir(temp_lora_path)}")
                downloaded_files = [os.path.join(temp_lora_path, f) for f in os.listdir(temp_lora_path)]
            else:
                print(f"âš ï¸ Cache corrupted, re-downloading...")
                downloaded_files = None
        else:
            downloaded_files = None

        if downloaded_files is None:
            # ìºì‹œê°€ ì—†ìœ¼ë©´ S3ì—ì„œ ë‹¤ìš´ë¡œë“œ
            os.makedirs(temp_lora_path, exist_ok=True)
            print(f"Downloading LoRA model from S3...")

            s3_client = boto3.client('s3')
            bucket_name = os.environ.get("AWS_S3_MODELS_BUCKET", "lora-models-bucket")

            # lora_model_url íŒŒì‹± (ë‹¨ì¼ .safetensors íŒŒì¼ ê²½ë¡œ)
            if lora_model_url.startswith('http'):
                # HTTP URLì—ì„œ S3 í‚¤ ì¶”ì¶œ
                parsed = urlparse(lora_model_url)
                path_parts = parsed.path.strip('/').split('/')
                # bucket ì´ë¦„ ë‹¤ìŒë¶€í„°ê°€ S3 í‚¤
                if len(path_parts) >= 2:
                    s3_key = '/'.join(path_parts[1:])
                else:
                    s3_key = path_parts[-1]
                s3_key = unquote(s3_key)
            else:
                # s3://bucket/key í˜•ì‹
                s3_key = lora_model_url.replace(f's3://{bucket_name}/', '')
                s3_key = unquote(s3_key)

            print(f"S3 key: {s3_key}")

            # ë‹¨ì¼ safetensors íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            local_file = os.path.join(temp_lora_path, os.path.basename(s3_key))

            print(f"Downloading: s3://{bucket_name}/{s3_key} -> {local_file}")
            s3_client.download_file(bucket_name, s3_key, local_file)
            downloaded_files = [local_file]

            print(f"âœ… LoRA model downloaded to {temp_lora_path}")

        # .safetensors íŒŒì¼ í™•ì¸
        safetensors_files = [f for f in os.listdir(temp_lora_path) if f.endswith('.safetensors')]
        if not safetensors_files:
            print(f"âŒ ERROR: No .safetensors file found in {temp_lora_path}")
            raise FileNotFoundError(f"No .safetensors file found in {temp_lora_path}")

        # ì¶œë ¥ ë””ë ‰í† ë¦¬
        output_dir = f"{CACHE_DIR}/outputs/{user_id}"
        os.makedirs(output_dir, exist_ok=True)

        config = InferenceConfig(
            model_id=self.base_model_path,
            lora_path=temp_lora_path,
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_images=num_images,
            steps=steps,
            guidance_scale=guidance_scale,
            seed=seed,
            output_dir=output_dir
        )

        # ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (ë°±ì—”ë“œë¡œ POST ìš”ì²­)
        import time
        last_update_time = [0]  # mutable object to track last update time

        def progress_callback(status, current_image, total_images, current_step, total_steps, message):
            """1ì´ˆë§ˆë‹¤ ë°±ì—”ë“œë¡œ ì§„í–‰ë¥  ì „ì†¡"""
            current_time = time.time()

            # 1ì´ˆë§ˆë‹¤ë§Œ ì „ì†¡ (ì²« í˜¸ì¶œì€ ì¦‰ì‹œ ì „ì†¡)
            if current_time - last_update_time[0] < 1.0 and last_update_time[0] != 0:
                return

            last_update_time[0] = current_time

            if history_id and callback_url:
                try:
                    progress_data = {
                        "historyId": history_id,
                        "status": "GENERATING",
                        "currentStep": current_step,
                        "totalSteps": total_steps,
                        "message": message
                    }
                    requests.post(callback_url, json=progress_data, timeout=2)
                    print(f"ğŸ“Š Progress sent to backend: {message} ({current_step}/{total_steps})")
                except Exception as e:
                    print(f"âš ï¸ Failed to send progress: {e}")

        # ì´ë¯¸ì§€ ìƒì„±
        try:
            generated_files = generate_images(
                lora_path=temp_lora_path,
                config=config,
                callback=progress_callback
            )

            # S3 ì—…ë¡œë“œ
            print("Uploading generated images to S3...")
            s3_client = boto3.client('s3')
            bucket_name = os.environ.get("AWS_S3_IMAGES_BUCKET", "lora-generated-image-bucket")

            s3_keys = []
            for generated_file in generated_files:
                filename = os.path.basename(generated_file)
                s3_key = f"user-{user_id}/{filename}"

                s3_client.upload_file(
                    generated_file,
                    bucket_name,
                    s3_key,
                    ExtraArgs={'ContentType': 'image/png'}
                )

                s3_keys.append(s3_key)
                print(f"âœ… Uploaded: s3://{bucket_name}/{s3_key}")

            # ì½œë°± í˜¸ì¶œ
            if callback_url:
                try:
                    callback_data = {
                        "historyId": history_id,
                        "userId": user_id,
                        "modelId": model_id,
                        "prompt": prompt,
                        "negativePrompt": negative_prompt,
                        "steps": steps,
                        "guidanceScale": guidance_scale,
                        "seed": seed,
                        "numImages": num_images,
                        "imageS3Keys": s3_keys,
                        "status": "SUCCESS"
                    }
                    response = requests.post(callback_url, json=callback_data, timeout=10)
                    response.raise_for_status()
                    print(f"âœ… Callback successful: {callback_url}")
                except Exception as e:
                    print(f"âŒ Callback failed: {e}")

            # ì§„í–‰ë¥  ë”•ì…”ë„ˆë¦¬ì—ì„œ ì œê±° (ì™„ë£Œë¨)
            if history_id and history_id in generation_progress:
                del generation_progress[history_id]

            # ì„ì‹œ íŒŒì¼ ì •ë¦¬ (LoRA ëª¨ë¸ì€ ìºì‹œë¡œ ìœ ì§€)
            shutil.rmtree(output_dir, ignore_errors=True)

            return s3_keys

        except Exception as e:
            print(f"âŒ Image generation failed: {e}")

            # ì‹¤íŒ¨ ì½œë°±
            if callback_url:
                try:
                    callback_data = {
                        "historyId": history_id,
                        "userId": user_id,
                        "modelId": model_id,
                        "status": "FAIL",
                        "error": str(e)
                    }
                    requests.post(callback_url, json=callback_data, timeout=10)
                except Exception as cb_e:
                    print(f"âš ï¸ Failed to send failure callback: {cb_e}")
            print(f"âŒ Failure callback sent to backend for history {history_id}.")

            # ì§„í–‰ë¥  ë”•ì…”ë„ˆë¦¬ì—ì„œ ì œê±° (ì‹¤íŒ¨í•¨)
            if history_id and history_id in generation_progress:
                del generation_progress[history_id]

            shutil.rmtree(output_dir, ignore_errors=True)

            raise


# ì „ì—­ ìƒíƒœ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬ (SSE ì§„í–‰ë¥  ìŠ¤íŠ¸ë¦¬ë°)
# {history_id: {"status": "GENERATING", "current_step": 1, "total_steps": 30, ...}}
generation_progress = {}

# FastAPI ì›¹ ì„œë²„
@app.function(
    image=image,
    secrets=[secrets],
    min_containers=1,  # í•­ìƒ 1ê°œ ì¸ìŠ¤í„´ìŠ¤ ìœ ì§€
)
@modal.asgi_app()
def fastapi_app():
    """
    FastAPI ì›¹ ì„œë²„ (Modal ë°°í¬)
    """
    from fastapi import FastAPI, BackgroundTasks, Request
    from fastapi.responses import JSONResponse, StreamingResponse
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel, Field
    from typing import Optional, List
    import asyncio
    import json

    web_app = FastAPI(
        title="LoRA Training and Inference API (Modal)",
        description="Modalì—ì„œ ì‹¤í–‰ë˜ëŠ” ì„œë²„ë¦¬ìŠ¤ GPU ê¸°ë°˜ LoRA í•™ìŠµ ë° ì´ë¯¸ì§€ ìƒì„± API",
        version="2.0.0",
    )

    # CORS ì„¤ì •
    web_app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš©
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Pydantic ëª¨ë¸
    class TrainRequest(BaseModel):
        user_id: str = Field(..., description="ì‚¬ìš©ì ID")
        model_id: int = Field(..., description="ëª¨ë¸ ID")
        model_name: str = Field(..., description="ëª¨ë¸ ì´ë¦„")
        training_image_urls: List[str] = Field(..., description="S3 í•™ìŠµ ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸")
        callback_url: Optional[str] = Field(None, description="í•™ìŠµ ì™„ë£Œ ì‹œ ì½œë°± URL")

    class GenerateRequest(BaseModel):
        user_id: str = Field(..., description="ì‚¬ìš©ì ID")
        model_id: int = Field(..., description="ëª¨ë¸ ID")
        history_id: Optional[int] = Field(None, description="GenerationHistory ID")
        prompt: str = Field(..., description="ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸")
        lora_model_url: str = Field(..., description="S3 LoRA ëª¨ë¸ URL")
        negative_prompt: Optional[str] = Field("low quality, blurry, ugly, distorted, deformed")
        num_images: int = Field(1, description="ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜")
        steps: int = Field(40, description="ì¶”ë¡  ìŠ¤í…")
        guidance_scale: float = Field(7.5, description="CFG scale")
        seed: Optional[int] = Field(None, description="ëœë¤ ì‹œë“œ")
        callback_url: Optional[str] = Field(None, description="ì™„ë£Œ ì‹œ ì½œë°± URL")

    class MessageResponse(BaseModel):
        message: str

    # ì—”ë“œí¬ì¸íŠ¸
    @web_app.get("/")
    def root():
        return {"message": "LoRA Modal API is running"}

    @web_app.post("/train")
    async def start_training(req: TrainRequest):
        """í•™ìŠµ ì‹œì‘ (ë¹„ë™ê¸°)"""
        try:
            # LoraTrainer í´ë˜ìŠ¤ ë©”ì„œë“œ í˜¸ì¶œ (spawnìœ¼ë¡œ ë¹„ë™ê¸° ì‹¤í–‰)
            trainer = LoraTrainer()
            trainer.train_lora.spawn(
                user_id=req.user_id,
                model_id=req.model_id,
                model_name=req.model_name,
                training_image_urls=req.training_image_urls,
                callback_url=req.callback_url
            )

            return {"message": "Training started on Modal GPU (A10G)"}

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"message": f"Failed to start training: {str(e)}"}
            )

    @web_app.post("/generate")
    async def generate_images_api(req: GenerateRequest):
        """ì´ë¯¸ì§€ ìƒì„± (ë¹„ë™ê¸°)"""
        try:
            # ImageGenerator í´ë˜ìŠ¤ ë©”ì„œë“œ í˜¸ì¶œ (spawnìœ¼ë¡œ ë¹„ë™ê¸° ì‹¤í–‰)
            generator = ImageGenerator()
            call = generator.generate_images.spawn(
                user_id=req.user_id,
                model_id=req.model_id,
                history_id=req.history_id,
                prompt=req.prompt,
                lora_model_url=req.lora_model_url,
                negative_prompt=req.negative_prompt,
                num_images=req.num_images,
                steps=req.steps,
                guidance_scale=req.guidance_scale,
                seed=req.seed,
                callback_url=req.callback_url
            )

            return {"message": "Image generation started on Modal GPU (T4)", "call_id": call.object_id}

        except Exception as e:
            return JSONResponse(
                status_code=500,
                content={"message": f"Failed to start generation: {str(e)}"}
            )

    @web_app.get("/generate/stream")
    async def stream_generation_progress():
        """SSE ìŠ¤íŠ¸ë¦¬ë° - ì´ë¯¸ì§€ ìƒì„± ì§„í–‰ë¥ """
        async def event_generator():
            last_sent = {}

            while True:
                # ì§„í–‰ ì¤‘ì¸ ì‘ì—…ì´ ìˆëŠ”ì§€ í™•ì¸
                if generation_progress:
                    for history_id, progress in generation_progress.items():
                        # ë³€ê²½ì‚¬í•­ì´ ìˆì„ ë•Œë§Œ ì „ì†¡
                        if last_sent.get(history_id) != progress:
                            data = {
                                "status": "IN_PROGRESS",
                                "historyId": history_id,
                                "current_step": progress.get("current_step", 0),
                                "total_steps": progress.get("total_steps", 0),
                                "message": progress.get("message", "Generating...")
                            }
                            yield f"data: {json.dumps(data)}\n\n"
                            last_sent[history_id] = progress.copy()

                await asyncio.sleep(0.5)  # 0.5ì´ˆë§ˆë‹¤ ì²´í¬

        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"  # Nginx ë²„í¼ë§ ë¹„í™œì„±í™”
            }
        )

    @web_app.get("/health")
    def health_check():
        return {"status": "healthy"}

    return web_app


# CLI ëª…ë ¹ì–´
@app.local_entrypoint()
def main():
    """
    ë¡œì»¬ì—ì„œ Modal í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
    """
    print("Starting Modal deployment...")

    # ë² ì´ìŠ¤ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ìµœì´ˆ 1íšŒ)
    print("Downloading base model...")
    download_base_model.remote()

    print("âœ… Modal deployment ready!")
    print("Deploy with: modal deploy modal_app.py")
