"""
LoRA í•™ìŠµ ëª¨ë“ˆ
"""

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from diffusers import DDPMScheduler, AutoencoderKL, UNet2DConditionModel
from diffusers.optimization import get_cosine_schedule_with_warmup
from peft import LoraConfig, get_peft_model
from transformers import CLIPTextModel, CLIPTokenizer
from PIL import Image
import os
import numpy as np
from tqdm import tqdm
from pathlib import Path
import multiprocessing

from .config import TrainingConfig
from .preprocess import preprocess_dataset

# CUDA + multiprocessing í˜¸í™˜ì„±ì„ ìœ„í•´ spawn ë°©ì‹ ì‚¬ìš©
try:
    multiprocessing.set_start_method('spawn', force=True)
except RuntimeError:
    pass  # ì´ë¯¸ ì„¤ì •ëœ ê²½ìš° ë¬´ì‹œ


class LoRADataset(Dataset):
    """LoRA í•™ìŠµìš© Dataset í´ë˜ìŠ¤ (DataLoader ë³‘ë ¬ ë¡œë”©ìš©)"""

    def __init__(
        self,
        image_caption_pairs,
        image_size=512,
        text_embeddings_cache=None,
        use_cached_latents=False,
        latents_dir=None
    ):
        """
        Args:
            image_caption_pairs: [(image_path, caption), ...] í˜•ì‹ì˜ ë¦¬ìŠ¤íŠ¸
            image_size: ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ 512)
            text_embeddings_cache: ì‚¬ì „ ê³„ì‚°ëœ text embeddings ë”•ì…”ë„ˆë¦¬ (optional)
            use_cached_latents: Trueë©´ ì´ë¯¸ì§€ ëŒ€ì‹  ì‚¬ì „ ê³„ì‚°ëœ latents ë¡œë“œ
            latents_dir: ì‚¬ì „ ê³„ì‚°ëœ latents ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.data = image_caption_pairs
        self.image_size = image_size
        self.text_embeddings_cache = text_embeddings_cache
        self.use_cached_latents = use_cached_latents
        self.latents_dir = Path(latents_dir) if latents_dir else None

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        """
        ë‹¨ì¼ ì´ë¯¸ì§€/latentì™€ ìº¡ì…˜/embedding ë¡œë“œ

        Returns:
            tuple: (image_tensor or latent_tensor, text_embedding or caption)
        """
        img_path, caption = self.data[idx]

        # ì´ë¯¸ì§€ ë˜ëŠ” Latent ë¡œë“œ
        if self.use_cached_latents and self.latents_dir:
            # ì‚¬ì „ ê³„ì‚°ëœ latent ë¡œë“œ (ì´ˆê³ ì†!)
            latent_file = self.latents_dir / f"{Path(img_path).stem}_latent.pt"
            if latent_file.exists():
                latent = torch.load(latent_file, map_location='cpu')  # CPUë¡œ ë¡œë“œ
                image_data = latent.squeeze(0)  # (1, C, H, W) â†’ (C, H, W)
            else:
                # Latent íŒŒì¼ì´ ì—†ìœ¼ë©´ ì´ë¯¸ì§€ ë¡œë“œ (í´ë°±)
                print(f"âš ï¸ Latent not found for {img_path.name}, loading image instead")
                img = Image.open(img_path).convert("RGB").resize(
                    (self.image_size, self.image_size), Image.LANCZOS
                )
                img_array = np.array(img).astype(np.float32) / 255.0
                img_array = (img_array - 0.5) / 0.5
                image_data = torch.from_numpy(img_array).permute(2, 0, 1)
        else:
            # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
            img = Image.open(img_path).convert("RGB").resize(
                (self.image_size, self.image_size), Image.LANCZOS
            )
            img_array = np.array(img).astype(np.float32) / 255.0
            img_array = (img_array - 0.5) / 0.5  # normalize to [-1, 1]
            image_data = torch.from_numpy(img_array).permute(2, 0, 1)

        # Text embedding
        if self.text_embeddings_cache is not None:
            # ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸° (ì´ˆê³ ì†!)
            text_data = self.text_embeddings_cache[caption]
        else:
            # ìº¡ì…˜ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë‚˜ì¤‘ì— ì¸ì½”ë”©)
            text_data = caption

        return image_data, text_data


def load_models(config: TrainingConfig):
    """
    Stable Diffusion ëª¨ë¸ + LoRA ì„¤ì •

    Args:
        config: í•™ìŠµ ì„¤ì •

    Returns:
        tuple: (vae, unet, text_encoder, tokenizer, noise_scheduler)
    """
    print(f"\nLoading models from: {config.model_id}")

    # VAE, UNet, Text Encoder ë¡œë“œ
    vae = AutoencoderKL.from_pretrained(
        config.model_id, subfolder="vae", torch_dtype=torch.float16
    )
    unet = UNet2DConditionModel.from_pretrained(
        config.model_id, subfolder="unet", torch_dtype=torch.float16
    )
    text_encoder = CLIPTextModel.from_pretrained(
        config.model_id, subfolder="text_encoder", torch_dtype=torch.float16
    )
    tokenizer = CLIPTokenizer.from_pretrained(
        config.model_id, subfolder="tokenizer"
    )
    noise_scheduler = DDPMScheduler.from_pretrained(
        config.model_id, subfolder="scheduler"
    )

    # VAEì™€ Text EncoderëŠ” freeze
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)

    # Gradient checkpointing
    unet.enable_gradient_checkpointing()

    # Deviceë¡œ ì´ë™
    vae.to(config.device)
    text_encoder.to(config.device)
    unet.to(config.device)

    # LoRA ì„¤ì •
    print("Setting up LoRA...")
    lora_config = LoraConfig(
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        target_modules=config.target_modules,
        lora_dropout=config.lora_dropout,
        bias="none",
    )

    unet = get_peft_model(unet, lora_config)
    unet.train()

    trainable_params = [p for p in unet.parameters() if p.requires_grad]
    print(f"Trainable parameters: {sum(p.numel() for p in trainable_params):,}")

    return vae, unet, text_encoder, tokenizer, noise_scheduler


def load_images_with_captions(dataset_path: str, trigger_word: str = None):
    """ì´ë¯¸ì§€ íŒŒì¼ + ìº¡ì…˜ ë¡œë“œ"""
    path = Path(dataset_path)
    image_files = list(path.glob("*.png")) + list(path.glob("*.jpg"))

    if len(image_files) == 0:
        raise ValueError(f"No images found in {dataset_path}")

    # ì´ë¯¸ì§€ì™€ ìº¡ì…˜ ë§¤í•‘
    image_caption_pairs = []
    for img_file in image_files:
        # ìº¡ì…˜ íŒŒì¼ ì°¾ê¸°
        caption_file = img_file.with_suffix('.txt')

        if caption_file.exists():
            # ìº¡ì…˜ íŒŒì¼ì´ ìˆìœ¼ë©´ ì½ê¸°
            with open(caption_file, 'r', encoding='utf-8') as f:
                caption = f.read().strip()
        else:
            # ì—†ìœ¼ë©´ ê¸°ë³¸ trigger word ì‚¬ìš© (Noneì´ë©´ ë¹ˆ ë¬¸ìì—´)
            caption = trigger_word if trigger_word else ""

        image_caption_pairs.append((img_file, caption))

    print(f"Loaded {len(image_caption_pairs)} images with captions")

    # ìƒ˜í”Œ ìº¡ì…˜ ì¶œë ¥
    if len(image_caption_pairs) > 0:
        print(f"\nSample captions from training data:")
        for i in range(min(3, len(image_caption_pairs))):
            img_file, caption = image_caption_pairs[i]
            print(f"  {img_file.name}: {caption}")

    return image_caption_pairs


def load_and_preprocess_image(img_paths: list[str], device: str, size: int = 512):
    """ì´ë¯¸ì§€ë¥¼ tensorë¡œ ë³€í™˜"""
    images = []
    for img_path in img_paths:
        img = Image.open(img_path).convert("RGB").resize((size, size), Image.LANCZOS)
        img_array = np.array(img).astype(np.float32) / 255.0
        img_array = (img_array - 0.5) / 0.5  # normalize to [-1, 1]
        images.append(torch.from_numpy(img_array).permute(2, 0, 1))
    return torch.stack(images).to(device, dtype=torch.float16)


def encode_prompt(text_encoder, tokenizer, prompt_texts: list[str], device: str):
    """í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¸ì½”ë”©"""
    text_input = tokenizer(
        prompt_texts,
        padding="max_length",
        max_length=tokenizer.model_max_length,
        truncation=True,
        return_tensors="pt"
    )
    with torch.no_grad():
        text_embeddings = text_encoder(text_input.input_ids.to(device))[0]
    return text_embeddings


def precompute_text_embeddings(
    text_encoder,
    tokenizer,
    captions_list: list[str],
    device: str
) -> dict:
    """
    ìœ ë‹ˆí¬ ìº¡ì…˜ë“¤ì˜ embeddingì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ë©”ëª¨ë¦¬ì— ìºì‹±

    Args:
        text_encoder: CLIP Text Encoder
        tokenizer: CLIP Tokenizer
        captions_list: ëª¨ë“  ìº¡ì…˜ ë¦¬ìŠ¤íŠ¸
        device: cuda/cpu

    Returns:
        dict: {caption: embedding_tensor} ë”•ì…”ë„ˆë¦¬
    """
    # ì¤‘ë³µ ì œê±°
    unique_captions = list(set(captions_list))

    print(f"\nğŸ“ Precomputing text embeddings for {len(unique_captions)} unique captions...")
    print(f"   (Total captions: {len(captions_list)}, Duplicates removed: {len(captions_list) - len(unique_captions)})")

    embeddings_cache = {}

    for caption in tqdm(unique_captions, desc="Computing embeddings"):
        # Tokenize
        text_input = tokenizer(
            [caption],
            padding="max_length",
            max_length=tokenizer.model_max_length,
            truncation=True,
            return_tensors="pt"
        )

        # Encode
        with torch.no_grad():
            embedding = text_encoder(text_input.input_ids.to(device))[0]

        # CPUë¡œ ì´ë™í•˜ì—¬ ìºì‹± (pin_memory í˜¸í™˜)
        embeddings_cache[caption] = embedding.cpu()

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
    embedding_size = next(iter(embeddings_cache.values())).element_size() * \
                     next(iter(embeddings_cache.values())).nelement()
    total_memory = embedding_size * len(embeddings_cache) / 1024 / 1024  # MB

    print(f"âœ… Text embeddings cached in memory")
    print(f"   Memory usage: {total_memory:.2f} MB ({embedding_size/1024:.1f} KB per embedding)")

    return embeddings_cache


def compute_snr(timesteps, noise_scheduler):
    """Min-SNR weightingì„ ìœ„í•œ SNR ê³„ì‚°"""
    alphas_cumprod = noise_scheduler.alphas_cumprod
    sqrt_alphas_cumprod = alphas_cumprod**0.5
    sqrt_one_minus_alphas_cumprod = (1.0 - alphas_cumprod) ** 0.5

    sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device=timesteps.device)[timesteps].float()
    while len(sqrt_alphas_cumprod.shape) < len(timesteps.shape):
        sqrt_alphas_cumprod = sqrt_alphas_cumprod[..., None]
    alpha = sqrt_alphas_cumprod.expand(timesteps.shape)

    sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.to(device=timesteps.device)[timesteps].float()
    while len(sqrt_one_minus_alphas_cumprod.shape) < len(timesteps.shape):
        sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod[..., None]
    sigma = sqrt_one_minus_alphas_cumprod.expand(timesteps.shape)

    snr = (alpha / sigma) ** 2
    return snr




def train_lora(
    dataset_path: str,
    output_dir: str,
    config: TrainingConfig = None,
    callback = None,
    use_cached_latents: bool = False,
    latents_dir: str = None
):
    """
    LoRA í•™ìŠµ í•¨ìˆ˜ (Modal APIìš©)

    Args:
        dataset_path: ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹ ê²½ë¡œ
        output_dir: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        config: í•™ìŠµ ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        callback: ì§„í–‰ë„ ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜ (status, phase, current_epoch, total_epochs, message)
        use_cached_latents: Trueë©´ ì‚¬ì „ ê³„ì‚°ëœ latents ì‚¬ìš©
        latents_dir: ì‚¬ì „ ê³„ì‚°ëœ latents ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        dict: í•™ìŠµ ê²°ê³¼ ì •ë³´
    """
    if config is None:
        config = TrainingConfig()

    config.output_dir = output_dir

    # ëª¨ë¸ ë¡œë“œ
    vae, unet, text_encoder, tokenizer, noise_scheduler = load_models(config)

    # ë°ì´í„° ë¡œë“œ (ì´ë¯¸ì§€ + ìº¡ì…˜)
    image_caption_pairs = load_images_with_captions(dataset_path, config.trigger_word)

    # Text Embeddings ì‚¬ì „ ê³„ì‚° (ë©”ëª¨ë¦¬ ìºì‹±)
    all_captions = [caption for _, caption in image_caption_pairs]
    text_embeddings_cache = precompute_text_embeddings(
        text_encoder, tokenizer, all_captions, config.device
    )

    # Dataset ë° DataLoader ìƒì„± (ë³‘ë ¬ ë¡œë”© + Text Embeddings ìºì‹± + VAE Latents ìºì‹±)
    if use_cached_latents:
        print(f"âœ… Using cached VAE latents from: {latents_dir}")

    train_dataset = LoRADataset(
        image_caption_pairs,
        config.image_size,
        text_embeddings_cache=text_embeddings_cache,  # Text embeddings ìºì‹œ
        use_cached_latents=use_cached_latents,  # VAE latents ìºì‹± ì—¬ë¶€
        latents_dir=latents_dir  # VAE latents ë””ë ‰í† ë¦¬
    )
    train_dataloader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,  # ì—í¬í¬ë§ˆë‹¤ ì…”í”Œ
        num_workers=10,  # ë³‘ë ¬ ì›Œì»¤ ìˆ˜ ì¦ê°€ (GPU ì‚¬ìš©ë¥  í–¥ìƒ)
        prefetch_factor=3,  # ì›Œì»¤ë‹¹ 3ê°œ ë°°ì¹˜ ë¯¸ë¦¬ ì¤€ë¹„
        pin_memory=True,  # GPU ì§ì ‘ ì „ì†¡ (ë¹ ë¦„)
        drop_last=False,  # ë§ˆì§€ë§‰ ë°°ì¹˜ë„ ì‚¬ìš©
        persistent_workers=True  # ì›Œì»¤ ì¬ì‚¬ìš©ìœ¼ë¡œ ì‹œì‘ ì˜¤ë²„í—¤ë“œ ê°ì†Œ
    )
    print(f"âœ… DataLoader created with 10 workers + prefetch (optimized for GPU utilization)")

    # Optimizer & Scheduler
    trainable_params = [p for p in unet.parameters() if p.requires_grad]
    optimizer = torch.optim.AdamW(
        trainable_params,
        lr=config.learning_rate,
        weight_decay=config.weight_decay
    )

    total_steps = (len(image_caption_pairs) // config.batch_size) * config.num_epochs // config.gradient_accumulation_steps
    warmup_steps = total_steps // 10
    lr_scheduler = get_cosine_schedule_with_warmup(
        optimizer,
        num_warmup_steps=warmup_steps,
        num_training_steps=total_steps
    )

    # Mixed Precision Training ì„¤ì •
    scaler = torch.cuda.amp.GradScaler()
    print("âœ… Mixed Precision Training (AMP) enabled")

    # í•™ìŠµ ì‹œì‘
    print(f"\nStarting training:")
    print(f"  Epochs: {config.num_epochs}")
    print(f"  Learning rate: {config.learning_rate}")
    print(f"  Batch size: {config.batch_size}")
    print(f"  Total steps: {total_steps}")

    loss_history = []
    global_step = 0

    # í•™ìŠµ ì‹œì‘ ì½œë°±
    if callback:
        callback(
            status="TRAINING",
            phase="training",
            current_epoch=0,
            total_epochs=config.num_epochs,
            message=f"í•™ìŠµ ì‹œì‘... (0/{config.num_epochs} ì—í¬í¬)"
        )

    for epoch in range(config.num_epochs):
        epoch_loss = 0

        # DataLoader ì‚¬ìš© (ë³‘ë ¬ ë¡œë”©)
        total_batches = len(train_dataloader)
        progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{config.num_epochs}")

        # ì—í¬í¬ ì‹œì‘ ì‹œ ì½œë°± í˜¸ì¶œ
        if callback:
            callback(
                status="TRAINING",
                phase="training",
                current_epoch=epoch + 1,
                total_epochs=config.num_epochs,
                message=f"Training {epoch + 1}/{config.num_epochs}"
            )

        for batch_idx, (data, text_data) in enumerate(progress_bar):
            # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
            if use_cached_latents:
                # ì´ë¯¸ latent! (VAE encoding ìƒëµ)
                latents = data.to(config.device, dtype=torch.float16)
            else:
                # ì´ë¯¸ì§€ â†’ latent ë³€í™˜ í•„ìš”
                pixel_values = data.to(config.device, dtype=torch.float16)
                with torch.no_grad():
                    latents = vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * vae.config.scaling_factor

            # Text embeddings (ì´ë¯¸ ìºì‹œì—ì„œ ê°€ì ¸ì˜¨ ìƒíƒœ!)
            # text_dataëŠ” ì´ë¯¸ embedding tensor (ë°°ì¹˜ë¡œ ìŠ¤íƒë¨)
            # DataLoaderê°€ ìë™ìœ¼ë¡œ ë°°ì¹˜ë¥¼ ë§Œë“¤ì–´ì£¼ë¯€ë¡œ squeeze(1) í•„ìš”
            encoder_hidden_states = torch.stack([t.squeeze(0) for t in text_data]).to(config.device)

            # Noise ì¶”ê°€
            noise = torch.randn_like(latents)
            if config.noise_offset > 0:
                noise += config.noise_offset * torch.randn(
                    (latents.shape[0], latents.shape[1], 1, 1),
                    device=latents.device
                )

            timesteps = torch.randint(
                0,
                noise_scheduler.config.num_train_timesteps,
                (latents.shape[0],),
                device=config.device
            )
            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)

            # Mixed Precision Forward Pass
            with torch.cuda.amp.autocast():
                # UNetìœ¼ë¡œ noise ì˜ˆì¸¡
                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample

                # Loss ê³„ì‚°
                loss = F.mse_loss(model_pred.float(), noise.float(), reduction="none")
                loss = loss.mean([1, 2, 3])

                # Min-SNR weighting
                if config.snr_gamma is not None:
                    snr = compute_snr(timesteps, noise_scheduler)
                    mse_loss_weights = torch.stack(
                        [snr, config.snr_gamma * torch.ones_like(timesteps)], dim=1
                    ).min(dim=1)[0]
                    mse_loss_weights = mse_loss_weights / snr
                    loss = loss * mse_loss_weights

                loss = loss.mean() / config.gradient_accumulation_steps

            # Mixed Precision Backward Pass
            scaler.scale(loss).backward()

            # Gradient accumulation
            if (batch_idx + 1) % config.gradient_accumulation_steps == 0 or (batch_idx + 1) == total_batches:
                # Gradient clipping (unscale first for correct norm calculation)
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(trainable_params, config.max_grad_norm)

                # Optimizer step with scaler
                scaler.step(optimizer)
                scaler.update()

                lr_scheduler.step()
                optimizer.zero_grad()

            # Loss ê¸°ë¡
            actual_loss = loss.item() * config.gradient_accumulation_steps
            epoch_loss += actual_loss
            loss_history.append(actual_loss)
            global_step += 1

            # Progress bar ì—…ë°ì´íŠ¸
            current_lr = lr_scheduler.get_last_lr()[0]
            progress_bar.set_postfix({
                "loss": f"{actual_loss:.4f}",
                "lr": f"{current_lr:.2e}"
            })

            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë„ˆë¬´ ìì£¼ í˜¸ì¶œí•˜ë©´ ì˜¤íˆë ¤ ëŠë ¤ì§)
            if global_step % 30 == 0:
                torch.cuda.empty_cache()

        avg_loss = epoch_loss / total_batches
        print(f"Epoch {epoch+1} completed - Average Loss: {avg_loss:.4f}")

        # 50 ì—í¬í¬ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if (epoch + 1) % 50 == 0 or (epoch + 1) == config.num_epochs:
            checkpoint_dir = os.path.join(output_dir, f"checkpoint-{epoch + 1}")
            print(f"\nSaving checkpoint to: {checkpoint_dir}")
            os.makedirs(checkpoint_dir, exist_ok=True)

            # WebUI/Civitai í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ë‹¨ì¼ .safetensors íŒŒì¼)
            from .lora_utils import save_lora_as_webui

            safetensors_path = os.path.join(checkpoint_dir, "lora_weights.safetensors")
            save_lora_as_webui(
                unet,
                safetensors_path,
                lora_alpha=config.lora_alpha,
                lora_rank=config.lora_r
            )

    # ìµœì¢… ëª¨ë¸ ì €ì¥ ë©”ì‹œì§€ (ì´ì œ ì²´í¬í¬ì¸íŠ¸ë¡œ ì €ì¥ë˜ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬ ë˜ëŠ” ìˆ˜ì •)
    # print(f"\nSaving model to: {output_dir}")
    # os.makedirs(output_dir, exist_ok=True)
    # unet.save_pretrained(output_dir)

    # í†µê³„ ì¶œë ¥
    print(f"\nTraining Statistics:")
    print(f"  Total steps: {len(loss_history)}")
    print(f"  Initial loss: {loss_history[0]:.4f}")
    print(f"  Final loss: {loss_history[-1]:.4f}")
    print(f"  Average loss: {np.mean(loss_history):.4f}")
    print(f"  Min loss: {np.min(loss_history):.4f}")

    return {
        "total_steps": len(loss_history),
        "final_loss": loss_history[-1],
        "avg_loss": np.mean(loss_history),
        "model_path": output_dir
    }


def train_with_preprocessing(
    raw_dataset_path: str,
    output_dir: str,
    config: TrainingConfig = None,
    skip_preprocessing: bool = False,
    callback = None
):
    """
    ì „ì²˜ë¦¬ + í•™ìŠµ ì „ì²´ íŒŒì´í”„ë¼ì¸ (Modal APIìš©)

    Args:
        raw_dataset_path: ì›ë³¸ ë°ì´í„°ì…‹ ê²½ë¡œ
        output_dir: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        config: í•™ìŠµ ì„¤ì •
        skip_preprocessing: ì „ì²˜ë¦¬ ìŠ¤í‚µ ì—¬ë¶€
        callback: ì§„í–‰ë„ ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜

    Returns:
        dict: í•™ìŠµ ê²°ê³¼
    """
    if config is None:
        config = TrainingConfig()

    # 1. ì „ì²˜ë¦¬ ë° ìº¡ì…”ë‹
    clean_dataset_path = config.clean_dataset_path

    if not skip_preprocessing:
        # ì „ì²˜ë¦¬ + ìº¡ì…”ë‹ (ì „ì²´ íŒŒì´í”„ë¼ì¸)
        # ì „ì²˜ë¦¬ ì‹œì‘ ì½œë°±
        if callback:
            callback(
                status="PREPROCESSING",
                phase="preprocessing",
                current_epoch=0,
                total_epochs=0,
                message="ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° ìº¡ì…”ë‹ ì¤‘..."
            )

        print("\n" + "="*60)
        print("STEP 1: Dataset Preprocessing + Captioning")
        print("="*60)

        preprocess_result = preprocess_dataset(
            input_dir=raw_dataset_path,
            output_dir=clean_dataset_path,
            trigger_word=config.trigger_word  # trigger_word ì „ë‹¬
        )
        print(f"Preprocessing result: {preprocess_result}")

        # ì „ì²˜ë¦¬ ì™„ë£Œ ì½œë°±
        if callback:
            callback(
                status="PREPROCESSING",
                phase="preprocessing",
                current_epoch=0,
                total_epochs=0,
                message="ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ë° ìº¡ì…”ë‹ ì™„ë£Œ"
            )
    else:
        # ì „ì²˜ë¦¬ ìŠ¤í‚µ, ìº¡ì…”ë‹ë§Œ ìˆ˜í–‰ (í•„ìˆ˜)
        from .preprocess import caption_only_dataset

        # ìº¡ì…”ë‹ ì‹œì‘ ì½œë°±
        if callback:
            callback(
                status="PREPROCESSING",
                phase="preprocessing",
                current_epoch=0,
                total_epochs=0,
                message="ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì¤‘..."
            )

        print("\n" + "="*60)
        print("STEP 1: Captioning Only (Preprocessing Skipped)")
        print("="*60)

        caption_result = caption_only_dataset(
            input_dir=raw_dataset_path,
            trigger_word=config.trigger_word
        )
        print(f"Captioning result: {caption_result}")

        # ì „ì²˜ë¦¬ ìŠ¤í‚µí•˜ë¯€ë¡œ ì›ë³¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ë¥¼ í•™ìŠµì— ì‚¬ìš©
        clean_dataset_path = raw_dataset_path

        # ìº¡ì…”ë‹ ì™„ë£Œ ì½œë°±
        if callback:
            callback(
                status="PREPROCESSING",
                phase="preprocessing",
                current_epoch=0,
                total_epochs=0,
                message="ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì™„ë£Œ"
            )

    # 2. VAE Latents ì‚¬ì „ ê³„ì‚° (í•™ìŠµ ì†ë„ 30-40% í–¥ìƒ)
    print("\n" + "="*60)
    print("STEP 2: Precomputing VAE Latents (Speed Optimization)")
    print("="*60)

    latents_dir = os.path.join(output_dir, "cached_latents")

    if callback:
        callback(
            status="PREPROCESSING",
            phase="preprocessing",
            current_epoch=0,
            total_epochs=0,
            message="VAE latents ì‚¬ì „ ê³„ì‚° ì¤‘... (í•™ìŠµ ì†ë„ í–¥ìƒì„ ìœ„í•´)"
        )

    from .precompute_latents import precompute_latents
    precompute_latents(
        dataset_path=clean_dataset_path,
        output_path=latents_dir,
        model_id=config.model_id,
        image_size=config.image_size
    )
    print(f"âœ… VAE latents cached to: {latents_dir}")

    # 3. í•™ìŠµ
    print("\n" + "="*60)
    print("STEP 3: Training with Cached Latents")
    print("="*60)

    train_result = train_lora(
        dataset_path=clean_dataset_path,
        output_dir=output_dir,
        config=config,
        callback=callback,
        use_cached_latents=True,
        latents_dir=latents_dir
    )

    print("\n" + "="*60)
    print("Training completed successfully!")
    print("="*60)
    print(f"Model saved to: {output_dir}")
    print(f"To generate images, run:")
    print(f"  python inference.py --lora_path {output_dir}")

    return train_result


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    config = TrainingConfig()
    result = train_with_preprocessing(
        raw_dataset_path="./dataset",
        output_dir="my_lora_model",
        config=config
    )
    print(f"\nResult: {result}")
