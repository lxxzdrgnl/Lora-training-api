"""
LoRA ëª¨ë¸ ì¶”ë¡  ëª¨ë“ˆ
"""

import torch
from diffusers import StableDiffusionPipeline
from peft import PeftModel
import argparse
from datetime import datetime
import os
from pathlib import Path

from .config import InferenceConfig


# íŒŒì´í”„ë¼ì¸ ìºì‹œ í´ë˜ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
class PipelineCache:
    """íŒŒì´í”„ë¼ì¸ì„ ë©”ëª¨ë¦¬ì— ìºì‹±í•˜ì—¬ ì¬ì‚¬ìš©"""
    _instance = None
    _pipelines = {}  # {(model_id, lora_path): pipeline}

    @classmethod
    def get_pipeline(cls, model_id: str, lora_path: str, device: str):
        """ìºì‹œëœ íŒŒì´í”„ë¼ì¸ ë°˜í™˜ ë˜ëŠ” ìƒˆë¡œ ë¡œë“œ"""
        cache_key = (model_id, lora_path)

        if cache_key in cls._pipelines:
            print(f"ğŸš€ Using cached pipeline (model: {model_id}, lora: {lora_path})")
            return cls._pipelines[cache_key]

        print(f"ğŸ“¦ Loading new pipeline into cache...")
        pipe = load_pipeline(model_id, lora_path, device)
        cls._pipelines[cache_key] = pipe
        return pipe

    @classmethod
    def clear_cache(cls):
        """ìºì‹œ ì´ˆê¸°í™” (ë©”ëª¨ë¦¬ ì •ë¦¬ìš©)"""
        cls._pipelines.clear()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("ğŸ§¹ Pipeline cache cleared")


def load_pipeline(model_id: str, lora_path: str, device: str):
    """
    Stable Diffusion íŒŒì´í”„ë¼ì¸ + LoRA ë¡œë“œ

    Args:
        model_id: ë² ì´ìŠ¤ ëª¨ë¸ ID ë˜ëŠ” ë¡œì»¬ ê²½ë¡œ
        lora_path: LoRA ëª¨ë¸ ê²½ë¡œ (ë””ë ‰í† ë¦¬ ë˜ëŠ” .safetensors íŒŒì¼)
        device: ë””ë°”ì´ìŠ¤ (cuda/cpu)

    Returns:
        StableDiffusionPipeline: ë¡œë“œëœ íŒŒì´í”„ë¼ì¸
    """
    # Modal Volume ìºì‹± ë¡œì§
    # /cache/base_models/{model_id}/ ê²½ë¡œì— ìºì‹±
    cache_base = "/cache/base_models"

    # ëª¨ë¸ IDë¥¼ íŒŒì¼ì‹œìŠ¤í…œì— ì•ˆì „í•œ ì´ë¦„ìœ¼ë¡œ ë³€í™˜ (/ -> --)
    safe_model_id = model_id.replace("/", "--")
    cached_model_path = os.path.join(cache_base, safe_model_id)

    # ìºì‹œëœ ëª¨ë¸ í™•ì¸
    if os.path.exists(cached_model_path) and os.path.exists(os.path.join(cached_model_path, "model_index.json")):
        base_model_path = cached_model_path
        print(f"\nâœ… Using cached base model from volume: {base_model_path}")
    else:
        # ìºì‹œê°€ ì—†ìœ¼ë©´ HuggingFaceì—ì„œ ë‹¤ìš´ë¡œë“œ í›„ ìºì‹±
        print(f"\nğŸ“¥ Downloading base model from HuggingFace: {model_id}")
        print(f"ğŸ’¾ Will cache to: {cached_model_path}")

        # ì„ì‹œë¡œ HuggingFaceì—ì„œ ë¡œë“œ
        temp_pipe = StableDiffusionPipeline.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            safety_checker=None
        )

        # Modal Volumeì— ì €ì¥
        os.makedirs(cache_base, exist_ok=True)
        temp_pipe.save_pretrained(cached_model_path)
        print(f"âœ… Base model cached successfully!")

        base_model_path = cached_model_path

    print(f"Loading base model: {base_model_path}")
    pipe = StableDiffusionPipeline.from_pretrained(
        base_model_path,
        torch_dtype=torch.float16,
        safety_checker=None
    )

    print(f"Loading LoRA weights: {lora_path}")

    # LoRA ê²½ë¡œ í™•ì¸ ë° í˜•ì‹ íŒë‹¨
    if os.path.isdir(lora_path):
        # ë””ë ‰í† ë¦¬ì¸ ê²½ìš°: PEFT í˜•ì‹ ìš°ì„  ì‹œë„ (adapter_model.safetensors + adapter_config.json)
        adapter_config_path = os.path.join(lora_path, "adapter_config.json")
        adapter_model_path = os.path.join(lora_path, "adapter_model.safetensors")

        if os.path.exists(adapter_config_path) and os.path.exists(adapter_model_path):
            # PEFT í˜•ì‹ ë°œê²¬ - ì´ê²ƒì´ ê°€ì¥ ì •í™•í•¨ (alpha, rank ì •ë³´ í¬í•¨)
            print(f"âœ… Found PEFT format (adapter_model.safetensors + adapter_config.json)")
            print(f"   This preserves lora_alpha and lora_r from training")
            pipe.load_lora_weights(
                lora_path,  # ë””ë ‰í† ë¦¬ ì „ë‹¬
                weight_name="adapter_model.safetensors"
            )
        else:
            # PEFT í˜•ì‹ì´ ì—†ìœ¼ë©´ WebUI í˜•ì‹ ì°¾ê¸°
            safetensors_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
            if not safetensors_files:
                raise ValueError(f"No .safetensors file found in {lora_path}")
            lora_file = safetensors_files[0]
            print(f"Found WebUI format: {lora_file}")
            pipe.load_lora_weights(
                lora_path,
                weight_name=lora_file
            )
    elif os.path.isfile(lora_path) and lora_path.endswith('.safetensors'):
        # ë‹¨ì¼ safetensors íŒŒì¼ (Civitai ë‹¤ìš´ë¡œë“œ í˜•ì‹)
        print(f"Found single LoRA file (WebUI format): {lora_path}")
        pipe.load_lora_weights(
            os.path.dirname(lora_path),
            weight_name=os.path.basename(lora_path)
        )
    else:
        raise ValueError(f"Invalid LoRA path: {lora_path}")

    # GPUë¡œ ì´ë™
    pipe.to(device)

    # xFormers ìµœì í™” í™œì„±í™”
    try:
        pipe.enable_xformers_memory_efficient_attention()
        print("âœ… xFormers memory efficient attention enabled")
    except Exception as e:
        print(f"âš ï¸ xFormers not available: {e}")
        print("   Falling back to default attention (still works, just slower)")

    # Attention slicing í™œì„±í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
    try:
        pipe.enable_attention_slicing(slice_size="auto")
        print("âœ… Attention slicing enabled")
    except Exception as e:
        print(f"âš ï¸ Could not enable attention slicing: {e}")

    print("âœ… LoRA loaded successfully in WebUI/Civitai format!\n")
    return pipe


def generate_images(
    lora_path: str = None,
    prompt: str = None,
    negative_prompt: str = None,
    num_images: int = None,
    steps: int = None,
    guidance_scale: float = None,
    seed: int = None,
    output_dir: str = None,
    config: InferenceConfig = None,
    callback = None
):
    """
    ì´ë¯¸ì§€ ìƒì„± í•¨ìˆ˜ (Modal APIìš©)

    Args:
        lora_path: LoRA ëª¨ë¸ ê²½ë¡œ
        prompt: í”„ë¡¬í”„íŠ¸
        negative_prompt: ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸
        num_images: ìƒì„±í•  ì´ë¯¸ì§€ ìˆ˜
        steps: ì¶”ë¡  ìŠ¤í…
        guidance_scale: CFG scale
        seed: ëœë¤ ì‹œë“œ
        output_dir: ì¶œë ¥ í´ë”
        config: ì¶”ë¡  ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
        callback: ì§„í–‰ë„ ì—…ë°ì´íŠ¸ ì½œë°± í•¨ìˆ˜ (status, current_image, total_images, current_step, total_steps, message)

    Returns:
        list: ìƒì„±ëœ ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    if config is None:
        config = InferenceConfig()

    # ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ (Noneì´ ì•„ë‹ ë•Œë§Œ)
    if prompt is not None:
        config.prompt = prompt
    if negative_prompt is not None:
        config.negative_prompt = negative_prompt
    if lora_path is not None:
        config.lora_path = lora_path
    if num_images is not None:
        config.num_images = num_images
    if steps is not None:
        config.steps = steps
    if guidance_scale is not None:
        config.guidance_scale = guidance_scale
    if seed is not None:
        config.seed = seed
    if output_dir is not None:
        config.output_dir = output_dir

    # íŒŒì´í”„ë¼ì¸ ë¡œë“œ (ìºì‹œ ì‚¬ìš©)
    pipe = PipelineCache.get_pipeline(config.model_id, config.lora_path, config.device)

    # Trigger word ìë™ ì¶”ê°€ (ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ì œê±°ë¨)
    # if not config.prompt.startswith(config.trigger_word):
    #     full_prompt = f"{config.trigger_word}, {config.prompt}"
    # else:
    #     full_prompt = config.prompt
    full_prompt = config.prompt

    print("="*60)
    print(f"DEBUG: config.num_images = {config.num_images}")
    print(f"Generating {config.num_images} image(s)")
    print(f"Prompt: {full_prompt}")
    print(f"Negative: {config.negative_prompt}")
    print(f"Steps: {config.steps} | CFG Scale: {config.guidance_scale}")
    if config.seed is not None:
        print(f"Seed: {config.seed}")
    print("="*60)

    # ì‹œë“œ ì„¤ì •
    generator = None
    if config.seed is not None:
        generator = torch.Generator(device=config.device).manual_seed(config.seed)

    # ì¶œë ¥ í´ë” ìƒì„±
    output_dir = Path(config.output_dir)
    output_dir.mkdir(exist_ok=True)

    # ì´ë¯¸ì§€ ìƒì„±
    generated_files = []

    # ìƒì„± ì‹œì‘ ì½œë°±
    if callback:
        callback(
            status="GENERATING",
            current_image=0,
            total_images=config.num_images,
            current_step=0,
            total_steps=config.steps,
            message=f"ì´ë¯¸ì§€ ìƒì„± ì‹œì‘... (0/{config.num_images})"
        )

    for i in range(config.num_images):
        print(f"\n[{i+1}/{config.num_images}] Generating...")

        # Stepë³„ ì½œë°± í•¨ìˆ˜ ì •ì˜
        def step_callback(pipe_instance, step_index, timestep, callback_kwargs):
            if callback:
                callback(
                    status="GENERATING",
                    current_image=i + 1,
                    total_images=config.num_images,
                    current_step=step_index + 1,
                    total_steps=config.steps,
                    message=f"ì´ë¯¸ì§€ {i+1}/{config.num_images} ìƒì„± ì¤‘... (step {step_index+1}/{config.steps})"
                )
            return callback_kwargs

        with torch.no_grad():
            image = pipe(
                prompt=full_prompt,
                negative_prompt=config.negative_prompt,
                num_inference_steps=config.steps,
                guidance_scale=config.guidance_scale,
                generator=generator,
                callback_on_step_end=step_callback if callback else None,
                callback_on_step_end_tensor_inputs=["latents"],
                cross_attention_kwargs={"scale": config.lora_scale}  # LoRA ê°•ë„ ì ìš©
            ).images[0]

        # íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{i+1}.png"
        output_path = output_dir / filename

        # ì €ì¥
        image.save(output_path)
        generated_files.append(str(output_path))
        print(f"âœ… Saved: {output_path}")

        # ì´ë¯¸ì§€ ì™„ë£Œ ì½œë°±
        if callback:
            callback(
                status="GENERATING",
                current_image=i + 1,
                total_images=config.num_images,
                current_step=config.steps,
                total_steps=config.steps,
                message=f"ì´ë¯¸ì§€ {i+1}/{config.num_images} ì™„ë£Œ"
            )

    print("\n" + "="*60)
    print(f"âœ… Successfully generated {len(generated_files)} image(s)")
    print(f"ğŸ“ Output folder: {config.output_dir}")
    print("="*60)

    return generated_files


def main():
    """CLI ì‹¤í–‰"""
    parser = argparse.ArgumentParser(
        description="í•™ìŠµëœ LoRA ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ìƒì„±",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    # í•„ìˆ˜ ì„¤ì •
    parser.add_argument(
        "--lora_path",
        type=str,
        default=None,
        help="LoRA ëª¨ë¸ ê²½ë¡œ (ê¸°ë³¸ê°’: config ì„¤ì •)"
    )
    parser.add_argument(
        "--model_id",
        type=str,
        default=None,
        help="ë² ì´ìŠ¤ Stable Diffusion ëª¨ë¸ (ê¸°ë³¸ê°’: config ì„¤ì •)"
    )

    # í”„ë¡¬í”„íŠ¸ ì„¤ì •
    parser.add_argument(
        "--prompt",
        type=str,
        default=None,
        help="ì´ë¯¸ì§€ ìƒì„± í”„ë¡¬í”„íŠ¸ (ê¸°ë³¸ê°’: config ì„¤ì •)"
    )
    parser.add_argument(
        "--negative_prompt",
        type=str,
        default=None,
        help="ë„¤ê±°í‹°ë¸Œ í”„ë¡¬í”„íŠ¸ (ê¸°ë³¸ê°’: config ì„¤ì •)"
    )

    # ìƒì„± ì˜µì…˜
    parser.add_argument(
        "--num_images",
        type=int,
        default=None,
        help="ìƒì„±í•  ì´ë¯¸ì§€ ê°œìˆ˜ (ê¸°ë³¸ê°’: config ì„¤ì •)"
    )
    parser.add_argument(
        "--steps",
        type=int,
        default=None,
        help="Inference steps (ê¸°ë³¸ê°’: config ì„¤ì •)"
    )
    parser.add_argument(
        "--guidance_scale",
        type=float,
        default=None,
        help="CFG scale (ê¸°ë³¸ê°’: config ì„¤ì •)"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=None,
        help="ëœë¤ ì‹œë“œ (ì¬í˜„ì„±)"
    )

    # ì¶œë ¥ ì„¤ì •
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="ìƒì„±ëœ ì´ë¯¸ì§€ ì €ì¥ í´ë” (ê¸°ë³¸ê°’: config ì„¤ì •)"
    )

    args = parser.parse_args()

    # ì´ë¯¸ì§€ ìƒì„± (config ê¸°ë³¸ê°’ ì‚¬ìš© í›„ CLI ì¸ìë¡œ ì˜¤ë²„ë¼ì´ë“œ)
    config = InferenceConfig()

    # CLI ì¸ìê°€ ì§€ì •ëœ ê²½ìš°ë§Œ ì˜¤ë²„ë¼ì´ë“œ
    if args.model_id is not None:
        config.model_id = args.model_id
    if args.lora_path is not None:
        config.lora_path = args.lora_path

    if args.prompt is not None:
        config.prompt = args.prompt
    if args.negative_prompt is not None:
        config.negative_prompt = args.negative_prompt
    if args.num_images is not None:
        config.num_images = args.num_images
    if args.steps is not None:
        config.steps = args.steps
    if args.guidance_scale is not None:
        config.guidance_scale = args.guidance_scale
    if args.seed is not None:
        config.seed = args.seed
    if args.output_dir is not None:
        config.output_dir = args.output_dir

    # LoRA ëª¨ë¸ ì¡´ì¬ í™•ì¸
    if not os.path.exists(config.lora_path):
        print(f"âŒ Error: LoRA model not found at {config.lora_path}")
        print(f"Please train the model first: python training.py")
        return

    generate_images(
        lora_path=config.lora_path,
        config=config
    )


if __name__ == "__main__":
    main()
