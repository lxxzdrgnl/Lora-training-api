# LoRA 캐릭터 학습 파이프라인의 AI 기술 분석 자료

## 1. 서론

본 문서는 만화/웹툰 스크린샷으로부터 특정 캐릭터의 특징을 학습하는 LoRA(Low-Rank Adaptation) 모델을 자동으로 생성하는 파이프라인에 적용된 핵심 AI 기술을 분석하고 정리한다. 이 파이프라인은 데이터 수집, 전처리, 모델 학습, 추론, 그리고 배포에 이르는 전 과정에 AI 기술을 깊숙이 활용하여 복잡한 과정을 자동화하고 고품질의 결과물을 생성하는 것을 목표로 한다. 본고에서는 각 단계별 적용된 AI 기술의 원리, 파라미터, 그리고 실용적인 서비스 구현을 위한 MLOps 최적화 방안까지 상세히 기술한다.

---

## 2. AI 기반 데이터 자동 구축 파이프라인

고품질의 데이터셋은 딥러닝 모델의 성능을 좌우하는 핵심 요소이다. 본 파이프라인은 사용자가 제공한 소수의 원본 스크린샷으로부터 LoRA 학습에 최적화된 데이터를 자동으로 구축하기 위해 다음과 같은 AI 기술을 순차적으로 적용한다.

### 2.1. 파이프라인 순서도

```
[원본 스크린샷] -> (1. 텍스트 검출: EasyOCR) -> [텍스트 영역 좌표] -> (2. 텍스트 제거: cv2.inpaint) -> [텍스트 제거된 이미지] -> (3. 캐릭터 분할: rembg) -> [캐릭터 마스크] -> (4. 크롭 및 리사이즈) -> [정방형 캐릭터 이미지] -> (5. 자동 캡셔닝: BLIP) -> [캡션 텍스트] -> [최종 학습 데이터 (이미지 + 캡션)]
```

### 2.2. 텍스트 검출 및 제거 (Text Detection and Removal)

- **텍스트 검출 (EasyOCR)**:
    - **기술**: 딥러닝 기반의 OCR(Optical Character Recognition) 라이브러리. 내부적으로 텍스트 영역을 감지하는 CRAFT(Character-Region Awareness for Text detection) 모델과 감지된 영역의 문자를 인식하는 CRNN(Convolutional Recurrent Neural Network) 모델을 결합한 구조를 가진다.
    - **역할**: 만화의 말풍선, 효과음, 자막 등 학습에 방해가 되는 텍스트 영역의 경계 상자(bounding box)를 정확히 감지한다. 한국어와 영어를 동시에 지원하여 다국어 환경에 대응한다.
    - **파라미터**: `easyocr.Reader(['ko', 'en'], gpu=True)`

- **텍스트 제거 (Inpainting)**:
    - **기술**: OpenCV의 `inpaint` 함수 (Navier-Stokes 기반 알고리즘).
    - **역할**: 검출된 텍스트 영역을 주변 픽셀의 정보를 이용하여 유체 역학적 원리로 자연스럽게 채워 넣어, 텍스트가 원래 없었던 것처럼 이미지를 복원한다.

### 2.3. 캐릭터 객체 분할 (Character Object Segmentation)

- **기술**: 딥러닝 기반 배경 제거 라이브러리인 `rembg`. 내부적으로 **U2-Net(U-squared-Net)** 아키텍처를 사용하여 이미지에서 전경(사람, 사물)과 배경을 정교하게 분리한다. U2-Net은 ReSidual U-block(RSU)이라는 새로운 모듈을 제안하여, 다양한 스케일의 정보를 손실 없이 효율적으로 포착함으로써 객체의 경계선을 매우 정밀하게 분리해낸다.
- **역할**: 텍스트가 제거된 이미지에서 캐릭터(전경) 영역을 분리하여 마스크를 생성한다. 이 마스크에서 가장 큰 윤곽선(contour)을 추출하여 캐릭터의 정확한 위치를 특정, 후속 크롭(crop) 단계의 기준점을 제공한다.
- **파라미터**: `remove(image, only_mask=True)`

### 2.4. 자동 이미지 캡셔닝 (Automatic Image Captioning)

- **기술**: Salesforce에서 개발한 Vision-Language 모델인 **BLIP (Bootstrapped Language-Image Pre-training)** (`Salesforce/blip-image-captioning-base`)를 사용한다. BLIP은 트랜스포머(Transformer) 기반의 MED(Multimodal Mixture of Encoder-Decoder) 아키텍처를 채택하여, 이미지 인코더와 텍스트 인코더/디코더를 함께 학습시켜 이미지와 텍스트 간의 깊은 연관 관계를 이해한다.
- **역할**: 전처리가 완료된 캐릭터 이미지에 대해 "a manga girl with black hair" 와 같이 구체적인 캡션을 자동으로 생성한다. 이 캡션은 학습 시 텍스트 프롬프트로 사용되어, 모델이 단순히 캐릭터의 시각적 특징뿐만 아니라 이미지의 맥락까지 학습하게 한다.
- **파라미터**: `max_length=50`, `num_beams=3`. 생성된 캡션 앞에 사용자가 지정한 `trigger_word` (예: `sks`)를 추가하여 특정 캐릭터에 대한 개념을 강화한다.

---

## 3. LoRA 파인튜닝 방법론

LoRA는 사전 학습된 거대 모델의 가중치는 동결(freeze)한 채, 각 레이어에 저차원(low-rank) 행렬을 추가하여 소수의 파라미터만으로 효율적인 미세조정(fine-tuning)을 수행하는 기법이다.

### 3.1. LoRA의 개념 및 장점

- **파라미터 효율성**: 전체 모델(수십억 파라미터)을 모두 학습시키는 Full Fine-tuning과 달리, LoRA는 원본 가중치에 추가되는 저차원 행렬 `A`와 `B`만을 학습시킨다. 예를 들어, 베이스 모델의 학습 파라미터가 수십억 개인 데 반해, 본 프로젝트의 LoRA 학습 파라미터는 약 1,700만 개(`r=32` 기준)에 불과하여, 전체의 1% 미만이다. 이는 학습 시간과 컴퓨팅 자원을 획기적으로 절약한다.
- **치명적 망각 방지 (Catastrophic Forgetting)**: 원본 모델의 가중치를 변경하지 않으므로, 사전 학습된 모델이 가진 방대한 지식을 보존하면서 새로운 태스크(캐릭터)에 대한 정보만 효율적으로 추가할 수 있다.

### 3.2. 학습 아키텍처 및 주요 파라미터

- **베이스 모델**: `stablediffusionapi/anything-v5`. 2D 애니메이션 및 만화 스타일에 특화된 Stable Diffusion 모델.
- **LoRA 파라미터**:
  - **`lora_r` (Rank)**: 분해되는 행렬의 랭크. (기본값: `32`, 권장: `8-64`)
  - **`lora_alpha`**: LoRA의 스케일링 강도. 최종적으로 LoRA의 영향력은 `alpha / r`로 결정된다. (기본값: `r`과 동일)
  - **`target_modules`**: LoRA를 적용할 대상 레이어. 본 프로젝트에서는 `UNet`의 cross-attention 블록 내 `to_q`, `to_k`, `to_v`, `to_out.0` (Query, Key, Value, Output 프로젝션)에 적용한다.

### 3.3. 학습 성능 및 품질 향상 기법

- **Min-SNR (Signal-to-Noise Ratio) 가중치 전략**:
  - **원리**: Diffusion 모델 학습 시, 노이즈가 많은 스텝(초기)과 적은 스텝(후기)의 중요도가 달라 학습이 불안정해질 수 있다. 각 타임스텝의 SNR을 계산하여 손실(loss)에 가중치를 부여함으로써, 노이즈가 많은 스텝의 손실 기여도를 줄여 학습 안정성을 높이고 최종 이미지 품질을 개선한다.
  - **파라미터**: `snr_gamma = 5.0`
- **Noise Offset**:
  - **원리**: 학습 시 추가적인 노이즈를 주입하여 모델이 평균적인 밝기(mid-tone)에만 집중하는 경향을 완화하고, 매우 어둡거나 밝은 영역의 디테일 표현력을 향상시킨다.
  - **파라미터**: `noise_offset = 0.1`
- **Gradient Checkpointing**:
  - **원리**: 순전파(forward pass) 시 중간 활성화(activation) 값들을 모두 저장하는 대신, 일부만 저장하고 역전파(backward pass) 시 필요할 때 재계산하는 기법. 메모리 사용량을 크게 줄여 더 큰 배치 사이즈나 고해상도 이미지 학습을 가능하게 한다.
  - **구현**: `unet.enable_gradient_checkpointing()`

---

## 4. 이미지 생성 및 품질 최적화

### 4.1. LoRA 형식 변환 및 호환성

- **문제점**: 허깅페이스 PEFT 라이브러리로 학습된 LoRA 모델은 WebUI/Civitai에서 사용하는 형식과 키(key) 이름 규칙이 달라 호환되지 않는다. 또한, 변환 과정에서 LoRA 강도를 결정하는 `alpha` 값이 손실될 수 있다.
- **해결책**: `core/lora_utils.py`에 변환 유틸리티를 구현. PEFT 형식의 키(`base_model.model...`)를 WebUI 형식(`lora_unet...`)으로 변환하고, `safetensors` 파일 내에 각 레이어별로 `alpha` 값을 명시적으로 저장한다. 이를 통해 Diffusers 라이브러리가 모델을 로드할 때 `alpha / r` 스케일을 정확히 계산하여 LoRA의 효과가 올바르게 적용되도록 보장한다.

### 4.2. 생성 품질 향상을 위한 기술적 최적화

- **CLIP Skip**:
  - **기술**: Stable Diffusion의 텍스트 인코더인 CLIP의 마지막 레이어를 건너뛰는 기법.
  - **효과**: 마지막에서 두 번째 레이어의 출력을 사용(`clip_skip = 2`)함으로써, 텍스트의 세부 사항에 지나치게 얽매이지 않고 더 창의적이고 일반화된 특징을 추출하여 자연스러운 결과물을 생성한다.
- **VAE `float32` 변환**:
  - **문제점**: `float16` 정밀도로 VAE를 실행하면, 색상 디코딩 과정에서 정밀도 손실로 인해 채도가 비정상적으로 높거나 색상이 깨지는 "Fried Image" 현상이 발생한다.
  - **해결책**: `StableDiffusionPipeline`을 GPU 메모리로 옮긴 후, VAE(`pipe.vae`)만 `float32`로 데이터 타입을 명시적으로 변환하여 색상 정보의 손실을 방지한다.
  - **구현 코드 예시** (`core/generate.py`):
    ```python
    # 1. 먼저 전체 파이프라인을 GPU로 이동
    pipe.to(device)
    # 2. 그 다음 VAE만 float32로 변환
    pipe.vae.to(dtype=torch.float32)
    ```

---

## 5. MLOps: 서버리스 배포 및 운영 최적화

본 프로젝트는 AI 모델의 실용성과 확장성을 보장하기 위해 `Modal` 플랫폼을 활용한 다양한 MLOps 기술을 적용했다.

- **서버리스 GPU**: 학습(`A10G`)과 추론(`T4`)에 각각 다른 사양의 GPU를 할당하고, 요청이 있을 때만 컨테이너를 실행하여 비용 효율성을 극대화했다.
- **메모리 스냅샷 (`enable_memory_snapshot`)**: 베이스 모델이 GPU 메모리에 로드된 상태를 스냅샷으로 저장한다. 이를 통해 Cold Start 시 모델 로딩 시간(15-20초)을 생략하고, **2-3초 만에 컨테이너를 부팅**하여 사용자 요청에 즉시 가까운 속도로 응답할 수 있게 한다. (`개발중 이슈.md`에 따르면, 이 최적화로 전체 응답 시간이 **4-6배 개선**되었다.)
- **공유 볼륨 (`modal.Volume`)**: 학습 데이터셋과 동적으로 로드되는 여러 LoRA 모델을 공유 볼륨에 캐시하여, 컨테이너 간 데이터 접근 속도를 높이고 중복 다운로드를 방지한다.

---

## 6. 한계 및 향후 연구 방향

- **전처리 실패 케이스**: 현재의 `rembg` 기반 캐릭터 감지 로직은 배경이 매우 복잡하거나, 여러 캐릭터가 겹쳐 있거나, 캐릭터가 너무 작게 나온 컷에서는 실패할 확률이 있다. 향후에는 객체 탐지(Object Detection) 모델(예: YOLO)을 결합하여 여러 캐릭터를 분리하고 각각을 개별적으로 처리하는 방향으로 개선할 수 있다.
- **캡셔닝 품질의 일반성**: BLIP 모델은 범용적인 캡션을 생성하지만, 특정 캐릭터의 고유한 복장이나 아이템 이름을 정확히 명명하지 못할 수 있다. 더 정교한 프롬프트 엔지니어링이나, 특정 도메인에 파인튜닝된 Vision-Language 모델을 활용하여 캡션의 질을 높이는 연구가 필요하다.
- **PEFT 기법 확장**: 현재는 LoRA만 사용하지만, 최근 제안된 DoRA(Weight-Decomposed Low-Rank Adaptation)나 LoKr 등 다른 파라미터 효율적 파인튜닝(PEFT) 기법들을 적용하여 성능을 비교, 분석하고 특정 태스크에 더 적합한 방법을 찾는 연구를 진행할 수 있다.

---

## 7. 결론

본 프로젝트는 AI 기술을 활용하여 LoRA 모델 생성의 전 과정을 자동화하고, 서버리스 아키텍처와 다양한 MLOps 최적화 기법을 통해 이를 실용적인 서비스로 구현했다는 점에서 의의를 가진다. 데이터 준비부터 모델 학습, 최종 배포에 이르기까지 각 단계에 적용된 AI 기술들은 상호 보완적으로 작용하여 전체 파이프라인의 효율성과 결과물의 품질을 극대화한다.