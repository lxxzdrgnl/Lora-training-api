# ê°œë°œ ì¤‘ ì´ìŠˆ ë° í•´ê²° ë°©ë²•

## ëª©ì°¨
1. [CLIP Skip ì„¤ì • ì¶”ê°€](#1-clip-skip-ì„¤ì •-ì¶”ê°€)
2. [VAE ìƒ‰ìƒ ì™œê³¡ ë¬¸ì œ (Fried Image)](#2-vae-ìƒ‰ìƒ-ì™œê³¡-ë¬¸ì œ-fried-image)
3. [íŒŒì´í”„ë¼ì¸ ë¡œë”© ì†ë„ ìµœì í™”](#3-íŒŒì´í”„ë¼ì¸-ë¡œë”©-ì†ë„-ìµœì í™”)
4. [ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìµœì í™”](#4-ë©”ëª¨ë¦¬-ìŠ¤ëƒ…ìƒ·-ìµœì í™”)
5. [ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìºì‹œ ë¬¸ì œ](#5-ë©”ëª¨ë¦¬-ìŠ¤ëƒ…ìƒ·-ìºì‹œ-ë¬¸ì œ)
6. [ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ê³¼ LoRA ë™ì‘ ì›ë¦¬](#6-ë©”ëª¨ë¦¬-ìŠ¤ëƒ…ìƒ·ê³¼-lora-ë™ì‘-ì›ë¦¬)
7. [LoRA Alpha ê°’ ì†ì‹¤ ë¬¸ì œ](#7-lora-alpha-ê°’-ì†ì‹¤-ë¬¸ì œ)
8. [ğŸš€ ì¢…í•© ì„±ëŠ¥ ìµœì í™”](#8-ì¢…í•©-ì„±ëŠ¥-ìµœì í™”)

---

## 1. CLIP Skip ì„¤ì • ì¶”ê°€

### ë¬¸ì œ
- ê¸°ë³¸ CLIP ì„¤ì •ìœ¼ë¡œëŠ” ì´ë¯¸ì§€ê°€ ë„ˆë¬´ í”„ë¡¬í”„íŠ¸ì— ì¶©ì‹¤í•˜ì—¬ ì°½ì˜ì„±ì´ ë¶€ì¡±í•¨
- WebUIì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ CLIP Skip=2ë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, Diffusersì—ëŠ” ì´ ì˜µì…˜ì´ ì—†ìŒ

### í•´ê²° ë°©ë²•

#### 1.1 Configì— CLIP Skip ì¶”ê°€
**íŒŒì¼:** `core/config.py`
```python
@dataclass
class InferenceConfig:
    # ... ê¸°ì¡´ í•„ë“œë“¤ ...
    clip_skip: int = 2  # CLIP Skip (1=ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ, 2=ë§ˆì§€ë§‰ 2ê°œ ë ˆì´ì–´ ìŠ¤í‚µ)
```

#### 1.2 CLIP Skip ë¡œì§ êµ¬í˜„
**íŒŒì¼:** `core/generate.py`, `modal_app.py`
```python
# CLIP Skip ì ìš© (ê¸°ë³¸ê°’ 2)
if config.clip_skip > 1:
    print(f"ğŸ¨ Applying CLIP Skip: {config.clip_skip}")
    clip_skip_layers = -config.clip_skip

    # text_encoderì˜ forward hook ë“±ë¡
    def clip_skip_hook(module, args, output):
        # hidden_statesì—ì„œ ì›í•˜ëŠ” ë ˆì´ì–´ ì„ íƒ
        if hasattr(output, 'hidden_states') and output.hidden_states is not None:
            return type(output)(
                last_hidden_state=output.hidden_states[clip_skip_layers],
                hidden_states=output.hidden_states,
                attentions=output.attentions if hasattr(output, 'attentions') else None
            )
        return output

    # output_hidden_states=True ì„¤ì •
    pipe.text_encoder.text_model.config.output_hidden_states = True
    pipe.text_encoder.text_model.register_forward_hook(clip_skip_hook)
```

### íš¨ê³¼
- âœ… WebUIì™€ ë™ì¼í•œ CLIP Skip ë™ì‘
- âœ… ë” ì°½ì˜ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì´ë¯¸ì§€ ìƒì„±

---

## 2. VAE ìƒ‰ìƒ ì™œê³¡ ë¬¸ì œ (Fried Image)

### ë¬¸ì œ ì¦ìƒ
```
Keyword arguments {'upcast_vae': True} are not expected by StableDiffusionPipeline and will be ignored.
```
- ì´ë¯¸ì§€ ìƒ‰ê°ì´ ë¶€ìì—°ìŠ¤ëŸ½ê³  ì±„ë„ê°€ ë†’ìŒ (ê¸°ë¦„ì— íŠ€ê¸´ ë“¯í•œ íš¨ê³¼)
- ì–¼ë£©ëœë£©í•œ ìƒ‰ìƒ ì™œê³¡ ë°œìƒ

### ì›ì¸ ë¶„ì„
1. `upcast_vae=True` íŒŒë¼ë¯¸í„°ê°€ `StableDiffusionPipeline.from_pretrained()`ì˜ ê³µì‹ íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹˜
2. íŒŒë¼ë¯¸í„°ê°€ ë¬´ì‹œë˜ì–´ VAEê°€ float16ìœ¼ë¡œ ì‹¤í–‰ë¨
3. Float16 VAEëŠ” ì •ë°€ë„ ë¬¸ì œë¡œ ìƒ‰ìƒ ì™œê³¡ ë°œìƒ

### í•´ê²° ë°©ë²•

#### 2.1 ì˜ëª»ëœ ë°©ë²• (ì‘ë™ ì•ˆ í•¨)
```python
# âŒ ì´ë ‡ê²Œ í•˜ë©´ ì•ˆ ë¨ - íŒŒë¼ë¯¸í„°ê°€ ë¬´ì‹œë¨
pipe = StableDiffusionPipeline.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    upcast_vae=True  # ë¬´ì‹œë¨!
)
```

#### 2.2 ì˜ëª»ëœ ìˆœì„œ (ì˜¤ë¥˜ ë°œìƒ)
```python
# âŒ ì´ë ‡ê²Œ í•˜ë©´ íƒ€ì… ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ ë°œìƒ
pipe.vae.to(dtype=torch.float32)  # VAEë§Œ float32ë¡œ ë³€í™˜
pipe.to("cuda")  # ì „ì²´ê°€ ë‹¤ì‹œ float16ìœ¼ë¡œ ë˜ëŒì•„ê°!

# ì˜¤ë¥˜: RuntimeError: Input type (c10::Half) and bias type (float) should be the same
```

#### 2.3 ì˜¬ë°”ë¥¸ í•´ê²° ë°©ë²•
**íŒŒì¼:** `core/generate.py`, `modal_app.py`

```python
# âœ… ì˜¬ë°”ë¥¸ ìˆœì„œ
pipe = StableDiffusionPipeline.from_pretrained(
    base_model_path,
    torch_dtype=torch.float16,
    safety_checker=None
)

# 1. ë¨¼ì € GPUë¡œ ì´ë™
pipe.to(device)

# 2. ê·¸ ë‹¤ìŒ VAEë§Œ float32ë¡œ ë³€í™˜ (GPU ì´ë™ í›„ì— ë³€í™˜í•´ì•¼ í•¨!)
print("ğŸ”§ Converting VAE to float32 to prevent color distortion...")
pipe.vae.to(dtype=torch.float32)
```

#### 2.4 ì ìš© ìœ„ì¹˜
1. **core/generate.py** - `load_pipeline()` í•¨ìˆ˜ (line 67-73)
2. **modal_app.py** - `LoraTrainer.load_models()` (line 181-187)
3. **modal_app.py** - `ImageGenerator.load_models()` (line 433-439)

### íš¨ê³¼
- âœ… ìƒ‰ìƒ ì™œê³¡ ë¬¸ì œ ì™„ì „ í•´ê²°
- âœ… ìì—°ìŠ¤ëŸ½ê³  ë¶€ë“œëŸ¬ìš´ ìƒ‰ê°
- âœ… "Fried image" í˜„ìƒ ì œê±°

### ì°¸ê³  ì‚¬í•­
- ì´ë¯¸ì§€ ë¹Œë“œ ì‹œ(`download_base_model_to_image`)ì—ëŠ” VAE ë³€í™˜í•˜ì§€ ì•ŠìŒ
- ëŸ°íƒ€ì„(`@modal.enter()`)ì—ì„œë§Œ VAEë¥¼ float32ë¡œ ë³€í™˜
- ì´ë ‡ê²Œ í•´ì•¼ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•¨

---

## 3. íŒŒì´í”„ë¼ì¸ ë¡œë”© ì†ë„ ìµœì í™”

### ë¬¸ì œ
- ë§¤ë²ˆ ìš”ì²­ ì‹œ íŒŒì´í”„ë¼ì¸ì„ ìƒˆë¡œ ë¡œë“œ (10-15ì´ˆ ì†Œìš”)
- ë‹¤ë¥¸ ì´ë¯¸ì§€ ì„œë¹„ìŠ¤ëŠ” 3ì´ˆ ì´ë‚´ ì‹œì‘í•˜ëŠ”ë° ìš°ë¦¬ëŠ” ë„ˆë¬´ ëŠë¦¼

### ì›ì¸ ë¶„ì„
```python
# âŒ ê¸°ì¡´ ë°©ì‹ - ë§¤ë²ˆ ìƒˆë¡œ ë¡œë“œ
@modal.method()
def generate_images(self, ...):
    from core.generate import generate_images

    # ë‚´ë¶€ì—ì„œ load_pipeline() í˜¸ì¶œ
    # â†’ ë§¤ë²ˆ ëª¨ë¸ì„ ë””ìŠ¤í¬ì—ì„œ ì½ê³  GPUì— ë¡œë“œ (10-15ì´ˆ)
    generated_files = generate_images(
        lora_path=temp_lora_path,
        config=config,
        callback=progress_callback
    )
```

### í•´ê²° ë°©ë²•

#### 3.1 íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
**íŒŒì¼:** `modal_app.py` - `ImageGenerator.generate_images()` (line 587-713)

```python
# âœ… ìƒˆë¡œìš´ ë°©ì‹ - self.pipe ì¬ì‚¬ìš©
@modal.enter()
def load_models(self):
    # ì»¨í…Œì´ë„ˆ ì‹œì‘ ì‹œ 1íšŒë§Œ ë¡œë“œ (ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì— í¬í•¨ë¨)
    self.pipe = StableDiffusionPipeline.from_pretrained(...)
    self.pipe.to("cuda")
    self.pipe.vae.to(dtype=torch.float32)

@modal.method()
def generate_images(self, ...):
    # ê¸°ì¡´ LoRA ì–¸ë¡œë“œ (ìˆë‹¤ë©´)
    if hasattr(self.pipe, '_lora_loaded'):
        self.pipe.unload_lora_weights()

    # ìƒˆ LoRAë§Œ ë¡œë“œ (0.3ì´ˆ ì†Œìš”)
    lora_file = os.path.join(temp_lora_path, safetensors_files[0])
    self.pipe.load_lora_weights(lora_file)
    self.pipe._lora_loaded = True

    # CLIP Skip ì ìš© (ìµœì´ˆ 1íšŒë§Œ)
    if not hasattr(self.pipe, '_clip_skip_applied'):
        # ... CLIP Skip ë¡œì§ ...
        self.pipe._clip_skip_applied = True

    # ë°”ë¡œ ì´ë¯¸ì§€ ìƒì„± (self.pipe ì¬ì‚¬ìš©!)
    image = self.pipe(
        prompt=full_prompt,
        negative_prompt=negative_prompt,
        num_inference_steps=steps,
        guidance_scale=guidance_scale,
        generator=generator,
        cross_attention_kwargs={"scale": lora_scale}
    ).images[0]
```

### ì„±ëŠ¥ ë¹„êµ

| ë‹¨ê³„ | ê¸°ì¡´ ì‹œê°„ | ìµœì í™” í›„ | ê°œì„ ìœ¨ |
|------|----------|----------|--------|
| íŒŒì´í”„ë¼ì¸ ë¡œë“œ | 10-15ì´ˆ | **0ì´ˆ** (ì¬ì‚¬ìš©) | **âˆ** |
| LoRA ë¡œë“œ | 10-15ì´ˆ | **0.3ì´ˆ** | **50ë°°** |
| ì´ë¯¸ì§€ ìƒì„± (20 steps) | 6-8ì´ˆ | 6-8ì´ˆ | ë™ì¼ |
| **ì´ ì†Œìš” ì‹œê°„** | **26-38ì´ˆ** | **6.3-8.3ì´ˆ** | **4-6ë°° ë¹ ë¦„** |

### íš¨ê³¼
- âœ… íŒŒì´í”„ë¼ì¸ ë¡œë”© ì‹œê°„ ì™„ì „ ì œê±°
- âœ… LoRA êµì²´ë§Œ 0.3ì´ˆë¡œ ë§¤ìš° ë¹ ë¦„
- âœ… ì „ì²´ ì‘ë‹µ ì‹œê°„ 6-8ë°° ê°œì„ 

---

## 4. ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìµœì í™”

### Modal ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì´ë€?
- `enable_memory_snapshot=True` ì„¤ì • ì‹œ ì»¨í…Œì´ë„ˆì˜ ë©”ëª¨ë¦¬ ìƒíƒœë¥¼ ì €ì¥
- ì²« ì»¨í…Œì´ë„ˆ: ëª¨ë¸ ë¡œë“œ + ìŠ¤ëƒ…ìƒ· ìƒì„± (ëŠë¦¼, 30-40ì´ˆ)
- ì´í›„ ì»¨í…Œì´ë„ˆ: ìŠ¤ëƒ…ìƒ·ì—ì„œ ë³µì› (ë¹ ë¦„, 2-3ì´ˆ)

### ë¬¸ì œ
- ë§¤ë²ˆ `@modal.enter()`ê°€ ì‹¤í–‰ë˜ì–´ ìŠ¤ëƒ…ìƒ·ì´ ì œëŒ€ë¡œ ì‘ë™ ì•ˆ í•¨
- ì´ë¯¸ì§€ ë¹Œë“œ ì‹œ VAEë¥¼ ë³€í™˜í•˜ë©´ ëŸ°íƒ€ì„ê³¼ ë¶ˆì¼ì¹˜

### í•´ê²° ë°©ë²•

#### 4.1 ì´ë¯¸ì§€ ë¹Œë“œ ì‹œ VAE ë³€í™˜ ì œê±°
**íŒŒì¼:** `modal_app.py` - `download_base_model_to_image()` (line 25-35)

```python
# âŒ ê¸°ì¡´ ë°©ì‹ - ë¹Œë“œ ì‹œ VAE ë³€í™˜
pipe = StableDiffusionPipeline.from_pretrained(...)
pipe.vae.to(dtype=torch.float32)  # ì´ë ‡ê²Œ í•˜ë©´ ìŠ¤ëƒ…ìƒ·ì´ ì•ˆ ë§ìŒ
pipe.save_pretrained(cache_dir)

# âœ… ì˜¬ë°”ë¥¸ ë°©ì‹ - ë¹Œë“œ ì‹œì—ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ
pipe = StableDiffusionPipeline.from_pretrained(...)
# ëª¨ë¸ì„ ë¡œì»¬ì— ì €ì¥ (VAEëŠ” ë‚˜ì¤‘ì— ëŸ°íƒ€ì„ì— float32ë¡œ ë³€í™˜)
pipe.save_pretrained(cache_dir)
```

#### 4.2 ëŸ°íƒ€ì„ì—ì„œë§Œ VAE ë³€í™˜
**íŒŒì¼:** `modal_app.py` - `ImageGenerator.load_models()` (line 398-442)

```python
@modal.enter()
def load_models(self):
    # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
    self.pipe = StableDiffusionPipeline.from_pretrained(
        self.base_model_path,
        torch_dtype=torch.float16,
        safety_checker=None
    )

    # GPUë¡œ ì´ë™
    self.pipe.to("cuda")

    # VAEë¥¼ fp32ë¡œ ë³€í™˜ (ëŸ°íƒ€ì„ì—ì„œë§Œ!)
    self.pipe.vae.to(dtype=torch.float32)

    # ì´ ìƒíƒœê°€ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ì €ì¥ë¨
```

### ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ë™ì‘ íë¦„

```
[ì²« ë²ˆì§¸ ì»¨í…Œì´ë„ˆ]
1. ì»¨í…Œì´ë„ˆ ì‹œì‘
2. @modal.enter() ì‹¤í–‰
   - ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ (15ì´ˆ)
   - GPU ì´ë™ (2ì´ˆ)
   - VAE float32 ë³€í™˜ (1ì´ˆ)
3. ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìƒì„± (5ì´ˆ)
   ì´ ì†Œìš”: 23ì´ˆ

[ë‘ ë²ˆì§¸ ì»¨í…Œì´ë„ˆ ì´í›„]
1. ì»¨í…Œì´ë„ˆ ì‹œì‘
2. ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì—ì„œ ë³µì› (2ì´ˆ)
   - @modal.enter() ìŠ¤í‚µ!
   - ëª¨ë¸ì´ ì´ë¯¸ GPUì— ë¡œë“œëœ ìƒíƒœ
3. ë°”ë¡œ ìš”ì²­ ì²˜ë¦¬ ê°€ëŠ¥
   ì´ ì†Œìš”: 2ì´ˆ
```

### í™•ì¸ ë°©ë²•
ë¡œê·¸ì—ì„œ ë‹¤ìŒ ë©”ì‹œì§€ë¥¼ í™•ì¸:

```bash
# ì²« ì»¨í…Œì´ë„ˆ
ğŸš€ Initializing Image Generator...
ğŸ“¦ Loading base model from: /base_models/anything-v5
ğŸ”§ Converting VAE to float32 to prevent color distortion...
âœ… Base model loaded and ready!
ğŸ’¾ Memory snapshot will be created after this initialization
Creating CPU memory snapshot for Function.
Snapshot created.

# ì´í›„ ì»¨í…Œì´ë„ˆ
Restoring Function from memory snapshot.
Restoring Function from memory snapshot.
âš¡ Loading LoRA (fast): /cache/lora_models/model_6/xxx.safetensors
```

### íš¨ê³¼
- âœ… ìŠ¤ëƒ…ìƒ· ë³µì› ì‹œê°„: 2-3ì´ˆ
- âœ… íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©ìœ¼ë¡œ ì¶”ê°€ ë¡œë”© ì—†ìŒ
- âœ… ì´ ì‹œì‘ ì‹œê°„: 2-3ì´ˆ (ìŠ¤ëƒ…ìƒ·) + 0.3ì´ˆ (LoRA) = **2.3-3.3ì´ˆ**

---

## 5. Keep Warm ì„¤ì • (ì„ íƒì‚¬í•­)

### Keep Warmì´ë€?
- í•­ìƒ Nê°œì˜ ì»¨í…Œì´ë„ˆë¥¼ warm ìƒíƒœë¡œ ìœ ì§€
- Cold start ì—†ì´ ì¦‰ì‹œ ì‘ë‹µ ê°€ëŠ¥

### ì„¤ì • ë°©ë²•
**íŒŒì¼:** `modal_app.py` - `ImageGenerator` í´ë˜ìŠ¤

```python
@app.cls(
    # ... ê¸°íƒ€ ì„¤ì • ...
    keep_warm=1,  # í•­ìƒ 1ê°œ ì»¨í…Œì´ë„ˆë¥¼ warm ìƒíƒœë¡œ ìœ ì§€ (ì¦‰ì‹œ ì‘ë‹µ)
    container_idle_timeout=300,  # 5ë¶„ê°„ ìœ íœ´ ìƒíƒœë©´ ì¢…ë£Œ
)
class ImageGenerator:
    ...
```

### ë¹„ìš© ê³ ë ¤ì‚¬í•­
- **GPU T4 ë¹„ìš©:** ~$0.60/ì‹œê°„
- **24ì‹œê°„ ìœ ì§€:** $14.4/ì¼ (ì•½ 432$/ì›”)
- **ê¶Œì¥:** í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œë§Œ í™œì„±í™”

### í˜„ì¬ ì„¤ì •
```python
# keep_warm=1,  # ë¹„ìš© ì ˆê°ì„ ìœ„í•´ ë¹„í™œì„±í™” (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
```
- ê°œë°œ ì¤‘ì—ëŠ” ë¹„í™œì„±í™”
- í”„ë¡œë•ì…˜ ë°°í¬ ì‹œ í•„ìš”í•˜ë©´ ì£¼ì„ í•´ì œ

---

## ìš”ì•½

### í•´ê²°í•œ ì´ìŠˆë“¤
1. âœ… CLIP Skip 2 ì ìš© - ë” ì°½ì˜ì ì¸ ì´ë¯¸ì§€
2. âœ… VAE float32 ë³€í™˜ - ìƒ‰ìƒ ì™œê³¡ í•´ê²°
3. âœ… íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš© - ë¡œë”© ì‹œê°„ ì œê±°
4. âœ… ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìµœì í™” - ë¹ ë¥¸ ì»¨í…Œì´ë„ˆ ì‹œì‘

### ìµœì¢… ì„±ëŠ¥
| ì§€í‘œ | ìµœì í™” ì „ | ìµœì í™” í›„ |
|------|----------|----------|
| ì²« ì‹œì‘ (Cold) | 50-60ì´ˆ | 30-40ì´ˆ |
| ìŠ¤ëƒ…ìƒ· ë³µì› | 10-15ì´ˆ | **2-3ì´ˆ** |
| íŒŒì´í”„ë¼ì¸ ë¡œë“œ | 10-15ì´ˆ | **0ì´ˆ** (ì¬ì‚¬ìš©) |
| LoRA ë¡œë“œ | 10-15ì´ˆ | **0.3ì´ˆ** |
| ì´ë¯¸ì§€ ìƒì„± (20 steps) | 6-8ì´ˆ | 6-8ì´ˆ |
| **ì´ ì‘ë‹µ ì‹œê°„** | **36-48ì´ˆ** | **8-11ì´ˆ** |

### ê°œì„ ìœ¨
- âš¡ **4-6ë°° ë¹ ë¥¸ ì‘ë‹µ**
- ğŸ¨ **ë” ë‚˜ì€ ì´ë¯¸ì§€ í’ˆì§ˆ** (CLIP Skip, VAE float32)
- ğŸ’° **ë¹„ìš© íš¨ìœ¨ì ** (keep_warm ë¹„í™œì„±í™”)

---

## ë°°í¬ ë°©ë²•

```bash
# Modal ë°°í¬
modal deploy modal_app.py

# ë°°í¬ í™•ì¸
modal app list

# ë¡œê·¸ í™•ì¸
modal app logs lora-training-inference
```

---

---

## 5. ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìºì‹œ ë¬¸ì œ

### ë¬¸ì œ ì¦ìƒ
ì½”ë“œë¥¼ ìˆ˜ì •í•˜ê³  ë°°í¬í•´ë„ **VAE dtype ì˜¤ë¥˜ê°€ ê³„ì† ë°œìƒ**í–ˆìŠµë‹ˆë‹¤.
```
RuntimeError: Input type (c10::Half) and bias type (float) should be the same
```

### ê·¼ë³¸ ì›ì¸
**ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì´ ì˜ˆì „ ìƒíƒœ(VAE float16)ë¡œ ì €ì¥ë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤.**

Modalì˜ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì€ `@modal.enter()` ì‹¤í–‰ í›„ í•œ ë²ˆ ìƒì„±ë˜ë©´:
- ì´í›„ ì»¨í…Œì´ë„ˆë“¤ì€ **ì €ì¥ëœ ìŠ¤ëƒ…ìƒ·ì—ì„œ ë³µì›**
- `@modal.enter()`ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
- ì½”ë“œë¥¼ ìˆ˜ì •í•´ë„ **ìŠ¤ëƒ…ìƒ·ì€ ì—…ë°ì´íŠ¸ë˜ì§€ ì•ŠìŒ!**

### ë¬¸ì œ ìƒí™©
```python
# ì½”ë“œ ìˆ˜ì • ì „ (VAE float16)
@modal.enter()
def load_models(self):
    self.pipe = StableDiffusionPipeline.from_pretrained(...)
    self.pipe.to("cuda")
    # VAE ë³€í™˜ ì—†ìŒ
    # â†’ ì´ ìƒíƒœë¡œ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìƒì„±ë¨ (VAE float16)

# ì½”ë“œ ìˆ˜ì • í›„ (VAE float32)
@modal.enter()
def load_models(self):
    self.pipe = StableDiffusionPipeline.from_pretrained(...)
    self.pipe.to("cuda")
    self.pipe.vae.to(dtype=torch.float32)  # â† ì¶”ê°€í–ˆì§€ë§Œ...
    # â†’ ê¸°ì¡´ ìŠ¤ëƒ…ìƒ·ì´ ì‚¬ìš©ë˜ì–´ ì´ ì½”ë“œê°€ ì‹¤í–‰ ì•ˆ ë¨!
```

### í•´ê²° ë°©ë²• 1: ì•± ì´ë¦„ ë³€ê²½ (ê¶Œì¥)
**íŒŒì¼:** `modal_app.py`

```python
# ê¸°ì¡´
app = modal.App("lora-training-inference")

# ë³€ê²½ (ìƒˆë¡œìš´ ìŠ¤ëƒ…ìƒ· ìƒì„±)
app = modal.App("lora-training-inference-v2")
```

**íš¨ê³¼:**
- ìƒˆë¡œìš´ ì•± ì´ë¦„ = ìƒˆë¡œìš´ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·
- ì²« ì»¨í…Œì´ë„ˆì—ì„œ ìµœì‹  ì½”ë“œë¡œ ìŠ¤ëƒ…ìƒ· ìƒì„±
- ì´í›„ ì»¨í…Œì´ë„ˆëŠ” ìƒˆ ìŠ¤ëƒ…ìƒ· ì‚¬ìš©

### í•´ê²° ë°©ë²• 2: Modal CLIë¡œ ìŠ¤ëƒ…ìƒ· ì‚­ì œ

```bash
# ê¸°ì¡´ ì•± ì¤‘ì§€
modal app stop lora-training-inference

# ê¸°ì¡´ ì•± ì‚­ì œ (ìŠ¤ëƒ…ìƒ·ë„ í•¨ê»˜ ì‚­ì œë¨)
modal app delete lora-training-inference

# ì¬ë°°í¬
modal deploy modal_app.py
```

### í•´ê²° ë°©ë²• 3: VAE ê°•ì œ ì¬ë³€í™˜

ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ë¬¸ì œë¥¼ ìš°íšŒí•˜ê¸° ìœ„í•´ **LoRA ë¡œë“œ ì‹œë§ˆë‹¤ VAEë¥¼ ë‹¤ì‹œ float32ë¡œ ë³€í™˜**:

```python
@modal.method()
def generate_images(self, ...):
    # LoRA ë¡œë“œ
    self.pipe.load_lora_weights(lora_file)

    # VAEë¥¼ ê°•ì œë¡œ ë‹¤ì‹œ float32ë¡œ ë³€í™˜
    self.pipe.vae = self.pipe.vae.to(dtype=torch.float32)
    if hasattr(self.pipe.vae, 'post_quant_conv'):
        self.pipe.vae.post_quant_conv = self.pipe.vae.post_quant_conv.to(dtype=torch.float32)
    if hasattr(self.pipe.vae, 'decoder'):
        self.pipe.vae.decoder = self.pipe.vae.decoder.to(dtype=torch.float32)
    if hasattr(self.pipe.vae, 'encoder'):
        self.pipe.vae.encoder = self.pipe.vae.encoder.to(dtype=torch.float32)
```

### ìµœì¢… í•´ê²°ì±… (ì ìš©ë¨)

**1. ì•± ì´ë¦„ ë³€ê²½** (modal_app.py:12)
```python
app = modal.App("lora-training-inference-v2")
```

**2. VAE tiling í™œì„±í™” + ëª…ì‹œì  ë³€í™˜** (modal_app.py:178-192)
```python
# VAE tiling í™œì„±í™” (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ê³  float32 ìœ ì§€ì— ë„ì›€)
self.pipe.enable_vae_tiling()

# VAEë¥¼ ëª…ì‹œì ìœ¼ë¡œ float32ë¡œ ì„¤ì •
self.pipe.vae = self.pipe.vae.to(dtype=torch.float32)

# VAEì˜ ê° ì„œë¸Œëª¨ë“ˆì„ í™•ì‹¤í•˜ê²Œ float32ë¡œ ë³€í™˜
if hasattr(self.pipe.vae, 'post_quant_conv'):
    self.pipe.vae.post_quant_conv = self.pipe.vae.post_quant_conv.to(dtype=torch.float32)
if hasattr(self.pipe.vae, 'decoder'):
    self.pipe.vae.decoder = self.pipe.vae.decoder.to(dtype=torch.float32)
if hasattr(self.pipe.vae, 'encoder'):
    self.pipe.vae.encoder = self.pipe.vae.encoder.to(dtype=torch.float32)
```

**3. LoRA ë¡œë“œ í›„ ì¬í™•ì¸** (modal_app.py:637-653)
```python
# LoRA ë¡œë“œ í›„ VAEë¥¼ ë‹¤ì‹œ float32ë¡œ ì¬ì„¤ì •
self.pipe.vae = self.pipe.vae.to(dtype=torch.float32)
if hasattr(self.pipe.vae, 'post_quant_conv'):
    self.pipe.vae.post_quant_conv = self.pipe.vae.post_quant_conv.to(dtype=torch.float32)
# ... (decoder, encoderë„ ë™ì¼)
```

### êµí›ˆ
- âœ… **ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì€ í•œ ë²ˆ ìƒì„±ë˜ë©´ ìºì‹œë¨**
- âœ… **ì½”ë“œ ìˆ˜ì • í›„ì—ëŠ” ì•± ì´ë¦„ì„ ë³€ê²½í•˜ê±°ë‚˜ ìŠ¤ëƒ…ìƒ·ì„ ì‚­ì œ**
- âœ… **ì¤‘ìš”í•œ ì´ˆê¸°í™”ëŠ” ë§¤ ìš”ì²­ë§ˆë‹¤ ì¬í™•ì¸í•˜ëŠ” ê²ƒì´ ì•ˆì „**

---

## 6. ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ê³¼ LoRA ë™ì‘ ì›ë¦¬

### ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì˜ ë²”ìœ„

**ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì— í¬í•¨ë˜ëŠ” ê²ƒ âœ…**
```python
@modal.enter()
def load_models(self):
    # ì´ ë¶€ë¶„ì´ ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì— ì €ì¥ë¨
    self.pipe = StableDiffusionPipeline.from_pretrained(
        "/base_models/anything-v5",  # â† ë² ì´ìŠ¤ ëª¨ë¸ë§Œ!
        torch_dtype=torch.float16,
        safety_checker=None
    )
    self.pipe.to("cuda")
    self.pipe.vae = self.pipe.vae.to(dtype=torch.float32)
    # â† ì—¬ê¸°ê¹Œì§€ ìŠ¤ëƒ…ìƒ·ì— ì €ì¥
```

**ìŠ¤ëƒ…ìƒ· ë‚´ìš©:**
- âœ… Anything-v5 ë² ì´ìŠ¤ ëª¨ë¸ (ì „ì²´ ~4GB)
- âœ… VAE (float32)
- âœ… UNet
- âœ… Text Encoder
- âœ… CLIP Skip ì„¤ì •
- âŒ **LoRA ëª¨ë¸ (í¬í•¨ ì•ˆ ë¨!)**

**ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì— í¬í•¨ë˜ì§€ ì•ŠëŠ” ê²ƒ âŒ**
```python
@modal.method()
def generate_images(self, ...):
    # ì´ ë¶€ë¶„ì€ ë§¤ ìš”ì²­ë§ˆë‹¤ ì‹¤í–‰ë¨ (ìŠ¤ëƒ…ìƒ·ì— í¬í•¨ ì•ˆ ë¨)

    # LoRA ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (S3ì—ì„œ)
    s3_client.download_file(bucket, s3_key, local_file)

    # LoRA ë¡œë“œ (ë™ì ìœ¼ë¡œ!)
    self.pipe.load_lora_weights(lora_file)  # â† ë§¤ë²ˆ ìƒˆë¡œ ë¡œë“œ!
```

### ì™œ LoRAëŠ” ìŠ¤ëƒ…ìƒ·ì— í¬í•¨í•˜ì§€ ì•Šë‚˜?

**ë§Œì•½ LoRAë¥¼ ìŠ¤ëƒ…ìƒ·ì— í¬í•¨í•˜ë©´:**
```
ì‚¬ìš©ì Aì˜ ëª¨ë¸: animal_crossing.safetensors
ì‚¬ìš©ì Bì˜ ëª¨ë¸: portrait_style.safetensors
ì‚¬ìš©ì Cì˜ ëª¨ë¸: anime_style.safetensors

â†’ 3ê°œì˜ ë‹¤ë¥¸ ìŠ¤ëƒ…ìƒ· í•„ìš” (ì‚¬ìš©ìë§ˆë‹¤!)
â†’ ìŠ¤ëƒ…ìƒ· ìƒì„± ì‹œê°„ ì¦ê°€ (ê° 30ì´ˆ ì´ìƒ)
â†’ ìŠ¤í† ë¦¬ì§€ ë‚­ë¹„ (ë² ì´ìŠ¤ ëª¨ë¸ì´ ì¤‘ë³µ ì €ì¥)
â†’ ìŠ¤ëƒ…ìƒ·ì˜ ì˜ë¯¸ê°€ ì—†ì–´ì§
```

**í˜„ì¬ ë°©ì‹ (LoRA ë™ì  ë¡œë“œ):**
```
ë² ì´ìŠ¤ ëª¨ë¸ ìŠ¤ëƒ…ìƒ· 1ê°œ (ëª¨ë“  ì‚¬ìš©ì ê³µìœ )
â””â”€ LoRAëŠ” ìš”ì²­ ì‹œ ë™ì  ë¡œë“œ (0.3ì´ˆ)
   â”œâ”€ ì‚¬ìš©ì A â†’ animal_crossing.safetensors ë¡œë“œ
   â”œâ”€ ì‚¬ìš©ì B â†’ portrait_style.safetensors ë¡œë“œ
   â””â”€ ì‚¬ìš©ì C â†’ anime_style.safetensors ë¡œë“œ

âœ… íš¨ìœ¨ì !
âœ… ë¹ ë¦„!
âœ… í™•ì¥ ê°€ëŠ¥!
```

### ì „ì²´ ë™ì‘ íë¦„

```
[ì»¨í…Œì´ë„ˆ ì‹œì‘]
1. ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ë³µì› (2-3ì´ˆ)
   â”œâ”€ ë² ì´ìŠ¤ ëª¨ë¸: Anything-v5 âœ…
   â”œâ”€ VAE: float32 âœ…
   â”œâ”€ UNet: float16 âœ…
   â”œâ”€ Text Encoder: float16 âœ…
   â””â”€ LoRA: ì—†ìŒ âŒ

[ìš”ì²­ 1: ì‚¬ìš©ì A, ëª¨ë¸ ID = 6]
2. S3ì—ì„œ LoRA ë‹¤ìš´ë¡œë“œ
   â””â”€ /cache/lora_models/model_6/animal_crossing.safetensors
3. LoRAë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ì ìš©
   â””â”€ self.pipe.load_lora_weights(...)
4. VAE float32 ì¬í™•ì¸
5. ì´ë¯¸ì§€ ìƒì„± âœ…

[ìš”ì²­ 2: ì‚¬ìš©ì B, ëª¨ë¸ ID = 7]
6. ì´ì „ LoRA ì–¸ë¡œë“œ (model_6)
7. S3ì—ì„œ ìƒˆ LoRA ë‹¤ìš´ë¡œë“œ
   â””â”€ /cache/lora_models/model_7/portrait_style.safetensors
8. ìƒˆ LoRAë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ì ìš©
9. VAE float32 ì¬í™•ì¸
10. ì´ë¯¸ì§€ ìƒì„± âœ…

[ìš”ì²­ 3: ì‚¬ìš©ì A, ëª¨ë¸ ID = 6 ë‹¤ì‹œ]
11. ì´ì „ LoRA ì–¸ë¡œë“œ (model_7)
12. ìºì‹œëœ LoRA ì‚¬ìš© (model_6) - ë‹¤ìš´ë¡œë“œ ìŠ¤í‚µ! ë¹ ë¦„!
13. LoRAë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ì— ì ìš©
14. VAE float32 ì¬í™•ì¸
15. ì´ë¯¸ì§€ ìƒì„± âœ…
```

### LoRA ìºì‹± êµ¬ì¡°

```
/cache/lora_models/
â”œâ”€â”€ model_6/
â”‚   â””â”€â”€ animal_crossing.safetensors  (100MB)
â”œâ”€â”€ model_7/
â”‚   â””â”€â”€ portrait_style.safetensors   (100MB)
â””â”€â”€ model_8/
    â””â”€â”€ anime_style.safetensors      (100MB)

íŠ¹ì§•:
- í•œ ë²ˆ ë‹¤ìš´ë¡œë“œí•˜ë©´ ìºì‹œë¨
- ê°™ì€ ëª¨ë¸ ì¬ì‚¬ìš© ì‹œ ì¦‰ì‹œ ë¡œë“œ (0.3ì´ˆ)
- ì»¨í…Œì´ë„ˆ ê°„ ìºì‹œ ê³µìœ  (Modal Volume ì‚¬ìš©)
- ì»¨í…Œì´ë„ˆ ì¢…ë£Œ ì‹œ ìºì‹œ ìœ ì§€ (Volumeì— ì €ì¥)
- scaledown_window(5ë¶„) í›„ ì»¨í…Œì´ë„ˆ ì¢…ë£Œë˜ì–´ë„ Volumeì€ ìœ ì§€
```

### ì„±ëŠ¥ ë¹„êµ

| ì‹œë‚˜ë¦¬ì˜¤ | ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© | LoRA ë¡œë”© | ì´ ì‹œê°„ |
|---------|----------------|----------|---------|
| **Cold Start** (ì²« ì»¨í…Œì´ë„ˆ) | 30ì´ˆ (1íšŒë§Œ) | 1ì´ˆ (S3 ë‹¤ìš´ë¡œë“œ) | 31ì´ˆ |
| **Warm Start** (ìŠ¤ëƒ…ìƒ· ë³µì›) | 2-3ì´ˆ (ë³µì›) | 1ì´ˆ (S3 ë‹¤ìš´ë¡œë“œ) | 3-4ì´ˆ |
| **Cached LoRA** (ìºì‹œëœ ëª¨ë¸) | 2-3ì´ˆ (ë³µì›) | 0.3ì´ˆ (ìºì‹œ) | 2.3-3.3ì´ˆ âš¡ |

### í•µì‹¬ ê°œë… ì •ë¦¬

**1. ë² ì´ìŠ¤ ëª¨ë¸ (ìŠ¤ëƒ…ìƒ·ì— í¬í•¨)**
- Anything-v5
- ëª¨ë“  ì‚¬ìš©ìê°€ ê³µìœ 
- ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·ì— ì €ì¥
- í•œ ë²ˆ ë¡œë“œí•˜ë©´ ë¹ ë¥´ê²Œ ë³µì› (2-3ì´ˆ)

**2. LoRA ëª¨ë¸ (ë™ì  ë¡œë“œ)**
- ì‚¬ìš©ìë³„ë¡œ ë‹¤ë¦„
- S3ì— ì €ì¥
- ìš”ì²­ ì‹œë§ˆë‹¤ ë™ì ìœ¼ë¡œ ë¡œë“œ
- ìºì‹œë˜ë©´ ë¹ ë¦„ (0.3ì´ˆ)

**3. ì™œ ì´ë ‡ê²Œ ì„¤ê³„í–ˆë‚˜?**
- âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨**: ë² ì´ìŠ¤ ëª¨ë¸ 1ê°œë§Œ ë©”ëª¨ë¦¬ì— ìœ ì§€
- âœ… **ë¹ ë¥¸ ì‘ë‹µ**: ìŠ¤ëƒ…ìƒ· ë³µì› (2-3ì´ˆ) + LoRA ë¡œë“œ (0.3ì´ˆ)
- âœ… **í™•ì¥ì„±**: ìˆ˜ì²œ ê°œì˜ LoRA ëª¨ë¸ ì§€ì› ê°€ëŠ¥
- âœ… **ë¹„ìš© íš¨ìœ¨**: ì»¨í…Œì´ë„ˆë‹¹ ë¹„ìš© ìµœì†Œí™”

**4. ì‹¤ì œ ì‚¬ìš©ì ê²½í—˜**
```
[ì‚¬ìš©ì A - ì²« ìš”ì²­]
- ëŒ€ê¸° ì‹œê°„: 3-4ì´ˆ
- ì²´ê°: "ë¹ ë¥´ë„¤!"

[ì‚¬ìš©ì A - ë‘ ë²ˆì§¸ ìš”ì²­ (ê°™ì€ ëª¨ë¸)]
- ëŒ€ê¸° ì‹œê°„: 2.3ì´ˆ
- ì²´ê°: "ì—„ì²­ ë¹ ë¥´ë„¤!"

[ì‚¬ìš©ì B - ì²« ìš”ì²­ (ë‹¤ë¥¸ ëª¨ë¸)]
- ëŒ€ê¸° ì‹œê°„: 3-4ì´ˆ
- ì²´ê°: "ë¹ ë¥´ë„¤!"
```

### ê²°ë¡ 

- ğŸ¯ **ìŠ¤ëƒ…ìƒ· = ë² ì´ìŠ¤ ëª¨ë¸** (ê³µìœ  ìì›)
- ğŸ¯ **LoRA = ë™ì  ë¡œë“œ** (ê°œì¸ ìì›)
- ğŸ¯ **ìºì‹± = ì„±ëŠ¥ ìµœì í™”** (ìì£¼ ì“°ëŠ” LoRAëŠ” ë¹ ë¦„)
- ğŸ¯ **ì´ êµ¬ì¡°ê°€ ê°€ì¥ íš¨ìœ¨ì !** (ì†ë„ + ë¹„ìš© + í™•ì¥ì„±)

---

## ë°°í¬ ë°©ë²•

```bash
# Modal ë°°í¬
modal deploy modal_app.py

# ë°°í¬ í™•ì¸
modal app list

# ë¡œê·¸ í™•ì¸
modal app logs lora-training-inference-v2  # â† ì•± ì´ë¦„ ë³€ê²½ë¨!
```

---

## 7. LoRA Alpha ê°’ ì†ì‹¤ ë¬¸ì œ

### ë¬¸ì œ ì¦ìƒ
- LoRA ëª¨ë¸ íŠ¹ì§•ì´ ì „í˜€ ì‚´ì•„ë‚˜ì§€ ì•ŠìŒ
- WebUI í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•œ Reze.safetensorsê°€ ì œëŒ€ë¡œ ì‘ë™ ì•ˆ í•¨
- ì´ë¯¸ì§€ê°€ ë² ì´ìŠ¤ ëª¨ë¸ ìŠ¤íƒ€ì¼ë¡œë§Œ ìƒì„±ë¨

### ì›ì¸ ë¶„ì„

#### LoRA ìŠ¤ì¼€ì¼ë§ ê³µì‹
LoRAëŠ” ë‹¤ìŒ ê³µì‹ìœ¼ë¡œ ì›ë³¸ ê°€ì¤‘ì¹˜ì— ì ìš©ë©ë‹ˆë‹¤:
```
W_new = W_original + (alpha / r) Ã— (lora_B @ lora_A)
```

- `alpha`: LoRA ìŠ¤ì¼€ì¼ë§ ê°•ë„ (í•™ìŠµ ì‹œ ì„¤ì •, ì˜ˆ: 64)
- `r`: LoRA rank (í•™ìŠµ ì‹œ ì„¤ì •, ì˜ˆ: 32)
- `alpha / r = 64 / 32 = 2.0` â†’ ì‹¤ì œ ì ìš© ê°•ë„

#### ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸
PEFT í˜•ì‹ì„ WebUI í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•  ë•Œ **alpha ê°’ì´ ì†ì‹¤**ë˜ì—ˆìŠµë‹ˆë‹¤:

**PEFT í˜•ì‹ (ì •ìƒ):**
```
my_lora_model/
â”œâ”€â”€ adapter_config.json  â† lora_alpha=64, lora_r=32 í¬í•¨ âœ…
â””â”€â”€ adapter_model.safetensors
```

**WebUI í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ê¸°ì¡´ ë°©ì‹ - ë¬¸ì œ):**
```python
def save_lora_as_webui(unet_model, save_path):
    peft_state_dict = {k: v for k, v in unet_model.state_dict().items() if 'lora' in k}
    webui_state_dict = convert_peft_to_webui(peft_state_dict)
    save_file(webui_state_dict, save_path)  # â† alpha ì •ë³´ ì—†ìŒ! âŒ
```

ê²°ê³¼:
```
Reze.safetensors (256ê°œ ê°€ì¤‘ì¹˜ë§Œ)
â”œâ”€â”€ lora_unet_*.lora_down.weight
â””â”€â”€ lora_unet_*.lora_up.weight
âŒ alpha ê°’ ì—†ìŒ â†’ Diffusersê°€ alpha=1ë¡œ ê°€ì •
âŒ ì‹¤ì œ ê°•ë„: 1/32 = 0.03125 (ë„ˆë¬´ ì•½í•¨!)
âœ… í•„ìš” ê°•ë„: 64/32 = 2.0
```

### í•´ê²° ë°©ë²•

#### 1. lora_utils.py ìˆ˜ì • - alpha ê°’ í¬í•¨
**íŒŒì¼:** `core/lora_utils.py`

```python
def save_lora_as_webui(unet_model, save_path, lora_alpha=64, lora_rank=32):
    """
    PEFT UNet ëª¨ë¸ì—ì„œ LoRA ê°€ì¤‘ì¹˜ë¥¼ WebUI í˜•ì‹ìœ¼ë¡œ ì €ì¥

    Args:
        lora_alpha: LoRA alpha ê°’ (ìŠ¤ì¼€ì¼ë§) - í•™ìŠµ ì‹œ ì„¤ì •í•œ ê°’
        lora_rank: LoRA rank ê°’ - í•™ìŠµ ì‹œ ì„¤ì •í•œ r ê°’
    """
    # LoRA ê°€ì¤‘ì¹˜ ì¶”ì¶œ
    peft_state_dict = {k: v.cpu() for k, v in unet_model.state_dict().items() if 'lora' in k}
    webui_state_dict = convert_peft_to_webui(peft_state_dict)

    # ë©”íƒ€ë°ì´í„°ì— alpha ê°’ ì¶”ê°€ (ê° LoRA ë ˆì´ì–´ë§ˆë‹¤)
    for key in webui_state_dict.keys():
        if '.lora_up.weight' in key or '.lora_down.weight' in key:
            base_key = key.rsplit('.', 2)[0]
            alpha_key = f"{base_key}.alpha"
            if alpha_key not in webui_state_dict:
                # alpha ê°’ì„ tensorë¡œ ì €ì¥
                webui_state_dict[alpha_key] = torch.tensor(lora_alpha, dtype=torch.float32)

    save_file(webui_state_dict, save_path)
```

#### 2. train.py ìˆ˜ì • - alphaì™€ rank ì „ë‹¬
**íŒŒì¼:** `core/train.py` (line 329-334)

```python
# WebUI/Civitai í˜•ì‹ìœ¼ë¡œ ì €ì¥ (alpha ê°’ í¬í•¨)
from .lora_utils import save_lora_as_webui

safetensors_path = os.path.join(checkpoint_dir, "lora_weights.safetensors")
save_lora_as_webui(
    unet,
    safetensors_path,
    lora_alpha=config.lora_alpha,  # 64
    lora_rank=config.lora_r        # 32
)
```

#### 3. Reze.safetensors ì¬ìƒì„±
ê¸°ì¡´ íŒŒì¼ì„ alpha ê°’ì´ í¬í•¨ëœ ë²„ì „ìœ¼ë¡œ ë‹¤ì‹œ ìƒì„±:

```bash
source venv/bin/activate && python -c "
from safetensors.torch import load_file, save_file
from core.lora_utils import convert_peft_to_webui
import torch

# PEFT í˜•ì‹ ë¡œë“œ
peft_weights = load_file('my_lora_model/adapter_model.safetensors')
webui_weights = convert_peft_to_webui(peft_weights)

# alpha ê°’ ì¶”ê°€
lora_alpha = 64
for key in list(webui_weights.keys()):
    if '.lora_up.weight' in key or '.lora_down.weight' in key:
        base_key = key.rsplit('.', 2)[0]
        alpha_key = f'{base_key}.alpha'
        if alpha_key not in webui_weights:
            webui_weights[alpha_key] = torch.tensor(lora_alpha, dtype=torch.float32)

# ì €ì¥
save_file(webui_weights, 'my_lora_model/Reze.safetensors')
"
```

### ê²€ì¦

#### íŒŒì¼ êµ¬ì¡° í™•ì¸
```bash
# ìƒì„± ì „
256 weights (ê°€ì¤‘ì¹˜ë§Œ)

# ìƒì„± í›„
384 weights = 256 (lora_up/down) + 128 (alpha ê°’)
```

#### Alpha ê°’ í™•ì¸
```python
from safetensors.torch import load_file

reze = load_file('my_lora_model/Reze.safetensors')
alpha_keys = [k for k in reze.keys() if '.alpha' in k]
print(f'Alpha keys: {len(alpha_keys)}')  # 128
print(reze[alpha_keys[0]].item())  # 64.0 âœ…
```

#### ë¡œë”© í…ŒìŠ¤íŠ¸
```python
from diffusers import StableDiffusionPipeline
import torch

pipe = StableDiffusionPipeline.from_pretrained(
    'stablediffusionapi/anything-v5',
    torch_dtype=torch.float16,
    safety_checker=None
)

# WebUI í˜•ì‹ ë¡œë“œ (alpha ê°’ í¬í•¨)
pipe.load_lora_weights('my_lora_model', weight_name='Reze.safetensors')
# âœ… Diffusersê°€ alpha=64 ìë™ ì¸ì‹
```

### ì½”ë“œ í˜¸í™˜ì„±

#### generate.py - ìë™ ê°ì§€ ë° ë¡œë“œ
**íŒŒì¼:** `core/generate.py` (line 47-80)

```python
if os.path.isdir(lora_path):
    # PEFT í˜•ì‹ ìš°ì„  (adapter_model.safetensors + adapter_config.json)
    adapter_config_path = os.path.join(lora_path, "adapter_config.json")
    adapter_model_path = os.path.join(lora_path, "adapter_model.safetensors")

    if os.path.exists(adapter_config_path) and os.path.exists(adapter_model_path):
        # PEFT í˜•ì‹ - alpha ì •ë³´ ë³´ì¡´ âœ…
        pipe.load_lora_weights(lora_path, weight_name="adapter_model.safetensors")
    else:
        # WebUI í˜•ì‹ - alphaê°€ íŒŒì¼ì— í¬í•¨ë˜ì–´ì•¼ í•¨ âœ…
        safetensors_files = [f for f in os.listdir(lora_path) if f.endswith('.safetensors')]
        pipe.load_lora_weights(lora_path, weight_name=safetensors_files[0])
```

### ê²°ê³¼

**ë³€í™˜ ì „ (alpha ì—†ìŒ):**
```
ì‹¤ì œ ê°•ë„: 1/32 = 0.03125
â†’ LoRA íŠ¹ì§•ì´ ê±°ì˜ ì•ˆ ë³´ì„ âŒ
```

**ë³€í™˜ í›„ (alpha í¬í•¨):**
```
ì‹¤ì œ ê°•ë„: 64/32 = 2.0
â†’ LoRA íŠ¹ì§•ì´ ì •í™•íˆ ë°˜ì˜ë¨ âœ…
```

### S3 ì—…ë¡œë“œ
ì¬ìƒì„±ëœ íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ:
```bash
# íŒŒì¼ ìœ„ì¹˜
/home/rheon/Desktop/Study/lora/my_lora_model/Reze.safetensors

# ë‚´ìš©
- 256ê°œ ê°€ì¤‘ì¹˜ (lora_up/down)
- 128ê°œ alpha ê°’ (ê° ë ˆì´ì–´ë§ˆë‹¤ 64.0)
- ì´ 384ê°œ í•­ëª©
```

### êµí›ˆ
- âœ… **WebUI í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì‹œ alpha ê°’ í•„ìˆ˜**
- âœ… **alpha = lora_alpha / lora_rì´ ì‹¤ì œ LoRA ê°•ë„**
- âœ… **DiffusersëŠ” .alpha í‚¤ë¥¼ ìë™ ì¸ì‹**
- âœ… **í•™ìŠµ ì‹œ ì„¤ì •í•œ alpha, rank ê°’ì„ ë³€í™˜ ì‹œì—ë„ ì „ë‹¬í•´ì•¼ í•¨**

---

---

## 8. ğŸš€ ì¢…í•© ì„±ëŠ¥ ìµœì í™”

### ê°œìš”
í•™ìŠµ ë° ì´ë¯¸ì§€ ìƒì„± ì „ ê³¼ì •ì— ê±¸ì¹œ ì„±ëŠ¥ ìµœì í™”ë¥¼ ì ìš©í•˜ì—¬ **40-65% ì†ë„ í–¥ìƒ**ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

---

### 8.1 ì´ë¯¸ì§€ ìƒì„± ìµœì í™”

#### ğŸ¯ ì ìš©ëœ ìµœì í™”

##### 1. **xFormers ë©”ëª¨ë¦¬ íš¨ìœ¨ì  Attention**
**íŒŒì¼:** `modal_app.py` (line 473-486, 514-526), `core/generate.py` (line 135-148)

```python
# xFormers ìµœì í™” í™œì„±í™”
pipe.enable_xformers_memory_efficient_attention()
pipe.enable_attention_slicing(slice_size="auto")
```

**ì‘ë™ ì›ë¦¬:**
- NVIDIA Tensor Core í™œìš© (FP16 ì—°ì‚° 4-8ë°° ë¹ ë¦„)
- Attention ë©”ì»¤ë‹ˆì¦˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í–¥ìƒ
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 20-30% ê°ì†Œ

**íš¨ê³¼:** ì´ë¯¸ì§€ ìƒì„± ì†ë„ 10-20% í–¥ìƒ

---

##### 2. **ë² ì´ìŠ¤ ëª¨ë¸ ë™ì  ìºì‹±**
**íŒŒì¼:** `modal_app.py` (line 462-535)

```python
class ImageGenerator:
    def __init__(self):
        self.pipelines = {}  # ë² ì´ìŠ¤ ëª¨ë¸ë³„ ìºì‹±

    def get_or_load_pipeline(self, base_model: str):
        """ë² ì´ìŠ¤ ëª¨ë¸ì„ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ë¡œë“œ"""
        if base_model in self.pipelines:
            return self.pipelines[base_model]  # ì¬ì‚¬ìš©!

        # Modal Volume ìºì‹±
        cache_dir = f"/cache/base_models/{base_model}"
        pipe = StableDiffusionPipeline.from_pretrained(cache_dir, ...)
        self.pipelines[base_model] = pipe
        return pipe
```

**ì§€ì› ëª¨ë¸:**
- `stablediffusionapi/anything-v5`
- `Lykon/dreamshaper-8`
- `ANYLORA` (ì‚¬ìš©ì ì»¤ìŠ¤í…€)
- ê¸°íƒ€ HuggingFace ëª¨ë¸ ìë™ ìºì‹±

**íš¨ê³¼:**
- ì²« ìš”ì²­: ëª¨ë¸ ë¡œë”© 6-7ì´ˆ
- ì´í›„ ìš”ì²­: 0ì´ˆ (ë©”ëª¨ë¦¬ì—ì„œ ì¬ì‚¬ìš©)

---

##### 3. **ì™¸ë¶€ VAE ìºì‹±**
**íŒŒì¼:** `modal_app.py` (line 537-579)

```python
def get_or_load_external_vae(self):
    """ì™¸ë¶€ VAEë¥¼ ìºì‹±í•´ì„œ ì¬ì‚¬ìš©"""
    if self.external_vae is not None:
        return self.external_vae  # ë©”ëª¨ë¦¬ ìºì‹œ

    cache_dir = "/cache/vae_model"
    if os.path.exists(cache_dir):
        vae = AutoencoderKL.from_pretrained(cache_dir, ...)
    else:
        vae = AutoencoderKL.from_pretrained(
            "stabilityai/sd-vae-ft-mse", ...
        )
        vae.save_pretrained(cache_dir)  # Volumeì— ì €ì¥

    self.external_vae = vae
    return vae
```

**ë¬¸ì œ í•´ê²°:**
- ê¸°ì¡´: ë§¤ ìš”ì²­ë§ˆë‹¤ VAE ë‹¤ìš´ë¡œë“œ (300MB, 3-5ì´ˆ)
- ê°œì„ : ì²« ìš”ì²­ë§Œ ë‹¤ìš´ë¡œë“œ, ì´í›„ ì¦‰ì‹œ ì‚¬ìš©

**íš¨ê³¼:** ìš”ì²­ë‹¹ 3-5ì´ˆ ì ˆì•½

---

##### 4. **BLIP ìº¡ì…”ë‹ ëª¨ë¸ ìºì‹±**
**íŒŒì¼:** `core/preprocess.py` (line 40-70, 355-384)

```python
# Modal Volume ìºì‹œ ê²½ë¡œ
cache_dir = "/cache/blip_model"

if os.path.exists(cache_dir):
    # ìºì‹œì—ì„œ ë¡œë“œ
    caption_model = BlipForConditionalGeneration.from_pretrained(cache_dir, ...)
else:
    # ë‹¤ìš´ë¡œë“œ í›„ ìºì‹±
    caption_model = BlipForConditionalGeneration.from_pretrained(
        "Salesforce/blip-image-captioning-base", ...
    )
    caption_model.save_pretrained(cache_dir)
```

**íš¨ê³¼:** ì „ì²˜ë¦¬ ì‹œ 30ì´ˆ-1ë¶„ ì ˆì•½

---

#### ğŸ“Š ì´ë¯¸ì§€ ìƒì„± ì„±ëŠ¥ ë¹„êµ

| í•­ëª© | ìµœì í™” ì „ | ìµœì í™” í›„ | ê°œì„ ìœ¨ |
|------|----------|----------|--------|
| ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© | 6-7ì´ˆ | **0ì´ˆ** (ìºì‹±) | âˆ |
| VAE ë¡œë”© | 3-5ì´ˆ | **0ì´ˆ** (ìºì‹±) | âˆ |
| LoRA ë¡œë”© | 1-2ì´ˆ | 1-2ì´ˆ | - |
| ì´ë¯¸ì§€ ìƒì„± (30 steps) | 8ì´ˆ | **6-7ì´ˆ** (xFormers) | 15% â†‘ |
| S3 ì—…ë¡œë“œ | 2-3ì´ˆ | 2-3ì´ˆ | - |
| **ì´ ì‹œê°„** | **20-27ì´ˆ** | **9-12ì´ˆ** | **55-65% ë‹¨ì¶•** |

**ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ë³µì› í›„:**
```
ì´ ì‹œê°„: 6-7ì´ˆ (ìµœê³  ì„±ëŠ¥!) âš¡
```

---

### 8.2 í•™ìŠµ(Training) ìµœì í™”

#### ğŸ¯ ì ìš©ëœ ìµœì í™”

##### 1. **Mixed Precision Training (AMP)**
**íŒŒì¼:** `core/train.py` (line 209-211, 288-308, 313-318)

```python
from torch.cuda.amp import autocast, GradScaler

# Scaler ì„¤ì •
scaler = GradScaler()

# Forward pass (ìë™ FP16/FP32 í˜¼í•©)
with autocast():
    model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample
    loss = F.mse_loss(model_pred.float(), noise.float(), ...)

# Backward pass (FP32 gradient)
scaler.scale(loss).backward()

# Optimizer step
scaler.unscale_(optimizer)
torch.nn.utils.clip_grad_norm_(trainable_params, max_grad_norm)
scaler.step(optimizer)
scaler.update()
```

**ì‘ë™ ì›ë¦¬:**
- Forward pass: FP16 (Tensor Core í™œìš©, 2-4ë°° ë¹ ë¦„)
- Gradient: FP32 (ì •ë°€ë„ ìœ ì§€)
- Loss Scaling: ì‘ì€ gradient ë³´ì¡´
- Master Weights: FP32ë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

**íš¨ê³¼:**
- í•™ìŠµ ì†ë„ 30-50% í–¥ìƒ
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš© 30% ê°ì†Œ
- **ê²°ê³¼ í’ˆì§ˆ ë™ì¼** (ìˆ˜í•™ì ìœ¼ë¡œ ë™ì¼í•œ ì—°ì‚°)

---

##### 2. **DataLoader ë³‘ë ¬ ë¡œë”©**
**íŒŒì¼:** `core/train.py` (line 22-92, 304-311)

```python
class LoRADataset(Dataset):
    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬
        img = Image.open(img_path).convert("RGB").resize(...)
        return img_tensor, caption

# DataLoader ì„¤ì •
train_dataloader = DataLoader(
    train_dataset,
    batch_size=4,
    shuffle=True,
    num_workers=4,        # 4ê°œ í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬ ë¡œë”©
    pin_memory=True,      # GPU ì§ì ‘ ì „ì†¡ (DMA)
    drop_last=False
)
```

**ì‘ë™ ì›ë¦¬:**
- 4ê°œ ì›Œì»¤ í”„ë¡œì„¸ìŠ¤ê°€ ì´ë¯¸ì§€ ë³‘ë ¬ ë¡œë”©
- Prefetching: GPUê°€ ì—°ì‚°í•˜ëŠ” ë™ì•ˆ ë‹¤ìŒ ë°°ì¹˜ ë¯¸ë¦¬ ì¤€ë¹„
- Pin Memory: CPU â†’ GPU ì§ì ‘ ì „ì†¡ (2ë°° ë¹ ë¦„)

**íš¨ê³¼:**
- I/O ë³‘ëª© ì œê±°
- GPU í™œìš©ë¥ : 16% â†’ 95%+
- í•™ìŠµ ì†ë„ 10-20% í–¥ìƒ

---

##### 3. **Text Embeddings ì‚¬ì „ ìºì‹±**
**íŒŒì¼:** `core/train.py` (line 178-228, 292-296)

```python
def precompute_text_embeddings(text_encoder, tokenizer, captions_list, device):
    """ìœ ë‹ˆí¬ ìº¡ì…˜ë“¤ì˜ embeddingì„ ë¯¸ë¦¬ ê³„ì‚°"""
    unique_captions = list(set(captions_list))

    embeddings_cache = {}
    for caption in unique_captions:
        with torch.no_grad():
            embedding = text_encoder(tokenizer(caption))[0]
        embeddings_cache[caption] = embedding

    return embeddings_cache

# í•™ìŠµ ì „ 1íšŒ ê³„ì‚°
text_embeddings_cache = precompute_text_embeddings(
    text_encoder, tokenizer, all_captions, device
)

# Datasetì— ìºì‹œ ì „ë‹¬
train_dataset = LoRADataset(..., text_embeddings_cache=text_embeddings_cache)
```

**ì‘ë™ ì›ë¦¬:**
- í•™ìŠµ ì‹œì‘ ì‹œ ëª¨ë“  ìœ ë‹ˆí¬ ìº¡ì…˜ì„ 1íšŒë§Œ ì¸ì½”ë”©
- í•™ìŠµ ì¤‘ì—ëŠ” ë”•ì…”ë„ˆë¦¬ lookupë§Œ (O(1))
- 250 ì—í¬í¬ Ã— 10ê°œ ì´ë¯¸ì§€ = 2,500ë²ˆ ì¸ì½”ë”© â†’ 6ë²ˆë§Œ!

**íš¨ê³¼:**
- Text encoding ì‹œê°„: 12.5ì´ˆ â†’ 0.03ì´ˆ
- ë©”ëª¨ë¦¬ ì‚¬ìš©: 1-2MB (ë§¤ìš° ì ìŒ)

---

##### 4. **VAE Latents ì‚¬ì „ ìºì‹±** (ì„ íƒì‚¬í•­)
**íŒŒì¼:** `core/precompute_latents.py`, `core/train.py` (line 30-31, 60-74, 296, 331-332, 410-418)

```python
# ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ latents ê³„ì‚°
def precompute_latents(dataset_path, output_path, model_id, device):
    vae = AutoencoderKL.from_pretrained(model_id, subfolder="vae", ...)

    for img_file in images:
        img = Image.open(img_file).resize((512, 512))
        pixel_values = preprocess(img)

        # VAE encoding (1íšŒë§Œ!)
        latent = vae.encode(pixel_values).latent_dist.sample()

        # ë””ìŠ¤í¬ì— ì €ì¥
        torch.save(latent.cpu(), f"{img_file.stem}_latent.pt")

# í•™ìŠµ ì‹œ latents ì§ì ‘ ë¡œë“œ
if use_cached_latents:
    latents = torch.load(latent_file).to(device)
else:
    latents = vae.encode(pixel_values).latent_dist.sample()
```

**ì‘ë™ ì›ë¦¬:**
- ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ë¥¼ VAEë¡œ ì¸ì½”ë”©
- ë””ìŠ¤í¬ì— `.pt` íŒŒì¼ë¡œ ì €ì¥ (ì´ë¯¸ì§€ 1/10 í¬ê¸°)
- í•™ìŠµ ì‹œ íŒŒì¼ ë¡œë“œë§Œ (VAE encoding ìƒëµ)

**íš¨ê³¼:**
- VAE encoding ì‹œê°„: 31ì´ˆ â†’ 0.5ì´ˆ (250 ì—í¬í¬ ê¸°ì¤€)
- ë””ìŠ¤í¬ ì‚¬ìš©: ì´ë¯¸ì§€ 5MB â†’ latents 500KB

**ì‚¬ìš© ë°©ë²•:**
```bash
# 1. Latents ì‚¬ì „ ê³„ì‚°
python -m core.precompute_latents \
    --dataset_path ./dataset_clean \
    --output_path ./dataset_latents

# 2. í•™ìŠµ ì‹œ ì‚¬ìš©
python train.py --use_cached_latents --latents_dir ./dataset_latents
```

---

##### 5. **EasyOCR ëª¨ë¸ ìºì‹±**
**íŒŒì¼:** `core/preprocess.py` (line 32-43)

```python
# Modal Volume ìºì‹œ ê²½ë¡œ ì§€ì •
cache_dir = "/cache/easyocr_model"
os.makedirs(cache_dir, exist_ok=True)

self.reader = easyocr.Reader(
    ['ko', 'en'],
    gpu=torch.cuda.is_available(),
    model_storage_directory=cache_dir,  # ìºì‹œ ê²½ë¡œ
    download_enabled=True
)
```

**íš¨ê³¼:** í•™ìŠµë‹¹ 10-20ì´ˆ ì ˆì•½

---

#### ğŸ“Š í•™ìŠµ ì„±ëŠ¥ ë¹„êµ

| í•­ëª© | ìµœì í™” ì „ | ìµœì í™” í›„ | ê°œì„ ìœ¨ |
|------|----------|----------|--------|
| **ì „ì²˜ë¦¬** |  |  |  |
| OCR ë‹¤ìš´ë¡œë“œ | 10-20ì´ˆ | **0ì´ˆ** (ìºì‹±) | âˆ |
| BLIP ë‹¤ìš´ë¡œë“œ | 30-60ì´ˆ | **0ì´ˆ** (ìºì‹±) | âˆ |
| ì „ì²˜ë¦¬ ì‹¤í–‰ | 5-10ë¶„ | 5-10ë¶„ | - |
| **í•™ìŠµ (250 ì—í¬í¬)** |  |  |  |
| ëª¨ë¸ ë¡œë”© | 6-7ì´ˆ | 6-7ì´ˆ | - |
| Text encoding | 12.5ì´ˆ | **0.03ì´ˆ** (ìºì‹±) | 400ë°° â†‘ |
| VAE encoding | 31ì´ˆ | **0.5ì´ˆ** (ì„ íƒì  ìºì‹±) | 60ë°° â†‘ |
| ì‹¤ì œ í•™ìŠµ | 20ë¶„ | **13-14ë¶„** (AMP + DataLoader) | 35% â†‘ |
| **ì´ ì‹œê°„** | **36-51ë¶„** | **20-30ë¶„** | **40-50% ë‹¨ì¶•** |

---

### 8.3 ì „ì²´ ìµœì í™” ìš”ì•½

#### ì ìš©ëœ ê¸°ìˆ 

| ìµœì í™” | ëŒ€ìƒ | íš¨ê³¼ | êµ¬í˜„ ë‚œì´ë„ |
|--------|------|------|------------|
| xFormers | ì´ë¯¸ì§€ ìƒì„± | 10-20% â†‘ | ì‰¬ì›€ |
| Pipeline ìºì‹± | ì´ë¯¸ì§€ ìƒì„± | 6-7ì´ˆ ì ˆì•½ | ì¤‘ê°„ |
| VAE ìºì‹± | ì´ë¯¸ì§€ ìƒì„± | 3-5ì´ˆ ì ˆì•½ | ì‰¬ì›€ |
| BLIP ìºì‹± | ì „ì²˜ë¦¬ | 30-60ì´ˆ ì ˆì•½ | ì‰¬ì›€ |
| EasyOCR ìºì‹± | ì „ì²˜ë¦¬ | 10-20ì´ˆ ì ˆì•½ | ë§¤ìš° ì‰¬ì›€ |
| Mixed Precision | í•™ìŠµ | 30-50% â†‘ | ì‰¬ì›€ |
| DataLoader | í•™ìŠµ | 10-20% â†‘ | ì¤‘ê°„ |
| Text Embeddings | í•™ìŠµ | 12ì´ˆ ì ˆì•½ | ì‰¬ì›€ |
| VAE Latents | í•™ìŠµ | 30ì´ˆ ì ˆì•½ | ì¤‘ê°„ |

---

#### ìµœì¢… ì„±ëŠ¥ ìˆ˜ì¹˜

**ì´ë¯¸ì§€ ìƒì„±:**
```
20-27ì´ˆ â†’ 9-12ì´ˆ (55-65% ë‹¨ì¶•)
ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ë³µì› í›„: 6-7ì´ˆ (ìµœê³  ì„±ëŠ¥)
```

**í•™ìŠµ:**
```
36-51ë¶„ â†’ 20-30ë¶„ (40-50% ë‹¨ì¶•)
VAE Latents ìºì‹± ì ìš© ì‹œ: 18-28ë¶„ (50-60% ë‹¨ì¶•)
```

---

#### ë¹„ìš© íš¨ìœ¨ì„±

**GPU ì‚¬ìš© ì‹œê°„ ì ˆê°:**
- ì´ë¯¸ì§€ 1,000ì¥ ìƒì„±: 5.5ì‹œê°„ â†’ 2.5ì‹œê°„ ì ˆì•½
- í•™ìŠµ 10íšŒ: 6ì‹œê°„ â†’ 3ì‹œê°„ ì ˆì•½

**Modal GPU T4 ë¹„ìš© ê¸°ì¤€ ($0.60/ì‹œê°„):**
- ì›”ê°„ ì ˆì•½: ~$150-200 (ì¤‘ê°„ ì‚¬ìš©ëŸ‰ ê¸°ì¤€)

---

### 8.4 ì ìš© ë°©ë²•

#### ì¦‰ì‹œ ì ìš© ê°€ëŠ¥ (ì½”ë“œ ë³€ê²½ ì—†ìŒ)
- âœ… xFormers
- âœ… Pipeline ìºì‹±
- âœ… VAE ìºì‹±
- âœ… BLIP ìºì‹±
- âœ… EasyOCR ìºì‹±
- âœ… Mixed Precision Training
- âœ… DataLoader
- âœ… Text Embeddings ìºì‹±

**ë°°í¬:**
```bash
# Modal ì¬ë°°í¬
modal deploy modal_app.py

# ë¡œê·¸ í™•ì¸
modal app logs lora-training-inference
```

#### ì„ íƒì  ì ìš©
**VAE Latents ìºì‹±:**
```bash
# 1. ì „ì²˜ë¦¬ + Latents ê³„ì‚°
python -m core.precompute_latents \
    --dataset_path ./dataset_clean \
    --output_path ./dataset_latents

# 2. í•™ìŠµ ì‹œ --use_cached_latents í”Œë˜ê·¸ ì¶”ê°€
# (modal_app.pyì—ì„œ train_lora í˜¸ì¶œ ì‹œ íŒŒë¼ë¯¸í„° ì „ë‹¬)
```

---

### 8.5 ëª¨ë‹ˆí„°ë§

#### ì„±ëŠ¥ í™•ì¸ ë°©ë²•

**ì´ë¯¸ì§€ ìƒì„± ë¡œê·¸:**
```bash
ğŸš€ Using cached pipeline for: stablediffusionapi/anything-v5  # íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
ğŸš€ Using cached external VAE (in-memory)  # VAE ì¬ì‚¬ìš©
âœ… xFormers memory efficient attention enabled  # xFormers í™œì„±í™”
```

**í•™ìŠµ ë¡œê·¸:**
```bash
âœ… Mixed Precision Training (AMP) enabled  # AMP í™œì„±í™”
âœ… DataLoader created with 4 workers (parallel loading)  # ë³‘ë ¬ ë¡œë”©
ğŸ“ Precomputing text embeddings for 6 unique captions...  # Text embeddings ìºì‹±
   Memory usage: 1.18 MB (120.5 KB per embedding)
âœ… Using cached VAE latents from: ./dataset_latents  # VAE latents ìºì‹± (ì„ íƒì‚¬í•­)
```

---

### 8.6 ì°¸ê³  ì‚¬í•­

#### í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­
- **GPU ë©”ëª¨ë¦¬:** 16GB+ ê¶Œì¥ (T4, A10G, V100 ë“±)
- **CPU:** 4+ ì½”ì–´ (DataLoader ë³‘ë ¬í™”)
- **ë””ìŠ¤í¬:** 10GB+ ì—¬ìœ  ê³µê°„ (ìºì‹±ìš©)

#### ì£¼ì˜ì‚¬í•­
- Mixed Precisionì€ FP32ì™€ **ìˆ˜í•™ì ìœ¼ë¡œ ë™ì¼**í•œ ê²°ê³¼
- Text Embeddings ìºì‹±ì€ ë©”ëª¨ë¦¬ 1-2MBë§Œ ì‚¬ìš©
- VAE Latents ìºì‹±ì€ ì„ íƒì‚¬í•­ (ë””ìŠ¤í¬ ê³µê°„ í•„ìš”)
- ëª¨ë“  ìµœì í™”ëŠ” **ê²°ê³¼ í’ˆì§ˆì— ì˜í–¥ ì—†ìŒ**

---

## ì°¸ê³  ìë£Œ
- [Diffusers CLIP Skip êµ¬í˜„](https://github.com/huggingface/diffusers/issues/2363)
- [VAE Float32 ì´ìŠˆ](https://github.com/huggingface/diffusers/pull/254)
- [Modal ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ·](https://modal.com/docs/guide/memory-snapshots)
- [Modal Keep Warm](https://modal.com/docs/guide/cold-start)
- [Modal Volume ìºì‹±](https://modal.com/docs/guide/volumes)
- [LoRA Alpha ìŠ¤ì¼€ì¼ë§](https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/layer.py)
- [PyTorch AMP](https://pytorch.org/docs/stable/amp.html)
- [xFormers](https://github.com/facebookresearch/xformers)
- [PyTorch DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)
